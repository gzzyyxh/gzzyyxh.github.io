

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="5.2-using-convnets-with-small-datasets">
  <meta name="author" content="游隼">
  <meta name="keywords" content="">
  
  <title>DL with python-7 - 游隼</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gzzyyxh.github.io","root":"/","version":"1.8.10","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>丰之余</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-comment"></i>
                留言板
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="DL with python-7">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-04-22 23:38" pubdate>
        2021年4月22日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      122
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">DL with python-7</h1>
            
            <div class="markdown-body">
              <p>5.2-using-convnets-with-small-datasets</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure>

<p>Using TensorFlow backend.</p>
<p>‘2.0.8’</p>
<h1 id="5-2-Using-convnets-with-small-datasets"><a href="#5-2-Using-convnets-with-small-datasets" class="headerlink" title="5.2 - Using convnets with small datasets"></a>5.2 - Using convnets with small datasets</h1><p>This notebook contains the code sample found in Chapter 5, Section 2 of <a target="_blank" rel="noopener" href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p>
<h2 id="Training-a-convnet-from-scratch-on-a-small-dataset"><a href="#Training-a-convnet-from-scratch-on-a-small-dataset" class="headerlink" title="Training a convnet from scratch on a small dataset"></a>Training a convnet from scratch on a small dataset</h2><p>Having to train an image classification model using only very little data is a common situation, which you likely encounter yourself in<br>practice if you ever do computer vision in a professional context.</p>
<p>Having “few” samples can mean anywhere from a few hundreds to a few tens of thousands of images. As a practical example, we will focus on<br>classifying images as “dogs” or “cats”, in a dataset containing 4000 pictures of cats and dogs (2000 cats, 2000 dogs). We will use 2000<br>pictures for training, 1000 for validation, and finally 1000 for testing.</p>
<p>In this section, we will review one basic strategy to tackle this problem: training a new model from scratch on what little data we have. We<br>will start by naively training a small convnet on our 2000 training samples, without any regularization, to set a baseline for what can be<br>achieved. This will get us to a classification accuracy of 71%. At that point, our main issue will be overfitting. Then we will introduce<br><em>data augmentation</em>, a powerful technique for mitigating overfitting in computer vision. By leveraging data augmentation, we will improve<br>our network to reach an accuracy of 82%.</p>
<p>In the next section, we will review two more essential techniques for applying deep learning to small datasets: <em>doing feature extraction<br>with a pre-trained network</em> (this will get us to an accuracy of 90% to 93%), and <em>fine-tuning a pre-trained network</em> (this will get us to<br>our final accuracy of 95%). Together, these three strategies – training a small model from scratch, doing feature extracting using a<br>pre-trained model, and fine-tuning a pre-trained model – will constitute your future toolbox for tackling the problem of doing computer<br>vision with small datasets.</p>
<h2 id="The-relevance-of-deep-learning-for-small-data-problems"><a href="#The-relevance-of-deep-learning-for-small-data-problems" class="headerlink" title="The relevance of deep learning for small-data problems"></a>The relevance of deep learning for small-data problems</h2><p>You will sometimes hear that deep learning only works when lots of data is available. This is in part a valid point: one fundamental<br>characteristic of deep learning is that it is able to find interesting features in the training data on its own, without any need for manual<br>feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where<br>the input samples are very high-dimensional, like images.</p>
<p>However, what constitutes “lots” of samples is relative – relative to the size and depth of the network you are trying to train, for<br>starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundreds can<br>potentially suffice if the model is small and well-regularized and if the task is simple.<br>Because convnets learn local, translation-invariant features, they are very<br>data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results<br>despite a relative lack of data, without the need for any custom feature engineering. You will see this in action in this section.</p>
<p>But what’s more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model<br>trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes. Specifically, in the case of<br>computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used<br>to bootstrap powerful vision models out of very little data. That’s what we will do in the next section.</p>
<p>For now, let’s get started by getting our hands on the data.</p>
<h2 id="Downloading-the-data"><a href="#Downloading-the-data" class="headerlink" title="Downloading the data"></a>Downloading the data</h2><p>The cats vs. dogs dataset that we will use isn’t packaged with Keras. It was made available by Kaggle.com as part of a computer vision<br>competition in late 2013, back when convnets weren’t quite mainstream. You can download the original dataset at:<br><code>https://www.kaggle.com/c/dogs-vs-cats/data</code> (you will need to create a Kaggle account if you don’t already have one – don’t worry, the<br>process is painless).</p>
<p>The pictures are medium-resolution color JPEGs. They look like this:</p>
<p><img src="https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg" srcset="/img/loading.gif" lazyload alt="cats_vs_dogs_samples"></p>
<p>Unsurprisingly, the cats vs. dogs Kaggle competition in 2013 was won by entrants who used convnets. The best entries could achieve up to<br>95% accuracy. In our own example, we will get fairly close to this accuracy (in the next section), even though we will be training our<br>models on less than 10% of the data that was available to the competitors.<br>This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). After downloading<br>and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, a validation<br>set with 500 samples of each class, and finally a test set with 500 samples of each class.</p>
<p>Here are a few lines of code to do this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os, shutil<br></code></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># The path to the directory where the original</span><br><span class="hljs-comment"># dataset was uncompressed</span><br>original_dataset_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/kaggle_original_data&#x27;</span><br><br><span class="hljs-comment"># The directory where we will</span><br><span class="hljs-comment"># store our smaller dataset</span><br>base_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/cats_and_dogs_small&#x27;</span><br>os.mkdir(base_dir)<br><br><span class="hljs-comment"># Directories for our training,</span><br><span class="hljs-comment"># validation and test splits</span><br>train_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;train&#x27;</span>)<br>os.mkdir(train_dir)<br>validation_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;validation&#x27;</span>)<br>os.mkdir(validation_dir)<br>test_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;test&#x27;</span>)<br>os.mkdir(test_dir)<br><br><span class="hljs-comment"># Directory with our training cat pictures</span><br>train_cats_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(train_cats_dir)<br><br><span class="hljs-comment"># Directory with our training dog pictures</span><br>train_dogs_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(train_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(validation_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(validation_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>test_cats_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(test_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>test_dogs_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(test_dogs_dir)<br><br><span class="hljs-comment"># Copy first 1000 cat images to train_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br><br><span class="hljs-comment"># Copy next 500 cat images to validation_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 cat images to test_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy first 1000 dog images to train_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to validation_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to test_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br></code></pre></td></tr></table></figure>

<p>As a sanity check, let’s count how many pictures we have in each training split (train/validation/test):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_cats_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total training cat images: 1000
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_dogs_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total training dog images: 1000
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_cats_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total validation cat images: 500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_dogs_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total validation dog images: 500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_cats_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total test cat images: 500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_dogs_dir)))<br></code></pre></td></tr></table></figure>

<pre><code>total test dog images: 500
</code></pre>
<p>So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of<br>samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate<br>measure of success.</p>
<h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>We’ve already built a small convnet for MNIST in the previous example, so you should be familiar with them. We will reuse the same<br>general structure: our convnet will be a stack of alternated <code>Conv2D</code> (with <code>relu</code> activation) and <code>MaxPooling2D</code> layers.</p>
<p>However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger: it will have one<br>more <code>Conv2D</code> + <code>MaxPooling2D</code> stage. This serves both to augment the capacity of the network, and to further reduce the size of the<br>feature maps, so that they aren’t overly large when we reach the <code>Flatten</code> layer. Here, since we start from inputs of size 150x150 (a<br>somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the <code>Flatten</code> layer.</p>
<p>Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is<br>decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.</p>
<p>Since we are attacking a binary classification problem, we are ending the network with a single unit (a <code>Dense</code> layer of size 1) and a<br><code>sigmoid</code> activation. This unit will encode the probability that the network is looking at one class or the other.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><br>model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure>

<p>Let’s take a look at how the dimensions of the feature maps change with every successive layer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>For our compilation step, we’ll go with the <code>RMSprop</code> optimizer as usual. Since we ended our network with a single sigmoid unit, we will<br>use binary crossentropy as our loss (as a reminder, check out the table in Chapter 4, section 5 for a cheatsheet on what loss function to<br>use in various situations).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure>

<h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p>As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our<br>network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:</p>
<ul>
<li>Read the picture files.</li>
<li>Decode the JPEG content to RBG grids of pixels.</li>
<li>Convert these into floating point tensors.</li>
<li>Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).</li>
</ul>
<p>It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image<br>processing helper tools, located at <code>keras.preprocessing.image</code>. In particular, it contains the class <code>ImageDataGenerator</code> which allows to<br>quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we<br>will use here.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator<br><br><span class="hljs-comment"># All images will be rescaled by 1./255</span><br>train_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br></code></pre></td></tr></table></figure>

<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</code></pre>
<p>Let’s take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape <code>(20, 150, 150, 3)</code>) and binary<br>labels (shape <code>(20,)</code>). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches<br>indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to <code>break</code> the iteration loop<br>at some point.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> data_batch, labels_batch <span class="hljs-keyword">in</span> train_generator:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data batch shape:&#x27;</span>, data_batch.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;labels batch shape:&#x27;</span>, labels_batch.shape)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure>

<pre><code>data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
</code></pre>
<p>Let’s fit our model to the data using the generator. We do it using the <code>fit_generator</code> method, the equivalent of <code>fit</code> for data generators<br>like ours. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does.<br>Because the data is being generated endlessly, the generator needs to know example how many samples to draw from the generator before<br>declaring an epoch over. This is the role of the <code>steps_per_epoch</code> argument: after having drawn <code>steps_per_epoch</code> batches from the<br>generator, i.e. after having run for <code>steps_per_epoch</code> gradient descent steps, the fitting process will go to the next epoch. In our case,<br>batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.</p>
<p>When using <code>fit_generator</code>, one may pass a <code>validation_data</code> argument, much like with the <code>fit</code> method. Importantly, this argument is<br>allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as <code>validation_data</code>, then<br>this generator is expected to yield batches of validation data endlessly, and thus you should also specify the <code>validation_steps</code> argument,<br>which tells the process how many batches to draw from the validation generator for evaluation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">30</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure>

<pre><code>Epoch 1/30
100/100 [==============================] - 9s - loss: 0.6898 - acc: 0.5285 - val_loss: 0.6724 - val_acc: 0.5950
Epoch 2/30
100/100 [==============================] - 8s - loss: 0.6543 - acc: 0.6340 - val_loss: 0.6565 - val_acc: 0.5950
Epoch 3/30
100/100 [==============================] - 8s - loss: 0.6143 - acc: 0.6690 - val_loss: 0.6116 - val_acc: 0.6650
Epoch 4/30
100/100 [==============================] - 8s - loss: 0.5626 - acc: 0.7125 - val_loss: 0.5774 - val_acc: 0.6970
Epoch 5/30
100/100 [==============================] - 8s - loss: 0.5266 - acc: 0.7335 - val_loss: 0.5726 - val_acc: 0.6960
Epoch 6/30
100/100 [==============================] - 8s - loss: 0.5007 - acc: 0.7550 - val_loss: 0.6075 - val_acc: 0.6580
Epoch 7/30
100/100 [==============================] - 8s - loss: 0.4723 - acc: 0.7840 - val_loss: 0.5516 - val_acc: 0.7060
Epoch 8/30
100/100 [==============================] - 8s - loss: 0.4521 - acc: 0.7875 - val_loss: 0.5724 - val_acc: 0.6980
Epoch 9/30
100/100 [==============================] - 8s - loss: 0.4163 - acc: 0.8095 - val_loss: 0.5653 - val_acc: 0.7140
Epoch 10/30
100/100 [==============================] - 8s - loss: 0.3988 - acc: 0.8185 - val_loss: 0.5508 - val_acc: 0.7180
Epoch 11/30
100/100 [==============================] - 8s - loss: 0.3694 - acc: 0.8385 - val_loss: 0.5712 - val_acc: 0.7300
Epoch 12/30
100/100 [==============================] - 8s - loss: 0.3385 - acc: 0.8465 - val_loss: 0.6097 - val_acc: 0.7110
Epoch 13/30
100/100 [==============================] - 8s - loss: 0.3229 - acc: 0.8565 - val_loss: 0.5827 - val_acc: 0.7150
Epoch 14/30
100/100 [==============================] - 8s - loss: 0.2962 - acc: 0.8720 - val_loss: 0.5928 - val_acc: 0.7190
Epoch 15/30
100/100 [==============================] - 8s - loss: 0.2684 - acc: 0.9005 - val_loss: 0.5921 - val_acc: 0.7190
Epoch 16/30
100/100 [==============================] - 8s - loss: 0.2509 - acc: 0.8980 - val_loss: 0.6148 - val_acc: 0.7250
Epoch 17/30
100/100 [==============================] - 8s - loss: 0.2221 - acc: 0.9110 - val_loss: 0.6487 - val_acc: 0.7010
Epoch 18/30
100/100 [==============================] - 8s - loss: 0.2021 - acc: 0.9250 - val_loss: 0.6185 - val_acc: 0.7300
Epoch 19/30
100/100 [==============================] - 8s - loss: 0.1824 - acc: 0.9310 - val_loss: 0.7713 - val_acc: 0.7020
Epoch 20/30
100/100 [==============================] - 8s - loss: 0.1579 - acc: 0.9425 - val_loss: 0.6657 - val_acc: 0.7260
Epoch 21/30
100/100 [==============================] - 8s - loss: 0.1355 - acc: 0.9550 - val_loss: 0.8077 - val_acc: 0.7040
Epoch 22/30
100/100 [==============================] - 8s - loss: 0.1247 - acc: 0.9545 - val_loss: 0.7726 - val_acc: 0.7080
Epoch 23/30
100/100 [==============================] - 8s - loss: 0.1111 - acc: 0.9585 - val_loss: 0.7387 - val_acc: 0.7220
Epoch 24/30
100/100 [==============================] - 8s - loss: 0.0932 - acc: 0.9710 - val_loss: 0.8196 - val_acc: 0.7050
Epoch 25/30
100/100 [==============================] - 8s - loss: 0.0707 - acc: 0.9790 - val_loss: 0.9012 - val_acc: 0.7190
Epoch 26/30
100/100 [==============================] - 8s - loss: 0.0625 - acc: 0.9855 - val_loss: 1.0437 - val_acc: 0.6970
Epoch 27/30
100/100 [==============================] - 8s - loss: 0.0611 - acc: 0.9820 - val_loss: 0.9831 - val_acc: 0.7060
Epoch 28/30
100/100 [==============================] - 8s - loss: 0.0488 - acc: 0.9865 - val_loss: 0.9721 - val_acc: 0.7310
Epoch 29/30
100/100 [==============================] - 8s - loss: 0.0375 - acc: 0.9915 - val_loss: 0.9987 - val_acc: 0.7100
Epoch 30/30
100/100 [==============================] - 8s - loss: 0.0387 - acc: 0.9895 - val_loss: 1.0139 - val_acc: 0.7240
</code></pre>
<p>It is good practice to always save your models after training:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_1.h5&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>Let’s plot the loss and accuracy of the model over the training and validation data during training:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/img/output_30_0p.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p><img src="/img/output_30_1.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p>These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our<br>validation accuracy stalls at 70-72%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss<br>keeps decreasing linearly until it reaches nearly 0.</p>
<p>Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. You already know about a<br>number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to<br>introduce a new one, specific to computer vision, and used almost universally when processing images with deep learning models: <em>data<br>augmentation</em>.</p>
<h2 id="Using-data-augmentation"><a href="#Using-data-augmentation" class="headerlink" title="Using data augmentation"></a>Using data augmentation</h2><p>Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data.<br>Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data<br>augmentation takes the approach of generating more training data from existing training samples, by “augmenting” the samples via a number<br>of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same<br>picture twice. This helps the model get exposed to more aspects of the data and generalize better.</p>
<p>In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our <code>ImageDataGenerator</code><br>instance. Let’s get started with an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">datagen = ImageDataGenerator(<br>      rotation_range=<span class="hljs-number">40</span>,<br>      width_shift_range=<span class="hljs-number">0.2</span>,<br>      height_shift_range=<span class="hljs-number">0.2</span>,<br>      shear_range=<span class="hljs-number">0.2</span>,<br>      zoom_range=<span class="hljs-number">0.2</span>,<br>      horizontal_flip=<span class="hljs-literal">True</span>,<br>      fill_mode=<span class="hljs-string">&#x27;nearest&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>These are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over what we just wrote:</p>
<ul>
<li><code>rotation_range</code> is a value in degrees (0-180), a range within which to randomly rotate pictures.</li>
<li><code>width_shift</code> and <code>height_shift</code> are ranges (as a fraction of total width or height) within which to randomly translate pictures<br>vertically or horizontally.</li>
<li><code>shear_range</code> is for randomly applying shearing transformations.</li>
<li><code>zoom_range</code> is for randomly zooming inside pictures.</li>
<li><code>horizontal_flip</code> is for randomly flipping half of the images horizontally – relevant when there are no assumptions of horizontal<br>asymmetry (e.g. real-world pictures).</li>
<li><code>fill_mode</code> is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.</li>
</ul>
<p>Let’s take a look at our augmented images:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This is module with image preprocessing utilities</span><br><span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> image<br><br>fnames = [os.path.join(train_cats_dir, fname) <span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> os.listdir(train_cats_dir)]<br><br><span class="hljs-comment"># We pick one image to &quot;augment&quot;</span><br>img_path = fnames[<span class="hljs-number">3</span>]<br><br><span class="hljs-comment"># Read the image and resize it</span><br>img = image.load_img(img_path, target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>))<br><br><span class="hljs-comment"># Convert it to a Numpy array with shape (150, 150, 3)</span><br>x = image.img_to_array(img)<br><br><span class="hljs-comment"># Reshape it to (1, 150, 150, 3)</span><br>x = x.reshape((<span class="hljs-number">1</span>,) + x.shape)<br><br><span class="hljs-comment"># The .flow() command below generates batches of randomly transformed images.</span><br><span class="hljs-comment"># It will loop indefinitely, so we need to `break` the loop at some point!</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> datagen.flow(x, batch_size=<span class="hljs-number">1</span>):<br>    plt.figure(i)<br>    imgplot = plt.imshow(image.array_to_img(batch[<span class="hljs-number">0</span>]))<br>    i += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">4</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">break</span><br><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/img/output_35_0.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p><img src="/img/output_35_1.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p><img src="/img/output_35_2.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p><img src="/img/output_35_3.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p>If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs<br>that it sees are still heavily intercorrelated, since they come from a small number of original images – we cannot produce new information,<br>we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight<br>overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure>

<p>Let’s train our network using data augmentation and dropout:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">train_datagen = ImageDataGenerator(<br>    rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>,<br>    rotation_range=<span class="hljs-number">40</span>,<br>    width_shift_range=<span class="hljs-number">0.2</span>,<br>    height_shift_range=<span class="hljs-number">0.2</span>,<br>    shear_range=<span class="hljs-number">0.2</span>,<br>    zoom_range=<span class="hljs-number">0.2</span>,<br>    horizontal_flip=<span class="hljs-literal">True</span>,)<br><br><span class="hljs-comment"># Note that the validation data should not be augmented!</span><br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">100</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure>

<pre><code>Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/100
100/100 [==============================] - 24s - loss: 0.6857 - acc: 0.5447 - val_loss: 0.6620 - val_acc: 0.5888
Epoch 2/100
100/100 [==============================] - 23s - loss: 0.6710 - acc: 0.5675 - val_loss: 0.6606 - val_acc: 0.5825
Epoch 3/100
100/100 [==============================] - 22s - loss: 0.6609 - acc: 0.5913 - val_loss: 0.6663 - val_acc: 0.5711.594 - ETA: 7s - loss: 0.6655 - ETA: 5s - los - ETA: 1s - loss: 0.6620 - acc: 
Epoch 4/100
100/100 [==============================] - 22s - loss: 0.6446 - acc: 0.6178 - val_loss: 0.6200 - val_acc: 0.6379
Epoch 5/100
100/100 [==============================] - 22s - loss: 0.6267 - acc: 0.6325 - val_loss: 0.6280 - val_acc: 0.5996
Epoch 6/100
100/100 [==============================] - 22s - loss: 0.6080 - acc: 0.6631 - val_loss: 0.6841 - val_acc: 0.5490
Epoch 7/100
100/100 [==============================] - 22s - loss: 0.5992 - acc: 0.6700 - val_loss: 0.5717 - val_acc: 0.6946
Epoch 8/100
100/100 [==============================] - 22s - loss: 0.5908 - acc: 0.6819 - val_loss: 0.5858 - val_acc: 0.6764
Epoch 9/100
100/100 [==============================] - 22s - loss: 0.5869 - acc: 0.6856 - val_loss: 0.5658 - val_acc: 0.6785
Epoch 10/100
100/100 [==============================] - 23s - loss: 0.5692 - acc: 0.6934 - val_loss: 0.5409 - val_acc: 0.7170
Epoch 11/100
100/100 [==============================] - 22s - loss: 0.5708 - acc: 0.6897 - val_loss: 0.5325 - val_acc: 0.7274
Epoch 12/100
100/100 [==============================] - 23s - loss: 0.5583 - acc: 0.7047 - val_loss: 0.5683 - val_acc: 0.7126
Epoch 13/100
100/100 [==============================] - 22s - loss: 0.5602 - acc: 0.7069 - val_loss: 0.6010 - val_acc: 0.6593
Epoch 14/100
100/100 [==============================] - 22s - loss: 0.5510 - acc: 0.7231 - val_loss: 0.5387 - val_acc: 0.7229
Epoch 15/100
100/100 [==============================] - 23s - loss: 0.5527 - acc: 0.7175 - val_loss: 0.5204 - val_acc: 0.7322
Epoch 16/100
100/100 [==============================] - 23s - loss: 0.5426 - acc: 0.7181 - val_loss: 0.5083 - val_acc: 0.7410
Epoch 17/100
100/100 [==============================] - 23s - loss: 0.5399 - acc: 0.7344 - val_loss: 0.5103 - val_acc: 0.7468
Epoch 18/100
100/100 [==============================] - 23s - loss: 0.5375 - acc: 0.7312 - val_loss: 0.5133 - val_acc: 0.7430
Epoch 19/100
100/100 [==============================] - 22s - loss: 0.5308 - acc: 0.7338 - val_loss: 0.4936 - val_acc: 0.7610
Epoch 20/100
100/100 [==============================] - 22s - loss: 0.5225 - acc: 0.7387 - val_loss: 0.4952 - val_acc: 0.7563
Epoch 21/100
100/100 [==============================] - 22s - loss: 0.5180 - acc: 0.7491 - val_loss: 0.4999 - val_acc: 0.7481
Epoch 22/100
100/100 [==============================] - 23s - loss: 0.5118 - acc: 0.7538 - val_loss: 0.4770 - val_acc: 0.7764
Epoch 23/100
100/100 [==============================] - 22s - loss: 0.5245 - acc: 0.7378 - val_loss: 0.4929 - val_acc: 0.7671
Epoch 24/100
100/100 [==============================] - 22s - loss: 0.5136 - acc: 0.7503 - val_loss: 0.4709 - val_acc: 0.7732
Epoch 25/100
100/100 [==============================] - 22s - loss: 0.4980 - acc: 0.7512 - val_loss: 0.4775 - val_acc: 0.7684
Epoch 26/100
100/100 [==============================] - 22s - loss: 0.4875 - acc: 0.7622 - val_loss: 0.4745 - val_acc: 0.7790
Epoch 27/100
100/100 [==============================] - 22s - loss: 0.5044 - acc: 0.7578 - val_loss: 0.5000 - val_acc: 0.7403
Epoch 28/100
100/100 [==============================] - 22s - loss: 0.4948 - acc: 0.7603 - val_loss: 0.4619 - val_acc: 0.7754
Epoch 29/100
100/100 [==============================] - 22s - loss: 0.4898 - acc: 0.7578 - val_loss: 0.4730 - val_acc: 0.7726
Epoch 30/100
100/100 [==============================] - 22s - loss: 0.4808 - acc: 0.7691 - val_loss: 0.4599 - val_acc: 0.7716
Epoch 31/100
100/100 [==============================] - 22s - loss: 0.4792 - acc: 0.7678 - val_loss: 0.4671 - val_acc: 0.7790
Epoch 32/100
100/100 [==============================] - 22s - loss: 0.4723 - acc: 0.7716 - val_loss: 0.4451 - val_acc: 0.7849
Epoch 33/100
100/100 [==============================] - 22s - loss: 0.4750 - acc: 0.7694 - val_loss: 0.4827 - val_acc: 0.7665
Epoch 34/100
100/100 [==============================] - 22s - loss: 0.4816 - acc: 0.7647 - val_loss: 0.4953 - val_acc: 0.7513
Epoch 35/100
100/100 [==============================] - 22s - loss: 0.4598 - acc: 0.7813 - val_loss: 0.4426 - val_acc: 0.7843
Epoch 36/100
100/100 [==============================] - 23s - loss: 0.4643 - acc: 0.7781 - val_loss: 0.4692 - val_acc: 0.7680
Epoch 37/100
100/100 [==============================] - 22s - loss: 0.4675 - acc: 0.7778 - val_loss: 0.4849 - val_acc: 0.7633
Epoch 38/100
100/100 [==============================] - 22s - loss: 0.4658 - acc: 0.7737 - val_loss: 0.4632 - val_acc: 0.7760
Epoch 39/100
100/100 [==============================] - 22s - loss: 0.4581 - acc: 0.7866 - val_loss: 0.4489 - val_acc: 0.7880
Epoch 40/100
100/100 [==============================] - 23s - loss: 0.4485 - acc: 0.7856 - val_loss: 0.4479 - val_acc: 0.7931
Epoch 41/100
100/100 [==============================] - 22s - loss: 0.4637 - acc: 0.7759 - val_loss: 0.4453 - val_acc: 0.7990
Epoch 42/100
100/100 [==============================] - 22s - loss: 0.4528 - acc: 0.7841 - val_loss: 0.4758 - val_acc: 0.7868
Epoch 43/100
100/100 [==============================] - 22s - loss: 0.4481 - acc: 0.7856 - val_loss: 0.4472 - val_acc: 0.7893
Epoch 44/100
100/100 [==============================] - 22s - loss: 0.4540 - acc: 0.7953 - val_loss: 0.4366 - val_acc: 0.7867A: 6s - loss: 0.4523 - acc: - ETA: 
Epoch 45/100
100/100 [==============================] - 22s - loss: 0.4411 - acc: 0.7919 - val_loss: 0.4708 - val_acc: 0.7697
Epoch 46/100
100/100 [==============================] - 22s - loss: 0.4493 - acc: 0.7869 - val_loss: 0.4366 - val_acc: 0.7829
Epoch 47/100
100/100 [==============================] - 22s - loss: 0.4436 - acc: 0.7916 - val_loss: 0.4307 - val_acc: 0.8090
Epoch 48/100
100/100 [==============================] - 22s - loss: 0.4391 - acc: 0.7928 - val_loss: 0.4203 - val_acc: 0.8065
Epoch 49/100
100/100 [==============================] - 23s - loss: 0.4284 - acc: 0.8053 - val_loss: 0.4422 - val_acc: 0.8041
Epoch 50/100
100/100 [==============================] - 22s - loss: 0.4492 - acc: 0.7906 - val_loss: 0.5422 - val_acc: 0.7437
Epoch 51/100
100/100 [==============================] - 22s - loss: 0.4292 - acc: 0.7953 - val_loss: 0.4446 - val_acc: 0.7932
Epoch 52/100
100/100 [==============================] - 22s - loss: 0.4275 - acc: 0.8037 - val_loss: 0.4287 - val_acc: 0.7989
Epoch 53/100
100/100 [==============================] - 22s - loss: 0.4297 - acc: 0.7975 - val_loss: 0.4091 - val_acc: 0.8046
Epoch 54/100
100/100 [==============================] - 23s - loss: 0.4198 - acc: 0.7978 - val_loss: 0.4413 - val_acc: 0.7964
Epoch 55/100
100/100 [==============================] - 23s - loss: 0.4195 - acc: 0.8019 - val_loss: 0.4265 - val_acc: 0.8001
Epoch 56/100
100/100 [==============================] - 22s - loss: 0.4081 - acc: 0.8056 - val_loss: 0.4374 - val_acc: 0.7957
Epoch 57/100
100/100 [==============================] - 22s - loss: 0.4214 - acc: 0.8006 - val_loss: 0.4228 - val_acc: 0.8020
Epoch 58/100
100/100 [==============================] - 22s - loss: 0.4050 - acc: 0.8097 - val_loss: 0.4332 - val_acc: 0.7900
Epoch 59/100
100/100 [==============================] - 22s - loss: 0.4162 - acc: 0.8134 - val_loss: 0.4088 - val_acc: 0.8099
Epoch 60/100
100/100 [==============================] - 22s - loss: 0.4042 - acc: 0.8141 - val_loss: 0.4436 - val_acc: 0.7957
Epoch 61/100
100/100 [==============================] - 23s - loss: 0.4016 - acc: 0.8212 - val_loss: 0.4082 - val_acc: 0.8189
Epoch 62/100
100/100 [==============================] - 22s - loss: 0.4167 - acc: 0.8097 - val_loss: 0.3935 - val_acc: 0.8236
Epoch 63/100
100/100 [==============================] - 23s - loss: 0.4052 - acc: 0.8138 - val_loss: 0.4509 - val_acc: 0.7824
Epoch 64/100
100/100 [==============================] - 22s - loss: 0.4011 - acc: 0.8209 - val_loss: 0.3874 - val_acc: 0.8299
Epoch 65/100
100/100 [==============================] - 22s - loss: 0.3966 - acc: 0.8131 - val_loss: 0.4328 - val_acc: 0.7970
Epoch 66/100
100/100 [==============================] - 23s - loss: 0.3889 - acc: 0.8163 - val_loss: 0.4766 - val_acc: 0.7719
Epoch 67/100
100/100 [==============================] - 22s - loss: 0.3960 - acc: 0.8163 - val_loss: 0.3859 - val_acc: 0.8325
Epoch 68/100
100/100 [==============================] - 22s - loss: 0.3893 - acc: 0.8231 - val_loss: 0.4172 - val_acc: 0.8128
Epoch 69/100
100/100 [==============================] - 23s - loss: 0.3828 - acc: 0.8219 - val_loss: 0.4023 - val_acc: 0.8215 loss: 0.3881 - acc:
Epoch 70/100
100/100 [==============================] - 22s - loss: 0.3909 - acc: 0.8275 - val_loss: 0.4275 - val_acc: 0.8008
Epoch 71/100
100/100 [==============================] - 22s - loss: 0.3826 - acc: 0.8244 - val_loss: 0.3815 - val_acc: 0.8177
Epoch 72/100
100/100 [==============================] - 22s - loss: 0.3837 - acc: 0.8272 - val_loss: 0.4040 - val_acc: 0.8287
Epoch 73/100
100/100 [==============================] - 23s - loss: 0.3812 - acc: 0.8222 - val_loss: 0.4039 - val_acc: 0.8058
Epoch 74/100
100/100 [==============================] - 22s - loss: 0.3829 - acc: 0.8281 - val_loss: 0.4204 - val_acc: 0.8015
Epoch 75/100
100/100 [==============================] - 22s - loss: 0.3708 - acc: 0.8350 - val_loss: 0.4083 - val_acc: 0.8204
Epoch 76/100
100/100 [==============================] - 22s - loss: 0.3831 - acc: 0.8216 - val_loss: 0.3899 - val_acc: 0.8215
Epoch 77/100
100/100 [==============================] - 22s - loss: 0.3695 - acc: 0.8375 - val_loss: 0.3963 - val_acc: 0.8293
Epoch 78/100
100/100 [==============================] - 22s - loss: 0.3809 - acc: 0.8234 - val_loss: 0.4046 - val_acc: 0.8236
Epoch 79/100
100/100 [==============================] - 22s - loss: 0.3637 - acc: 0.8362 - val_loss: 0.3990 - val_acc: 0.8325
Epoch 80/100
100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8400 - val_loss: 0.3925 - val_acc: 0.8350
Epoch 81/100
100/100 [==============================] - 22s - loss: 0.3762 - acc: 0.8303 - val_loss: 0.3813 - val_acc: 0.8331
Epoch 82/100
100/100 [==============================] - 23s - loss: 0.3672 - acc: 0.8347 - val_loss: 0.4539 - val_acc: 0.7931
Epoch 83/100
100/100 [==============================] - 22s - loss: 0.3636 - acc: 0.8353 - val_loss: 0.3988 - val_acc: 0.8261
Epoch 84/100
100/100 [==============================] - 22s - loss: 0.3503 - acc: 0.8453 - val_loss: 0.3987 - val_acc: 0.8325
Epoch 85/100
100/100 [==============================] - 22s - loss: 0.3586 - acc: 0.8437 - val_loss: 0.3842 - val_acc: 0.8306
Epoch 86/100
100/100 [==============================] - 22s - loss: 0.3624 - acc: 0.8353 - val_loss: 0.4100 - val_acc: 0.8196.834
Epoch 87/100
100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8422 - val_loss: 0.3814 - val_acc: 0.8331
Epoch 88/100
100/100 [==============================] - 22s - loss: 0.3487 - acc: 0.8494 - val_loss: 0.4266 - val_acc: 0.8109
Epoch 89/100
100/100 [==============================] - 22s - loss: 0.3598 - acc: 0.8400 - val_loss: 0.4076 - val_acc: 0.8325
Epoch 90/100
100/100 [==============================] - 22s - loss: 0.3510 - acc: 0.8450 - val_loss: 0.3762 - val_acc: 0.8388
Epoch 91/100
100/100 [==============================] - 22s - loss: 0.3458 - acc: 0.8450 - val_loss: 0.4684 - val_acc: 0.8015
Epoch 92/100
100/100 [==============================] - 22s - loss: 0.3454 - acc: 0.8441 - val_loss: 0.4017 - val_acc: 0.8204
Epoch 93/100
100/100 [==============================] - 22s - loss: 0.3402 - acc: 0.8487 - val_loss: 0.3928 - val_acc: 0.8204
Epoch 94/100
100/100 [==============================] - 22s - loss: 0.3569 - acc: 0.8394 - val_loss: 0.4005 - val_acc: 0.8338
Epoch 95/100
100/100 [==============================] - 22s - loss: 0.3425 - acc: 0.8494 - val_loss: 0.3641 - val_acc: 0.8439
Epoch 96/100
100/100 [==============================] - 22s - loss: 0.3335 - acc: 0.8531 - val_loss: 0.3811 - val_acc: 0.8363
Epoch 97/100
100/100 [==============================] - 22s - loss: 0.3204 - acc: 0.8581 - val_loss: 0.3786 - val_acc: 0.8331
Epoch 98/100
100/100 [==============================] - 22s - loss: 0.3250 - acc: 0.8606 - val_loss: 0.4205 - val_acc: 0.8236
Epoch 99/100
100/100 [==============================] - 22s - loss: 0.3255 - acc: 0.8581 - val_loss: 0.3518 - val_acc: 0.8460
Epoch 100/100
100/100 [==============================] - 22s - loss: 0.3280 - acc: 0.8491 - val_loss: 0.3776 - val_acc: 0.8439
</code></pre>
<p>Let’s save our model – we will be using it in the section on convnet visualization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_2.h5&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>Let’s plot our results again:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="/img/output_43_0.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p><img src="/img/output_43_1.png" srcset="/img/loading.gif" lazyload alt="output"></p>
<p>Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation<br>curves. We are now able to reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.</p>
<p>By leveraging regularization techniques even further and by tuning the network’s parameters (such as the number of filters per convolution<br>layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86-87%. However, it would prove<br>very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a<br>next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two<br>sections.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/">Deep Learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/Coding/">Coding</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/DL-code/">DL code</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/04/22/DL-with-python-6/">
                        <span class="hidden-mobile">DL with python-6</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@waline/client@0.14.8/dist/Waline.min.js', function () {
        new Waline({
          el: "#waline",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: true,
          serverURL: "http://gzzyyxh.cn",
          avatarCDN: "",
          avatarForce: true,
          requiredFields: [],
          emojiCDN: "",
          emojiMaps: {},
          anonymous: null,
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://octodex.github.com/images/jetpacktocat.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
