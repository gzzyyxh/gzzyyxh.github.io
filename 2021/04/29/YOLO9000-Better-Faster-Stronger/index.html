

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="YOLOv2模型论文及原理详解">
  <meta name="author" content="游隼">
  <meta name="keywords" content="">
  
  <title>YOLO9000: Better, Faster, Stronger - 游隼</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gzzyyxh.github.io","root":"/","version":"1.8.10","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>丰之余</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-comment"></i>
                留言板
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="YOLO9000: Better, Faster, Stronger">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-04-29 11:54" pubdate>
        2021年4月29日 中午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      55
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">YOLO9000: Better, Faster, Stronger</h1>
            
            <div class="markdown-body">
              <p>YOLOv2模型论文及原理详解</p>
<span id="more"></span>


	<div class="row">
    <embed src="./YOLOv2.pdf" width="100%" height="550" type="application/pdf">
	</div>



<h1 id="What’s-YOLOv2"><a href="#What’s-YOLOv2" class="headerlink" title="What’s YOLOv2?"></a>What’s YOLOv2?</h1><p>YOLOv2的论文全名为YOLO9000: Better, Faster, Stronger，它斩获了CVPR 2017 Best Paper Honorable Mention。在这篇文章中，作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。所以，这篇文章其实包含两个模型：YOLOv2和YOLO9000，不过后者是在前者基础上提出的，两者模型主体结构是一致的。YOLOv2相比YOLOv1做了很多方面的改进，这也使得YOLOv2的mAP有显著的提升，并且YOLOv2的速度依然很快，保持着自己作为one-stage方法的优势，YOLOv2和Faster R-CNN, SSD等模型的对比如图1所示。这里将首先介绍YOLOv2的改进策略，并给出YOLOv2的TensorFlow实现过程，然后介绍YOLO9000的训练方法。</p>
<p><img src="/img/4.29/1.png" srcset="/img/loading.gif" lazyload alt="YOLOv2与其它模型在VOC 2007数据集上的效果对比"></p>
<h1 id="YOLOv2的改进策略"><a href="#YOLOv2的改进策略" class="headerlink" title="YOLOv2的改进策略"></a>YOLOv2的改进策略</h1><p>YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图2所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。下面详细介绍各个改进策略。</p>
<p><img src="/img/4.29/2.jpg" srcset="/img/loading.gif" lazyload alt="YOLOv2相比YOLOv1的改进策略"></p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
<h2 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h2><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为<script type="math/tex">224 \times 224</script>的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用<script type="math/tex">224 \times 224</script>分类模型预训练后，将分辨率增加至<script type="math/tex">448 \times 448</script>，并使用这个高分辨率在检测数据集上finetune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用<script type="math/tex">448 \times 448</script>来finetune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
<h2 id="Convolutionlal-With-Anchor-Boxes"><a href="#Convolutionlal-With-Anchor-Boxes" class="headerlink" title="Convolutionlal With Anchor Boxes"></a>Convolutionlal With Anchor Boxes</h2><p>在YOLOv1中，输入图片最终被划分为7*7网格，每个单元格预测2个边界框。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比（scales and ratios）的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框（anchor boxes，prior boxes，SSD也采用了先验框）策略。RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体），并且各个位置设置不同尺度和比例的先验框，所以RPN预测的是边界框相对于先验框的offsets值（其实是transform值，详细见Faster R_CNN论文），采用先验框使得模型更容易学习。所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采481*418图片作为输入，而是采用416*416大小。因为YOLOv2模型下采样的总步长为32,对于416*416大小的图片，最终得到的特征图大小为13*13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。所以在YOLOv2设计中要保证最终的特征图有奇数个位置。对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值：<script type="math/tex">(x, y, w, h, c)</script>前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。</p>
<p>使用anchor boxes之后，YOLOv2的mAP有稍微下降（这里下降的原因，我猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框<script type="math/tex">(7 \times 7 \times 2)</script>，而YOLOv2使用anchor boxes之后可以预测上千个边界框<script type="math/tex">(13 \times 13 \times \text { num_anchors })</script>。所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%。</p>
<h2 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h2><p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： </p>
<script type="math/tex; mode=display">
d(\text { box, centroid })=1-I O U(\text { box }, \text { centroid })</script><p>下图为在VOC和COCO数据集上的聚类分析结果，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取5个聚类中心作为先验框，其相对于图片的大小如右边图所示。对于两个数据集，5个先验框的width和height如下所示（来源：YOLO源码的cfg文件）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">COCO: (<span class="hljs-number">0.57273</span>, <span class="hljs-number">0.677385</span>), (<span class="hljs-number">1.87446</span>, <span class="hljs-number">2.06253</span>), (<span class="hljs-number">3.33843</span>, <span class="hljs-number">5.47434</span>), (<span class="hljs-number">7.88282</span>, <span class="hljs-number">3.52778</span>), (<span class="hljs-number">9.77052</span>, <span class="hljs-number">9.16828</span>)<br>VOC: (<span class="hljs-number">1.3221</span>, <span class="hljs-number">1.73145</span>), (<span class="hljs-number">3.19275</span>, <span class="hljs-number">4.00944</span>), (<span class="hljs-number">5.05587</span>, <span class="hljs-number">8.09892</span>), (<span class="hljs-number">9.47112</span>, <span class="hljs-number">4.84053</span>), (<span class="hljs-number">11.2364</span>, <span class="hljs-number">10.0071</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/img/4.29/3.png" srcset="/img/loading.gif" lazyload alt="数据集VOC和COCO上的边界框聚类分析结果"></p>
<p>但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（13*13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。</p>
<h2 id="New-Network：Darknet-19"><a href="#New-Network：Darknet-19" class="headerlink" title="New Network：Darknet-19"></a>New Network：Darknet-19</h2><p>YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3*3卷积，采用2*2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3*3卷积之间使用1*1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p>
<p><img src="/img/4.29/4.jpg" srcset="/img/loading.gif" lazyload alt="Darknet-19模型结构"></p>
<h2 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h2><p>前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置<script type="math/tex">(x,y)</script>，需要根据预测的坐标偏移值<script type="math/tex">(t_{x},t_{y})</script>，先验框的尺度<script type="math/tex">\left(w_{a}, h_{a}\right)</script>以及中心坐标<script type="math/tex">\left(x_{a}, y_{a}\right)</script>（特征图每个位置的中心点）来计算：</p>
<script type="math/tex; mode=display">
x=\left(t_{x} \times w_{a}\right)-x_{a}, y=\left(t_{y} \times h_{a}\right)-y_{a}</script><p>但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当<script type="math/tex">t_{x}=1</script>时边界框将向右偏移先验框的一个宽度大小，而当<script type="math/tex">t_{x}=-1</script>时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets<script type="math/tex">t_{x}, t_{y}, t_{w}, t_{h}</script>，可以按如下公式计算出边界框实际位置和大小：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
b_{x}=\sigma\left(t_{x}\right)+c_{x}, b_{y}=\sigma\left(t_{y}\right)+c_{y} \\
b_{w}=p_{w} e^{t_{w}}, b_{h}=p_{h} e^{t_{h}}
\end{array}</script><p>其中<script type="math/tex">\left(c_{x}, x_{y}\right)</script>为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为<script type="math/tex">(1,1)</script>。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而<script type="math/tex">p_{w}</script>和<script type="math/tex">p_{h}</script>是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为<script type="math/tex">(W, H)</script>（在文中是<script type="math/tex">(13,13)</script>），这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
b_{x}=\left(\sigma\left(t_{x}\right)+c_{x}\right) / W, b_{y}=\left(\sigma\left(t_{y}\right)+c_{y}\right) / H \\
b_{w}=p_{w} e^{t_{w}} / W, b_{h}=p_{h} e^{t_{h}} / H
\end{array}</script><p>如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。</p>
<p><img src="/img/4.29/5.png" srcset="/img/loading.gif" lazyload alt="边界框位置与大小的计算示例图"></p>
<h2 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h2><p>YOLOv2的输入图片大小为416*416，经过5次maxpooling之后得到13*13大小的特征图，并以此特征图采用卷积做预测。13*13大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26*26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26*26*512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2*2的局部区域，然后将其转化为channel维度，对于26*26*512的特征图，经passthrough层处理之后就变成了13*13*2048的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的13*13*1024特征图连接在一起形成13*13*3072的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">out = tf.extract_image_patches(<span class="hljs-keyword">in</span>, [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], padding=<span class="hljs-string">&quot;VALID&quot;</span>)<br>// <span class="hljs-keyword">or</span> use tf.space_to_depth<br>out = tf.space_to_depth(<span class="hljs-keyword">in</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/img/4.29/6.jpg" srcset="/img/loading.gif" lazyload alt="passthrough层实例"></p>
<p>另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个1*1卷积核进行卷积，然后再进行passthrough处理，这样26*26*512的特征图得到13*13*256的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。</p>
<h2 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h2><p>由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416*416大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：<script type="math/tex">\{320,352, \ldots, 608\}</script>输入图片最小为320*320，此时对应的特征图大小为10*10（不是奇数了，确实有点尴尬），而输入图片最大为608*608,对应的特征图大小为19*19,在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。</p>
<p><img src="/img/4.29/7.jpg" srcset="/img/loading.gif" lazyload alt="Multi-Scale Training"></p>
<p>采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544,mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。</p>
<p><img src="/img/4.29/8.jpg" srcset="/img/loading.gif" lazyload alt="YOLOv2在VOC 2007数据集上的性能对比"></p>
<p>总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。</p>
<h1 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h1><p>YOLOv2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为224*224,共训练160个epochs。然后第二阶段将网络的输入调整为448*448,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个3*3*2014卷积层，同时增加了一个passthrough层，最后使用1*1卷积层输出预测结果，输出的channels数为：<script type="math/tex">\text { num_anchors } \times(5+\text { num_classes })</script>，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为T（shape为<script type="math/tex">(\text { batch_size }, 13,13,125)</script>），可以先将其reshape为<script type="math/tex">(\text { batch_size }, 13,13,5,25)</script>，其中<script type="math/tex">T[:,:,:,:, 0: 4]</script>为边界框的位置和大小，<script type="math/tex">\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$$$$T[:,:,:,:, 4]</script>为边界框的置信度，而<script type="math/tex">T[:,:,:,:, 5:]</script>为类别预测值。</p>
<p><img src="/img/4.29/9.jpg" srcset="/img/loading.gif" lazyload alt="YOLOv2训练的三个阶段"></p>
<p><img src="/img/4.29/10.jpg" srcset="/img/loading.gif" lazyload alt="YOLOv2结构示意图"></p>
<p>YOLOv2的网络结构以及训练参数我们都知道了，但是貌似少了点东西。仔细一想，原来作者并没有给出YOLOv2的训练过程的两个最重要方面，即先验框匹配（样本选择）以及训练的损失函数，难怪Ng说YOLO论文很难懂，没有这两方面的说明我们确实不知道YOLOv2到底是怎么训练起来的。不过默认按照YOLOv1的处理方式也是可以处理，我看了YOLO在TensorFlow上的实现darkflow（见yolov2/train.py），发现它就是如此处理的：和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的5个先验框所对应的边界框负责预测它，具体是哪个边界框预测它，需要在训练中确定，即由那个与ground truth的IOU最大的边界框预测它，而剩余的4个边界框不与该ground truth匹配。YOLOv2同样需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的先验框计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的边界框只计算置信度误差（此时target为0）。YOLOv2和YOLOv1的损失函数一样，为均方差函数。但是我看了YOLOv2的源码（训练样本处理与loss计算都包含在文件region_layer.c中，YOLO源码没有任何注释，反正我看了是直摇头），并且参考国外的blog以及allanzelener/YAD2K（Ng深度学习教程所参考的那个Keras实现）上的实现，发现YOLOv2的处理比原来的v1版本更加复杂。先给出loss计算公式：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
\operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} \quad 1_{\operatorname{Max} I O U<\text { Thresh }} \lambda_{\text {noobj }} *\left(-b_{i j k}^{o}\right)^{2} \\
+1_{t<12800} \lambda_{\text {prior }} * \sum_{r \in(x, y, w, h)}\left(\text { prior }_{k}^{r}-b_{i j k}^{r}\right)^{2} \\
+1_{k}^{\text {truth }}\left(\lambda_{\text {coord }} * \sum_{r \in(x, y, w, h)}\left(\text { truth }^{r}-b_{i j k}^{r}\right)^{2}\right. \\
+\lambda_{o b j} *\left(I O U_{\text {truth }}^{k}-b_{i j k}^{o}\right)^{2} \\
\left.+\lambda_{\text {class }} *\left(\sum_{c=1}^{C}\left(\text { truth }^{c}-b_{i j k}^{c}\right)^{2}\right)\right)
\end{array}</script><p>首先<script type="math/tex">W</script>,<script type="math/tex">H</script>分别指的是特征图<script type="math/tex">(13 \times 13)</script>的宽与高，而A指的是先验框数目（这里是5），各个<script type="math/tex">lambda</script>值是各个loss部分的权重系数。第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差。第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，在YOLOv1中target=1，而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的和误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale <em> (2 - truth.w</em>truth.h)，这样对于尺度较小的boxes其权重系数会更大一些，起到和YOLOv1计算平方根相似的效果（参考YOLO v2 损失函数源码分析）。</p>
<p>最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。</p>
<h1 id="YOLOv2在TensorFlow上实现"><a href="#YOLOv2在TensorFlow上实现" class="headerlink" title="YOLOv2在TensorFlow上实现"></a>YOLOv2在TensorFlow上实现</h1><p>这里参考YOLOv2在Keras上的复现（见yhcc/yolo2）,使用TensorFlow实现YOLOv2在COCO数据集上的test过程。首先是定义YOLOv2的主体网络结构Darknet-19：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet</span>(<span class="hljs-params">images, n_last_channels=<span class="hljs-number">425</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Darknet19 for YOLOv2&quot;&quot;&quot;</span><br>    net = conv2d(images, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv1&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv2&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_5&quot;</span>)<br>    shortcut = net<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool5&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_5&quot;</span>)<br>    <span class="hljs-comment"># ---------</span><br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_2&quot;</span>)<br>    <span class="hljs-comment"># shortcut</span><br>    shortcut = conv2d(shortcut, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv_shortcut&quot;</span>)<br>    shortcut = reorg(shortcut, <span class="hljs-number">2</span>)<br>    net = tf.concat([shortcut, net], axis=-<span class="hljs-number">1</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv8&quot;</span>)<br>    <span class="hljs-comment"># detection layer</span><br>    net = conv2d(net, n_last_channels, <span class="hljs-number">1</span>, batch_normalize=<span class="hljs-number">0</span>,<br>                 activation=<span class="hljs-literal">None</span>, use_bias=<span class="hljs-literal">True</span>, name=<span class="hljs-string">&quot;conv_dec&quot;</span>)<br>    <span class="hljs-keyword">return</span> net<br></code></pre></td></tr></table></figure>
<p>然后实现对Darknet-19模型输出的解码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decode</span>(<span class="hljs-params">detection_feat, feat_sizes=(<span class="hljs-params"><span class="hljs-number">13</span>, <span class="hljs-number">13</span></span>), num_classes=<span class="hljs-number">80</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">           anchors=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;decode from the detection feature&quot;&quot;&quot;</span><br>    H, W = feat_sizes<br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    detetion_results = tf.reshape(detection_feat, [-<span class="hljs-number">1</span>, H * W, num_anchors,<br>                                        num_classes + <span class="hljs-number">5</span>])<br><br>    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>])<br>    bbox_wh = tf.exp(detetion_results[:, :, :, <span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br>    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">4</span>])<br>    class_probs = tf.nn.softmax(detetion_results[:, :, :, <span class="hljs-number">5</span>:])<br><br>    anchors = tf.constant(anchors, dtype=tf.float32)<br><br>    height_ind = tf.<span class="hljs-built_in">range</span>(H, dtype=tf.float32)<br>    width_ind = tf.<span class="hljs-built_in">range</span>(W, dtype=tf.float32)<br>    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)<br>    x_offset = tf.reshape(x_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    y_offset = tf.reshape(y_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br>    <span class="hljs-comment"># decode</span><br>    bbox_x = (bbox_xy[:, :, :, <span class="hljs-number">0</span>] + x_offset) / W<br>    bbox_y = (bbox_xy[:, :, :, <span class="hljs-number">1</span>] + y_offset) / H<br>    bbox_w = bbox_wh[:, :, :, <span class="hljs-number">0</span>] * anchors[:, <span class="hljs-number">0</span>] / W * <span class="hljs-number">0.5</span><br>    bbox_h = bbox_wh[:, :, :, <span class="hljs-number">1</span>] * anchors[:, <span class="hljs-number">1</span>] / H * <span class="hljs-number">0.5</span><br><br>    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,<br>                       bbox_x + bbox_w, bbox_y + bbox_h], axis=<span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">return</span> bboxes, obj_probs, class_probs<br></code></pre></td></tr></table></figure>
<h1 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h1><p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p>
<p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p>
<p><img src="/img/4.29/11.jpg" srcset="/img/loading.gif" lazyload alt="基于COCO和ImageNet数据集建立的WordTree"></p>
<p>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。</p>
<p><img src="/img/4.29/12.jpg" srcset="/img/loading.gif" lazyload alt="ImageNet与WordTree预测的对比">在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是<script type="math/tex">Pr(physicalobject)</script>，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p>
<p>通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。我觉得这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Version/">Computer Version</a>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Version/Paper/">Paper</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/CV/">CV</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/05/02/YOLOv3-An-Incremental-Improvement/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">YOLOv3: An Incremental Improvement</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/04/27/You-Only-Look-Once-Unified-Real-Time-Object-Detection/">
                        <span class="hidden-mobile">You Only Look Once: Unified,Real-Time Object Detection</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@waline/client@0.14.8/dist/Waline.min.js', function () {
        new Waline({
          el: "#waline",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: true,
          serverURL: "http://gzzyyxh.cn",
          avatarCDN: "",
          avatarForce: true,
          requiredFields: [],
          emojiCDN: "",
          emojiMaps: {},
          anonymous: null,
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://octodex.github.com/images/jetpacktocat.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js" ></script>

  








  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
