

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="YOLOv4模型论文及原理详解">
  <meta name="author" content="游隼">
  <meta name="keywords" content="">
  
  <title>YOLOv4: Optimal Speed and Accuracy of Object Detection - 游隼</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gzzyyxh.github.io","root":"/","version":"1.8.10","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>丰之余</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-comment"></i>
                留言板
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="YOLOv4: Optimal Speed and Accuracy of Object Detection">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-05-03 00:03" pubdate>
        2021年5月3日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      10.2k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      121
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">YOLOv4: Optimal Speed and Accuracy of Object Detection</h1>
            
            <div class="markdown-body">
              <p>YOLOv4模型论文及原理详解</p>
<span id="more"></span>


	<div class="row">
    <embed src="./YOLOv4.pdf" width="100%" height="550" type="application/pdf">
	</div>



<h1 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h1><p><img src="/img/5.3/1.jpg" srcset="/img/loading.gif" lazyload alt="思维导图"></p>
<p>部分翻译：</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>有很多特征可以提高卷积神经网络（CNN）的准确性。需要在大型数据集上对这些特征的组合进行实际测试，并需要对结果进行理论证明。某些特征仅在某些模型上运行，并且仅在某些问题上运行，或者仅在小型数据集上运行；而某些特征（例如批归一化和残差连接）适用于大多数模型，任务和数据集。我们假设此类通用特征包括加权残差连接（WRC），跨阶段部分连接（CSP），交叉小批量标准化（CmBN），自对抗训练（SAT）和Mish激活。我们使用以下新功能：WRC，CSP，CmBN，SAT，Mish激活，马赛克数据增强，CmBN，DropBlock正则化和CIoU丢失，并结合其中的一些特征来实现最新的结果：在MS COCO数据集上利用Tesla V10以65 FPS的实时速度获得了43.5%的AP（65.7％AP50）。开源代码链接：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet。">https://github.com/AlexeyAB/darknet。</a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>大多数基于CNN的物体检测器仅适用于推荐系统。例如，通过慢速精确模型执行的城市摄像机搜索免费停车位，而汽车碰撞警告与快速不精确模型有关。提高实时物体检测器的精度不仅可以将它们用于提示生成推荐系统，还可以用于独立的过程管理和减少人工输入。常规图形处理单元（GPU）上的实时对象检测器操作允许以可承受的价格对其进行运行。最精确的现代神经网络无法实时运行，需要使用大量的GPU进行大量的mini-batch-size训练。我们通过创建在常规GPU上实时运行的CNN来解决此类问题，并且该培训仅需要一个传统的GPU。</p>
<p>这项工作的主要目标是在产生式系统中设计一个运行速度快的目标探测器，并对并行计算进行优化，而不是设计一个低计算量的理论指标(BFLOP)。我们希望所设计的对象易于训练和使用。如图1中的YOLOv4结果所示，任何人使用传统的GPU进行训练和测试，都可以获得实时、高质量和令人信服的目标检测结果。我们的贡献概括如下：</p>
<ol>
<li>我们开发了一个高效且功能强大的目标检测模型。它使每个人都可以使用1080Ti或2080TiGPU来训练超快、精确的物体探测器。</li>
<li>我们验证了当前最新的“免费袋”和“特殊袋”检测方法在探测器训练过程中的影响。</li>
<li>我们修改了最新的方法，使其更有效，更适合于单个GPU的培训，包括CBN[89]、PAN[49]、SAM[85]等。</li>
</ol>
<p><img src="/img/5.3/2.png" srcset="/img/loading.gif" lazyload alt="MS COCO Object Detection"></p>
<h2 id="相关工作（Related-work）"><a href="#相关工作（Related-work）" class="headerlink" title="相关工作（Related work）"></a>相关工作（Related work）</h2><h3 id="目标检测算法（Object-detection-models）"><a href="#目标检测算法（Object-detection-models）" class="headerlink" title="目标检测算法（Object detection models）"></a>目标检测算法（Object detection models）</h3><p>目标检测算法一般有两部分组成：一个是在ImageNet预训练的骨架（backbone），，另一个是用来预测对象类别和边界框的头部。对于在GPU平台上运行的检测器，其骨干可以是VGG [68]，ResNet [26]，ResNeXt [86]或DenseNet [30]。对于在CPU平台上运行的那些检测器，其主干可以是SqueezeNet [31]，MobileNet [28、66、27、74]或ShuffleNet [97、53]。对于头部，通常分为两类，即一级对象检测器和二级对象检测器。最具有代表性的两级对象检测器是R-CNN [19]系列，包括fast R-CNN [18]，faster R-CNN [64]，R-FCN [9]和Libra R-CNN [ 58]。还可以使两级对象检测器成为无锚对象检测器，例如RepPoints [87]。对于一级目标检测器，最具代表性的模型是YOLO [61、62、63]，SSD [50]和RetinaNet [45]。近年来，开发了无锚的（anchor free）一级物体检测器。这类检测器是CenterNet [13]，CornerNet [37、38]，FCOS [78]等。近年来，无锚点单级目标探测器得到了发展，这类探测器有CenterNet[13]、CornerNet[37，38]、FCOS[78]等。近年来发展起来的目标探测器经常在主干和头部之间插入一些层，这些层通常用来收集不同阶段的特征图。我们可以称它为物体探测器的颈部。通常，颈部由几个自下而上的路径和几个自上而下的路径组成。具有这种机制的网络包括特征金字塔网络(FPN)[44]、路径聚集网络(PAN)[49]、BiFPN[77]和NAS-FPN[17]。除上述模型外，一些研究人员将重点放在直接构建用于检测对象的新主干（DetNet [43]，DetNAS [7]）或新的整个模型（SpineNet [12]，HitDe-tector [20]）上。</p>
<p>总而言之，普通的检测器由以下几个部分组成：</p>
<ul>
<li>输入：图像，斑块，图像金字塔</li>
<li>骨架：VGG16 [68]，ResNet-50 [26]，SpineNet [12]，EfficientNet-B0 / B7 [75]，CSPResNeXt50 [81]，CSPDarknet53 [81]</li>
<li><p>颈部：</p>
<ul>
<li>其他块：SPP [25]，ASPP [5]，RFB [47]，SAM [85]</li>
<li>路径聚合块：FPN [44]，PAN [49]，NAS-FPN [17] ]，Fully-connected FPN，BiFPN [77]，ASFF [48]，SFAM [98]</li>
</ul>
</li>
<li><p>Heads ：</p>
<ul>
<li>RPN[64]，SSD [50]，YOLO [61]， RetinaNet [45]（基于锚）</li>
<li>CornerNet[37]，CenterNet [13]，MatrixNet [60]，FCOS [78]（无锚）</li>
<li>密集预测（一阶段）：</li>
</ul>
</li>
<li><p>稀疏预测（两阶段）：</p>
<ul>
<li>Faster R-CNN [64]，R-FCN [9]，Mask R-CNN [23]（基于锚）</li>
<li>RepPoints[87]（无锚）</li>
</ul>
</li>
</ul>
<p><img src="/img/5.3/3.png" srcset="/img/loading.gif" lazyload alt="Two-Stage Detector"></p>
<h3 id="Bag-of-freebies"><a href="#Bag-of-freebies" class="headerlink" title="Bag of freebies"></a>Bag of freebies</h3><p>通常，传统的物体检测器是离线训练的。因此，研究人员一直喜欢采用这种优势并开发出更好的训练方法，从而可以使目标检测器获得更好的精度而又不会增加推理成本。我们称这些仅改变培训策略或仅增加培训成本的方法为“免费赠品”。数据检测是对象检测方法经常采用的并符合免费赠品的定义。数据增强的目的是增加输入图像的可变性，从而使设计的物体检测模型对从不同环境获得的图像具有更高的鲁棒性。例如，光度失真和几何失真是两种常用的数据增强方法，它们无疑有利于物体检测任务。在处理光度失真时，我们调整图像的亮度，对比度，色相，饱和度和噪点。对于几何失真，我们添加了随机缩放，裁剪，翻转和旋转。</p>
<p>上面提到的数据增强方法是全像素调整，并且保留了调整区域中的所有原始像素信息。此外，一些从事数据增强的研究人员将重点放在模拟对象遮挡问题上。他们在图像分类和目标检测方面取得了良好的成果。例如，random erase[100]和CutOut [11]可以随机选择图像中的矩形区域，并填充零的随机或互补值。至于hide-and-seek[69]和grid mask[6]，他们随机或均匀地选择图像中的多个矩形区域，并将其替换为所有零。如果类似的概念应用于要素地图，则有DropOut [71]，DropConnect [80]和DropBlock [16]方法。另外，一些研究人员提出了使用多个图像一起执行数据增强的方法。例如，MixUp [92]使用两个图像对具有不同系数的图像进行乘法和叠加，然后使用这些叠加的系数来调整标签。对于CutMix [91]，它是将裁切后的图像覆盖到其他图像的矩形区域，并根据混合区域的大小调整标签。除了上述方法之外，样式转移GAN [15]还用于数据扩充，这种用法可以有效地减少CNN所学习的纹理偏差。</p>
<p>与上面提出的各种方法不同，其他一些免费赠品方法专用于解决数据集中语义分布可能存在偏差的问题。在处理语义分布偏向问题时，一个非常重要的问题是不同类之间存在数据不平衡的问题，这一问题通常是通过两阶段对象设计器中的难例挖掘[72]或在线难例挖掘[67]来解决的。但是实例挖掘方法不适用于一级目标检测器，因为这种检测器属于密集预测架构。因此Linet等 [45]提出了焦点损失，以解决各个类别之间存在的数据不平衡问题。另一个非常重要的问题是，很难用one-hot representation来表达不同类别之间的关联度。这种表示方法经常在执行标签时使用。[73]中提出的标签平滑是将硬标签转换为软标签以进行训练，这可以使模型更加健壮。为了获得更好的软标签，Islamet等人[33]引入知识蒸馏的概念来设计标签优化网络。</p>
<p>最后一袋赠品是边界框回归的目标函数。传统的目标检测器通常使用均方误差(MSE)直接对BBox的中心点坐标和高度、宽度进行回归，即{xcenter，ycenter，w，h}，或者对左上角和右下点，即{xtopleft，ytopleft，xbottomright，ybottomright}进行回归。对于基于锚点的方法，是估计相应的偏移量，例如{xcenterOffset，ycenterOffset，wOffset，hoffset}和{xtopleftoffset，ytopleftoffset，xbottomright toffset，ybottomright toffset}，例如{xtopleftoffset，ytopleftoffset}和{xtopleftoffset，ytopleftoffset}。然而，直接估计BBox中每个点的坐标值是将这些点作为独立变量来处理，而实际上并没有考虑对象本身的完整性。为了更好地处理这一问题，最近一些研究人员提出了IoU loss[90]，将预测BBOX区域的覆盖率和地面真实BBOX区域的覆盖率考虑在内。IOU loss计算过程将触发BBOX的四个坐标点的计算，方法是执行具有地面实况的借条，然后将生成的结果连接到一个完整的代码中。由于IOU是一种标度不变的表示，它可以解决传统方法计算{x，y，w，h}的l1或l2个损失时，损失会随着尺度的增大而增大的问题。最近，搜索者不断改善欠条损失。例如，GIOU损失[65]除了包括覆盖区域外，还包括对象的形状和方向。他们提出找出能同时覆盖预测BBOX和实际BBOX的最小面积BBOX，并用这个BBOX作为分母来代替原来在欠条损失中使用的分母。对于DIoU loss[99]，它另外考虑了物体中心的距离，而CIoU损失[99]则同时考虑了重叠面积、中心点之间的距离和纵横比。在求解BBox回归问题时，Ciou可以达到较好的收敛速度和精度。</p>
<h3 id="Bag-of-spedials"><a href="#Bag-of-spedials" class="headerlink" title="Bag of spedials"></a>Bag of spedials</h3><p>对于那些仅增加少量推理成本但可以显着提高对象检测准确性的插件模块和后处理方法，我们将其称为“特价商品”。一般而言，这些插件模块用于增强模型中的某些属性，例如扩大接受域，引入注意力机制或增强特征集成能力等，而后处理是用于筛选模型预测结果的方法。</p>
<p>可以用来增强接收域的通用模块是SPP [25]，ASPP [5]和RFB [47]。SPP模块起源于空间金字塔匹配（SPM）[39]，SPM的原始方法是将功能图分割成若干x不等的块，其中{1,2,3，…}可以是空间金字塔，然后提取词袋特征。SPP将SPM集成到CNN中，并使用最大池操作而不是词袋运算。由于Heet等人提出了SPP模块。[25]将输出一维特征向量，在全卷积网络（FCN）中应用是不可行的。因此，在YOLOv3的设计中[63]，Redmon和Farhadi将SPP模块改进为内核大小为k×k的最大池输出的串联，其中k = {1,5,9,13}，步长等于1。在这种设计下，相对大k×kmax池有效地增加了骨干特征的接受范围。在添加了改进版本的SPP模块之后，YOLOv3-608在MS COCOobject检测任务上将AP50升级了2.7％，而额外的计算费用为0.5％。ASPP[5]模块和改进的SPP模块之间的操作差异主要来自于原始k× kkernel大小，最大卷积步长等于1到3×3内核大小，膨胀比等于tok，步长等于1。RFB模块将使用k×kkernel的几个扩张卷积，扩张比率equalstok和步幅等于1来获得比ASPP更全面的空间覆盖范围。RFB [47]仅花费7％的额外推断时间即可将MS COCO上SSD的AP50提高5.7％。</p>
<p>物体检测中常用的注意模块主要分为通道式注意和点式注意，这两种注意模型的代表分别是挤压激发(SE)[29]和空间注意模块(SAM)[85]。虽然SE模块在Im-ageNet图像分类任务中可以提高1%的TOP-1准确率，但是在GPU上通常会增加10%左右的推理时间，因此更适合在移动设备上使用，虽然SE模块在Im-ageNet图像分类任务中可以提高1%的TOP-1准确率，但是在GPU上通常会增加10%左右的推理时间。而对于SAM，它只需要额外支付0.1%的计算量，在ImageNet图像分类任务上可以提高ResNet50-SE 0.5%的TOP-1准确率。最棒的是，它完全不影响GPU上的推理速度。</p>
<p>在特征集成方面，早期的实践是使用KIP连接[51]或超列[22]将低级物理特征集成到高级语义特征。随着模糊神经网络等多尺度预测方法的普及，人们提出了许多集成不同特征金字塔的轻量级模块。这种类型的模块包括SfAM[98]、ASFF[48]和BiFPN[77]。SfAM的主要思想是使用SE模块对多比例尺拼接的特征地图进行通道级的加权。ASFF采用Softmax作为逐点层次加权，然后添加不同尺度的特征地图；BiFPN采用多输入加权残差连接进行尺度层次重新加权，再添加不同尺度的特征地图。</p>
<p>在深度学习的研究中，有些人专注于寻找良好的激活功能。良好的激活函数可以使梯度更有效地传播，同时不会引起过多的计算成本。在2010年，Nair和Hin-ton [56]提出了ReLU，以基本上解决传统tanh和sigmoid激活函数中经常遇到的梯度消失问题。随后，LReLU [54]，PReLU [24]，ReLU6 [28]，比例指数线性单位（SELU）[35]，Swish [59]，hard-Swish [27]和Mish [55]等，它们也是已经提出了用于解决梯度消失问题的方法。LReLU和PReLU的主要目的是解决当输出小于零时ReLU的梯度为零的问题。至于ReLU6和hard-Swish，它们是专门为量化网络设计的。为了对神经网络进行自归一化，提出了SELU激活函数来满足这一目标。要注意的一件事是，Swish和Mishare都具有连续可区分的激活功能。</p>
<p>在基于深度学习的对象检测中通常使用的后处理方法是NMS，它可以用于过滤那些无法预测相同对象的BBox，并仅保留具有较高响应速度的候选BBox。NMS尝试改进的方法与优化目标函数的方法一致。NMS提出的原始方法没有考虑上下文信息，因此Girshicket等人。[19]在R-CNN中添加了分类置信度得分作为参考，并且根据置信度得分的顺序，从高分到低分的顺序执行贪婪的NMS。对于软网络管理系统[1]，考虑了一个问题，即物体的遮挡可能会导致带有IoU评分的贪婪的网络管理系统的置信度得分下降。DIoU NMS [99]开发人员的思维方式是在softNMS的基础上将中心距离的信息添加到BBox筛选过程中。值得一提的是，由于上述后处理方法均未直接涉及捕获的图像功能，因此在随后的无锚方法开发中不再需要后处理。</p>
<h2 id="方法（Methodology）"><a href="#方法（Methodology）" class="headerlink" title="方法（Methodology）"></a>方法（Methodology）</h2><p>其基本目标是在生产系统中对神经网络进行快速操作，并针对并行计算进行优化，而不是低计算量理论指示器（BFLOP）。我们提供了实时神经网络的两种选择：</p>
<ul>
<li>对于GPU，我们使用少量的组（1-8）卷积层：CSPResNeXt50 / CSPDarknet53</li>
<li>对于VPU-我们使用分组卷积，但是我们不再使用挤压和激发（SE）块-特别是这包括以下模型：EfficientNet-lite / MixNet [76] / GhostNet [21] / Mo-bileNetV3</li>
</ul>
<h3 id="Selection-of-architecture"><a href="#Selection-of-architecture" class="headerlink" title="Selection of architecture"></a>Selection of architecture</h3><p>我们的目标是在输入网络分辨率，卷积层数，参数数（filtersize2 过滤器通道/组）和层输出（过滤器）数目之间找到最佳平衡。例如，大量研究表明，在ILSVRC2012（ImageNet）数据集的对象分类方面，CSPResNext50比CSPDarknet53更好。然而，相反，就检测MS COCO数据集上的对象而言，CSPDarknet53比CSPResNext50更好。</p>
<p>下一个目标是为不同的检测器级别从不同的主干级别中选择其他块以增加接收场和参数聚集的最佳方法：FPN，PAN，ASFF，BiFPN。</p>
<p>对于分类最佳的参考模型对于检测器并非总是最佳的。与分类器相比，检测器需要满足以下条件：</p>
<ul>
<li>更大的网络输入，用于检测小目标</li>
<li>更多的层-以获得更大的感受野来覆盖增大的输入图像</li>
<li>更多的参数-为了增强从单张图像中检测出不同大小的多个对象的能力</li>
</ul>
<p>假设可以选择接受场较大(卷积层数为3×3)和参数数较多的模型作为主干。表1显示了CSPResNeXt50、CSPDarknet53和Effi-cientNet B3的信息。CSPResNext50只包含16个卷积层3×3，425×425感受野和20.6M参数，而CSPDarknet53包含29个卷积层3×3，725×725感受野和27.6M参数。这一理论证明，再加上我们的大量实验，表明CSPDarknet53神经网络是<strong>两者作为探测器骨干的最佳模型</strong>。</p>
<p><img src="/img/5.3/4.png" srcset="/img/loading.gif" lazyload alt="Parameters of neural networks for image classification"></p>
<p>不同大小的感受野对检测效果的影响如下所示：</p>
<ul>
<li>最大对象大小-允许查看整个对象</li>
<li>最大网络大小-允许查看对象周围的上下文</li>
<li>超过网络大小-增加图像点和最终激活之间的连接</li>
</ul>
<p>我们在CSPDarknet53上添加SPP块，因为它显著增加了接受场，分离出最重要的上下文特征，并且几乎不会降低网络操作速度。我们使用PANET代替YOLOv3中使用的FPN作为不同骨级的参数聚合方法，用于不同的检测器级别。</p>
<p>最后，我们选择了CSPDarknet53主干、SPP附加模块、PANET路径聚合Neck和YOLOv3(基于锚点的)头部作为YOLOv4的体系结构。</p>
<p>将来，我们计划显着扩展检测器的赠品袋（BoF）的内容，从理论上讲，它可以解决一些问题并提高检测器的准确性，并以实验方式依次检查每个功能的影响。</p>
<p>我们不使用跨GPU批量标准化（CGBNor SyncBN）或昂贵的专用设备。这使任何人都可以在传统的图形处理器上重现我们的最新成果，例如GTX 1080Ti或RTX2080Ti。</p>
<h3 id="Selection-of-BoF-and-BoS"><a href="#Selection-of-BoF-and-BoS" class="headerlink" title="Selection of BoF and BoS"></a>Selection of BoF and BoS</h3><p>为了改进目标检测训练，CNN通常使用以下方法:</p>
<ul>
<li>激活：ReLU，leaky-ReLU，parameter-ReLU，ReLU6，SELU，Swish或Mish</li>
<li>边界框回归损失：MSE，IoU，GIoU，CIoU，DIoU</li>
<li>数据增强：CutOut，MixUp，CutMix</li>
<li>正则化方法：DropOut， DropPath [36]，Spatial DropOut [79]或DropBlock</li>
<li>通过均值和方差对网络激活进行归一化：Batch Normalization (BN) [32],Cross-GPU Batch Normalization (CGBN or SyncBN)[93], Filter Response Normalization (FRN) [70], orCross-Iteration Batch Normalization (CBN) [89]</li>
<li>跨连接：Residual connections, Weightedresidual connections, Multi-input weighted residualconnections, or Cross stage partial connections (CSP)</li>
</ul>
<p>至于训练激活功能，由于PReLU和SELU更难训练，并且ReLU6是专门为量化网络设计的，因此我们从候选列表中删除了上述激活功能。在重新量化方法中，发布Drop-Block的人们将自己的方法与其他方法进行了详细的比较，而其正则化方法赢得了很多。因此，我们毫不犹豫地选择了DropBlock作为我们的正则化方法。至于标准化方法的选择，由于我们专注于仅使用一个GPU的训练策略，因此不考虑syncBN。</p>
<h3 id="Additional-improvements"><a href="#Additional-improvements" class="headerlink" title="Additional improvements"></a>Additional improvements</h3><p>为了使设计的检测器更适合在单个GPU上进行训练，我们进行了以下附加设计和改进：</p>
<ul>
<li>我们引入了一种新的数据增强方法：Mosaic, and Self-Adversarial Training (SAT)</li>
<li>在应用遗传算法时，我们选择最优的超参数</li>
<li>我们修改了一些现有方法，使我们的设计适合进行有效的训练和检测-改进的SAM，改进的PAN ，以及跨小批量标准化（CmBN）</li>
</ul>
<p>Mosaic是一种新的混合4幅训练图像的数据增强方法。所以四个不同的上下文信息被混合，而CutMix只混合了2种。</p>
<p>这允许检测其正常上下文之外的对象。此外，批量归一化从每层上的4个不同的图像计算激活统计。这极大地减少了对large mini-batch-size的需求。</p>
<p>自对抗训练(SAT)也代表了一种新的数据增强技术，它在两个前向后向阶段运行。在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对其自身执行对抗性攻击，改变原始图像，以制造图像上没有所需对象的欺骗。在第二阶段，训练神经网络，以正常的方式在修改后的图像上检测目标。</p>
<p>CmBN表示CBN的修改版本，如图4所示，定义为交叉小批量规范化(Cross mini-Batch Normalization，CMBN)。这仅在单个批次内的小批次之间收集统计信息。</p>
<p><img src="/img/5.3/5.png" srcset="/img/loading.gif" lazyload alt="Cross mini-Batch Normalization"></p>
<p>我们将SAM从空间注意修改为点注意，并将PAN的快捷连接分别替换为串联:</p>
<p><img src="/img/5.3/6.png" srcset="/img/loading.gif" lazyload alt="Modified SAN"></p>
<h3 id="YOLOv4"><a href="#YOLOv4" class="headerlink" title="YOLOv4"></a>YOLOv4</h3><p>YOLOv4的组成：</p>
<ul>
<li>Backbone:CSPDarknet53[81]</li>
<li>Neck:SPP[25],PAN[49]</li>
<li>Head:YOLOv3[63]</li>
</ul>
<p>YOLOv4的使用：</p>
<p><img src="/img/5.3/7.png" srcset="/img/loading.gif" lazyload alt="YOLOv4 uses"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>我们在ImageNet(ILSVRC 2012 Val)数据集上测试了不同训练改进技术对分类器精度的影响，然后在MS Coco(test-dev 2017)数据集上测试了不同训练改进技术对检测器精度的影响。</p>
<h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p>在ImageNet图像分类实验中，缺省超参数如下：训练步数为800万步；batch size和mini-batch size分别为128和32；采用多项式衰减学习率调度策略，初始学习率为0.1；预热步数为1000步；动量和权值分别设置为0.9和0.005。我们所有的BoS实验都使用与默认设置相同的超参数，并且在BoF实验中，我们增加了50%的训练步骤。在BoF实验中，我们验证了MixUp、CutMix、Mosaic、Bluring数据增强和标记平滑正则化方法。在BoS实验中，我们比较了LReLU、SWISH和MISHISH激活函数的效果。所有实验均使用1080Ti或2080Ti GPU进行训练。</p>
<p>在MS COCO目标检测实验中，缺省超参数如下：训练步数为500,500；采用阶跃衰减学习率调度策略，初始学习率为0.01，在400，000步和450，000步分别乘以因子0.1；动量衰减和权重衰减分别设置为0.9和0.0005。所有的体系结构都使用单个GPU来执行批处理大小为64的多尺度训练，而小批处理大小为8或4，具体取决于体系结构和GPU内存的限制。除采用遗传算法进行超参数搜索实验外，其余实验均采用默认设置。遗传算法使用YOLOv3-SPP算法在有GIoU损失的情况下进行训练，在300个历元中搜索Min-Val5k集。我们采用搜索学习率0.00261，动量0.949，IOU阈值分配地面真值0.213，损失归一化0.07%进行遗传算法实验。我们验证了大量的BoF算法，包括网格敏感度消除、moSAIC数据增强、IOU阈值、遗传算法、类标签平滑、交叉小批量归一化、自对抗训练、余弦退火调度器、动态小批量大小、DropBlock、优化锚点、不同类型的IOU损失。我们还在不同的BoS上进行了实验，包括MISH、SPP、SAM、RFB、BiFPN和Gaus-Sian YOLO[8]。对于所有的实验，我们只使用一个GPU进行训练，因此不使用诸如优化多个GPU的syncBN之类的技术。</p>
<h3 id="Influence-of-different-features-on-Classifier-training"><a href="#Influence-of-different-features-on-Classifier-training" class="headerlink" title="Influence of different features on Classifier training"></a>Influence of different features on Classifier training</h3><p>首先，我们研究了不同特征对分类器训练的影响；具体地说，类标签平滑的影响，不同数据扩充技术的影响，双边模糊，混合，CutMix和马赛克，如图7所示，以及不同活动的影响，如Leaky-relu(默认情况下)，SWISH和MISH。</p>
<p><img src="/img/5.3/8.png" srcset="/img/loading.gif" lazyload alt="Various method of data augmentation"></p>
<p>在我们的实验中，如表2所示，通过引入诸如：CutMix和Mosaic数据增强、类标签平滑和Mish激活等特征，提高了分类器的精度。因此，我们用于分类器训练的BoF-Backbone(Bag Of Freebies)包括以下内容：CutMix和Mosaic数据增强以及类标签平滑。此外，我们还使用MISH激活作为补充选项，如表2和表3所示。</p>
<p><img src="/img/5.3/9.png" srcset="/img/loading.gif" lazyload alt="Influence of BoF and Mish on the CSPResNeXt-50 classifier accuracy"></p>
<h3 id="Influence-of-different-features-on-Detector-training"><a href="#Influence-of-different-features-on-Detector-training" class="headerlink" title="Influence of different features on Detector training"></a>Influence of different features on Detector training</h3><p>进一步的研究涉及不同的免费袋(BOF探测器)对探测器训练精度的影响，如表4所示。我们通过研究在不影响FPS的情况下提高探测器精度的不同特性，显著扩展了BOF列表：</p>
<p><img src="/img/5.3/10.png" srcset="/img/loading.gif" lazyload alt="BOF列表"></p>
<p>进一步研究了不同的专业袋(BOS检测器)对检测器训练精度的影响，包括PAN、RFB、SAM、高斯YOLO(G)和ASFF，如表5所示。在我们的实验中，当使用SPP、PAN和SAM时，检测器的性能最佳。</p>
<p><img src="/img/5.3/11.png" srcset="/img/loading.gif" lazyload alt="Ablation Studies of Bag-of-Freebies."></p>
<h3 id="Influence-of-different-backbones-and-pretrained-weightings-on-Detector-training"><a href="#Influence-of-different-backbones-and-pretrained-weightings-on-Detector-training" class="headerlink" title="Influence of different backbones and pretrained weightings on Detector training"></a>Influence of different backbones and pretrained weightings on Detector training</h3><p>进一步，我们研究了不同主干模型对检测器精度的影响，如表6所示。请注意，具有最佳分类精度的模型在检测器精度方面并不总是最佳。</p>
<p><img src="/img/5.3/12.png" srcset="/img/loading.gif" lazyload alt="Using different classifier pre-trained weightings for detector training"></p>
<p>首先，尽管与CSPDarknet53模型相比，经过不同功能训练的CSPResNeXt-50模型的分类准确性更高，但CSPDarknet53模型在对象检测方面显示出更高的准确性。</p>
<p>其次，使用BoF和Mish进行CSPResNeXt50分类器训练会提高其分类准确性，但是将这些预先训练的权重进一步应用于检测器训练会降低检测器准确性。但是，将BoF和Mish用于CSPDarknet53分类器训练可以提高分类器和使用该分类器预训练加权的检测器的准确性。结果是，与CSPResNeXt50相比，主干CSPDarknet53更适合于检测器。</p>
<p>我们观察到，由于各种改进，CSPDarknet53模型具有更大的能力来提高检测器精度。</p>
<h3 id="Influence-of-different-mini-batch-size-on-Detector-training"><a href="#Influence-of-different-mini-batch-size-on-Detector-training" class="headerlink" title="Influence of different mini-batch size on Detector training"></a>Influence of different mini-batch size on Detector training</h3><p>最后，我们分析了在不同最小批量大小下训练的模型所获得的结果，结果如表7所示。从表7所示的结果中，我们发现在添加BoF和BoS训练策略之后，最小批量大小几乎没有影响在检测器的性能上。该结果表明，在引入BoF和BoS之后，不再需要使用昂贵的GPU进行训练。换句话说，任何人都只能使用传统的GPU来训练出色的探测器。</p>
<p><img src="/img/5.3/13.png" srcset="/img/loading.gif" lazyload alt="Using defferernt mini-batch size for detector training"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>与其他最先进的物体探测器获得的结果比较如图8所示。YOLOv4在速度和准确性方面均优于最快，为最准确的探测器。</p>
<p><img src="/img/5.3/14.png" srcset="/img/loading.gif" lazyload alt="Comparison of the speed and accuracy of differert object detectors"></p>
<p>由于不同的方法使用不同体系结构的GPU进行推理时间验证，我们在常用的Maxwell、Pascal和VoltaArchitecture体系结构的GPU上运行YOLOv4，并将它们与其他先进的方法进行了比较。表8列出了使用Maxwell GPU的帧率比较结果，可以是GTX Titan X(Maxwell)或Tesla M40 GPU。表9列出了使用Pascal GPU的帧率比较结果，它可以是Titan X(Pascal)、Titan XP、GTX 1080 Ti或Tesla P100 GPU。表10列出了使用VoltaGPU的帧率对比结果，可以是Titan Volta，也可以是Tesla V100 GPU。</p>
<p><img src="/img/5.3/15.png" srcset="/img/loading.gif" lazyload alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p>
<p><img src="/img/5.3/16.png" srcset="/img/loading.gif" lazyload alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p>
<p><img src="/img/5.3/17.png" srcset="/img/loading.gif" lazyload alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>我们提供最先进的检测器，其速度（FPS）和准确度（MS COCO AP50 … 95和AP50）比所有可用的替代检测器都高。所描述的检测器可以在具有8-16GB-VRAM的常规GPU上进行训练和使用，这使得它的广泛使用成为可能。一阶段基于锚的探测器的原始概念已证明其可行性。我们已经验证了大量特征，并选择使用其中的一些特征以提高分类器和检测器的准确性。这些功能可以用作将来研究和开发的最佳实践。</p>
<h2 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h2><p>作者要感谢Glenn Jocher进行Mosaic数据增强的想法，通过使用遗传算法选择超参数并解决网格敏感性问题的方法<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov3.10。">https://github.com/ultralytics/yolov3.10。</a></p>
<h1 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h1><h2 id="YOLOv4结构"><a href="#YOLOv4结构" class="headerlink" title="YOLOv4结构"></a>YOLOv4结构</h2><p><img src="/img/5.3/18.png" srcset="/img/loading.gif" lazyload alt="YOLOv4"></p>
<p>YOLOv4 consists of:</p>
<ul>
<li>Backbone:CSPDarknet53[81]</li>
<li>Neck:SPP[25],PAN[49]</li>
<li>Head:YOLOv3[63]</li>
</ul>
<p>上图为论文原图，形象将模型分为四部分：input，Backbone、Neck、Head</p>
<h3 id="结构讲解："><a href="#结构讲解：" class="headerlink" title="结构讲解："></a>结构讲解：</h3><p>假设输入图像大小为416×416，可以看到主干网络Darknet53–&gt;CSPDarknet53，激活函数由yolov3的Leak-Relu–&gt;Mish，Mish激活函数公式与图像如下：</p>
<p><img src="/img/5.3/19.png" srcset="/img/loading.gif" lazyload alt="Mish"></p>
<p>分析一下网络结构：</p>
<ol>
<li>主干网络Backbone</li>
</ol>
<ul>
<li>首先是输入层输入(416，416，3)的图，经过DarknetConv2D_BN_Mish（图中的Conv2d_BN_Mish）卷积块，卷积块由DarknetConv2D、BatchNormalization（BN）、Mish三部分组成，即该卷积结构包括l2正则化、批标准化、Mish激活函数这一系列操作。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   单次卷积</span><br><span class="hljs-meta">@wraps(<span class="hljs-params">Conv2D</span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    darknet_conv_kwargs = &#123;<span class="hljs-string">&#x27;kernel_regularizer&#x27;</span>: l2(<span class="hljs-number">5e-4</span>)&#125;<br>    darknet_conv_kwargs[<span class="hljs-string">&#x27;padding&#x27;</span>] = <span class="hljs-string">&#x27;valid&#x27;</span> <span class="hljs-keyword">if</span> kwargs.get(<span class="hljs-string">&#x27;strides&#x27;</span>)==(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;same&#x27;</span><br>    darknet_conv_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> Conv2D(*args, **darknet_conv_kwargs)<br><br><span class="hljs-comment">#   卷积块 DarknetConv2D + BatchNormalization + Mish</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D_BN_Mish</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    no_bias_kwargs = &#123;<span class="hljs-string">&#x27;use_bias&#x27;</span>: <span class="hljs-literal">False</span>&#125;<br>    no_bias_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> compose(<br>        DarknetConv2D(*args, **no_bias_kwargs),<br>        BatchNormalization(),<br>        Mish())<br></code></pre></td></tr></table></figure>
<ul>
<li>然后后面有5个CSPResblockBody，分别为×1，×2，×8，×8，×4，表示重复多少次resblock_body，可以看到该残差结构块中先有个ZeroPadding2D对二维矩阵的四周填充0，即零填充层，然后三个DarknetConv2D_BN_Mish卷积块，第二个DarknetConv2D_BN_Mish卷积块会生成一个大的残差边（后面会解释残差边），然后有个for循环对通道进行整合[compose]与特征提取，这个循环多少次就是图中×1，×2，×8等。，在后面进行一次1x1DarknetConv2D_BN_Mish卷积，再concatenate堆叠拼接，最后再对通道数进行整合并返回。可以看到，resblockbody中有大量的DarknetConv2D_BN_Mish卷积块进行卷积提取特征。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   CSPdarknet的结构块</span><br><span class="hljs-comment">#   存在一个大残差边</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resblock_body</span>(<span class="hljs-params">x, num_filters, num_blocks, all_narrow=<span class="hljs-literal">True</span></span>):</span><br>    <span class="hljs-comment"># 进行长和宽的压缩</span><br>    preconv1 = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(x)<br>    preconv1 = DarknetConv2D_BN_Mish(num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(preconv1)<br><br>    <span class="hljs-comment"># 生成一个大的残差边 </span><br>    shortconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(preconv1)<br><br>    <span class="hljs-comment"># 主干部分的卷积</span><br>    mainconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(preconv1)<br>    <span class="hljs-comment"># 1x1卷积对通道数进行整合-&gt;3x3卷积提取特征，使用残差结构</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks):<br>        y = compose(<br>                DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)))(mainconv)<br>        mainconv = Add()([mainconv,y])<br>    <span class="hljs-comment"># 1x1卷积后和残差边堆叠</span><br>    postconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(mainconv)<br>    route = Concatenate()([postconv, shortconv])<br><br>    <span class="hljs-comment"># 最后对通道数进行整合</span><br>    <span class="hljs-keyword">return</span> DarknetConv2D_BN_Mish(num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(route)<br></code></pre></td></tr></table></figure>
<p>CSPDarknet（yolov4）与Darknet（yolov3）的resbodyblock不一样地方就是使用了CSPnet结构</p>
<p><img src="/img/5.3/20.png" srcset="/img/loading.gif" lazyload alt="net结构"></p>
<p>CSPnet结构将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：<br>主干部分还是进行残差快的堆叠（Part 2）<br>另一部分则像是一条长边一样，经过少量处理直接连接到最后。<br>因此可以认为CSP中存在一个大的残差边。<br>残差边的作用：残差结构可以不通过卷积与堆叠，直接从前面一个特征层映射到后面的特征层（跳跃连接），有助于训练，也有助于特征的提取。<br>主干网络代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   CSPdarknet53 的主体部分</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet_body</span>(<span class="hljs-params">x</span>):</span><br>    x = DarknetConv2D_BN_Mish(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(x)<br>    x = resblock_body(x, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-literal">False</span>)<br>    x = resblock_body(x, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>)<br>    x = resblock_body(x, <span class="hljs-number">256</span>, <span class="hljs-number">8</span>)<br>    feat1 = x<br>    x = resblock_body(x, <span class="hljs-number">512</span>, <span class="hljs-number">8</span>)<br>    feat2 = x<br>    x = resblock_body(x, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span>)<br>    feat3 = x<br>    <span class="hljs-keyword">return</span> feat1,feat2,feat3<br></code></pre></td></tr></table></figure>
<p>darknet_body(x)中的x即为输入传递来的图像数组(416，416，3)，然后图像先通过 DarknetConv2D_BN_Mish卷积块进行卷积，得到shape为(416,416,32)，再进行5次resblockbody，每一次宽高都减半，深度翻倍，其中feat1，feat2，feat3是对当时的特征层做一个记录，因为后面还需用到，变化如下图。</p>
<p><img src="/img/5.3/21.png" srcset="/img/loading.gif" lazyload alt="网络结构"></p>
<ol>
<li>特征金字塔部分SSP与PAN</li>
</ol>
<p>相比YOLOv3，YOLOV4结合了两种改进:</p>
<ul>
<li>使用了SPP结构。</li>
<li>使用了PANet结构。</li>
</ul>
<p>注：除去CSPDarknet53主干网络和Yolo Head的结构外，都是特征金字塔的结构。</p>
<ul>
<li>SSP结构</li>
</ul>
<p>首先SSP结构进行了三次DarknetConv2D_BN_Leaky卷积，你没看错，这里不是Mish，分别利用四个不同尺度的最大池化进行处理，最大池化的池化核大小分别为13x13、9x9、5x5、1x1（1x1–无处理），该结构能分离出最显著的上下文特征，是强有力的特征提取，池化后，再进行堆叠。如下图</p>
<p><img src="/img/5.3/22.png" srcset="/img/loading.gif" lazyload alt="SSP结构"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成darknet53的主干模型</span><br>    feat1,feat2,feat3 = darknet_body(inputs)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat3)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br>    <span class="hljs-comment"># 使用了SPP结构，即不同尺度的最大池化后堆叠。</span><br>    maxpool1 = MaxPooling2D(pool_size=(<span class="hljs-number">13</span>,<span class="hljs-number">13</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    maxpool2 = MaxPooling2D(pool_size=(<span class="hljs-number">9</span>,<span class="hljs-number">9</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    maxpool3 = MaxPooling2D(pool_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    P5 = Concatenate()([maxpool1, maxpool2, maxpool3, P5])<br></code></pre></td></tr></table></figure>
<p>简单描述下代码，首先darknet_body(inputs)可以获得返回的三个参数feat1，feat2，feat3，其中feat3为主干网络经过5个网络输出的特征层，然后对它进行3次 DarknetConv2D_BN_Leaky卷积，再经过三个最大池化分离特征，一个1×1池化即不变，还是P5，最后用 Concatenate()将四个进行堆叠。</p>
<ul>
<li>PANet</li>
</ul>
<p>PANet是2018年发表的一种实例分割算法，它可以反复提取特征。</p>
<p><img src="/img/5.3/23.png" srcset="/img/loading.gif" lazyload alt="PAN结构"></p>
<p><img src="/img/5.3/24.png" srcset="/img/loading.gif" lazyload alt="PAN结构"></p>
<p>整个过程大致来讲就是上采样、再堆叠卷积重复，之后再进行下采样、堆叠，如上图，上采样是放大，下采样是压缩。</p>
<ol>
<li>输出结果yolo head</li>
</ol>
<p>与yolov3中一样，内部是一个3×3卷积，一个是1×1卷积，进行通道调整<br>最后得到三个输出(52,52,75)，(26,26,75)，(13,13,75)<br>注：假设基于VOC数据集</p>
<p><img src="/img/5.3/25.png" srcset="/img/loading.gif" lazyload alt="PAN结构"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 池化堆叠后的四次卷积，注意compose中还有一次</span><br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br>	<br>	<span class="hljs-comment"># 上采样</span><br>    P5_upsample = compose(DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), UpSampling2D(<span class="hljs-number">2</span>))(P5)<br>    <br>    <span class="hljs-comment"># 与左边主干网络的特征层feat2进行拼接</span><br>    P4 = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat2)<br>    P4 = Concatenate()([P4, P5_upsample])<br>    <span class="hljs-comment"># Conv2d×5</span><br>    P4 = make_five_convs(P4,<span class="hljs-number">256</span>)<br>	<br>	<span class="hljs-comment"># 上采样</span><br>    P4_upsample = compose(DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), UpSampling2D(<span class="hljs-number">2</span>))(P4)<br>    <br>    <span class="hljs-comment"># 与左边主干网络的特征层feat1进行拼接</span><br>    P3 = DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat1)<br>    P3 = Concatenate()([P3, P4_upsample])<br>    <span class="hljs-comment"># Conv2d×5</span><br>    P3 = make_five_convs(P3,<span class="hljs-number">128</span>)<br><br>    <span class="hljs-comment"># 52x52的输出yolo head1，若是VOC数据集，则3×(20+5)=75，shape为(52,52,75)</span><br>    <span class="hljs-comment"># conv2d 3*3</span><br>    P3_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P3)<br>    <span class="hljs-comment"># con2d 1*1</span><br>    P3_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P3_output)<br><br>	<span class="hljs-comment"># 注意一下，这里下采样有个零填充即可</span><br>    P3_downsample = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(P3)<br>    P3_downsample = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(P3_downsample)<br>    P4 = Concatenate()([P3_downsample, P4])<br>    P4 = make_five_convs(P4,<span class="hljs-number">256</span>)<br>    <br>    <span class="hljs-comment"># 38x38的out</span><br>    P4_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P4)<br>    P4_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P4_output)<br>    <br><br>    P4_downsample = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(P4)<br>    P4_downsample = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(P4_downsample)<br>    P5 = Concatenate()([P4_downsample, P5])<br>    P5 = make_five_convs(P5,<span class="hljs-number">512</span>)<br>    <br>    <span class="hljs-comment"># 19x19的out</span><br>    P5_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5_output)<br>    <span class="hljs-keyword">return</span> Model(inputs, [P5_output, P4_output, P3_output])<br></code></pre></td></tr></table></figure>
<p>输出三个yolo head后还需对其进行解码，因为这个yolo head预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。这部分在yolov3文章的第二、三步已经写了，可以直接看2、3步，链接如下：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39615182/article/details/109752498">yolov3原理详解</a></p>
<p>下面讲下实现预测结果解码与得分排序、非极大抑制筛选两部分代码<br>解码是为了得到真实的边界框，但是不只一个，通过得分排序和非极大抑制筛选得到众多边框中最准确的一个，即最优的一个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   将预测值的每个特征层调成真实值</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_head</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, calc_loss=<span class="hljs-literal">False</span></span>):</span><br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    <span class="hljs-comment"># [1, 1, 1, num_anchors, 2]</span><br>    anchors_tensor = K.reshape(K.constant(anchors), [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, num_anchors, <span class="hljs-number">2</span>])<br><br>    <span class="hljs-comment"># 获得x，y的网格</span><br>    <span class="hljs-comment"># (13, 13, 1, 2)</span><br>    grid_shape = K.shape(feats)[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] <span class="hljs-comment"># height, width</span><br>    grid_y = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">0</span>]), [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid_x = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">1</span>]), [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [grid_shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid = K.concatenate([grid_x, grid_y])<br>    grid = K.cast(grid, K.dtype(feats))<br><br>    <span class="hljs-comment"># (batch_size,19,19,3,85)</span><br>    feats = K.reshape(feats, [-<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">0</span>], grid_shape[<span class="hljs-number">1</span>], num_anchors, num_classes + <span class="hljs-number">5</span>])<br><br>    <span class="hljs-comment"># 将预测值调成真实值</span><br>    <span class="hljs-comment"># box_xy对应框的中心点</span><br>    <span class="hljs-comment"># box_wh对应框的宽和高</span><br>    box_xy = (K.sigmoid(feats[..., :<span class="hljs-number">2</span>]) + grid) / K.cast(grid_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))<br>    box_wh = K.exp(feats[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]) * anchors_tensor / K.cast(input_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))<br>    box_confidence = K.sigmoid(feats[..., <span class="hljs-number">4</span>:<span class="hljs-number">5</span>])<br>    box_class_probs = K.sigmoid(feats[..., <span class="hljs-number">5</span>:])<br><br>    <span class="hljs-comment"># 在计算loss的时候返回如下参数</span><br>    <span class="hljs-keyword">if</span> calc_loss == <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">return</span> grid, feats, box_xy, box_wh<br>    <span class="hljs-keyword">return</span> box_xy, box_wh, box_confidence, box_class_probs<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   对box进行调整，使其符合真实图片的样子</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_correct_boxes</span>(<span class="hljs-params">box_xy, box_wh, input_shape, image_shape</span>):</span><br>    box_yx = box_xy[..., ::-<span class="hljs-number">1</span>]<br>    box_hw = box_wh[..., ::-<span class="hljs-number">1</span>]<br>    <br>    input_shape = K.cast(input_shape, K.dtype(box_yx))<br>    image_shape = K.cast(image_shape, K.dtype(box_yx))<br><br>    new_shape = K.<span class="hljs-built_in">round</span>(image_shape * K.<span class="hljs-built_in">min</span>(input_shape/image_shape))<br>    offset = (input_shape-new_shape)/<span class="hljs-number">2.</span>/input_shape<br>    scale = input_shape/new_shape<br><br>    box_yx = (box_yx - offset) * scale<br>    box_hw *= scale<br><br>    box_mins = box_yx - (box_hw / <span class="hljs-number">2.</span>)<br>    box_maxes = box_yx + (box_hw / <span class="hljs-number">2.</span>)<br>    boxes =  K.concatenate([<br>        box_mins[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_min</span><br>        box_mins[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>],  <span class="hljs-comment"># x_min</span><br>        box_maxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_max</span><br>        box_maxes[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>]  <span class="hljs-comment"># x_max</span><br>    ])<br><br>    boxes *= K.concatenate([image_shape, image_shape])<br>    <span class="hljs-keyword">return</span> boxes<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   获取每个box和它的得分</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_boxes_and_scores</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, image_shape</span>):</span><br>    <span class="hljs-comment"># 将预测值调成真实值</span><br>    <span class="hljs-comment"># box_xy对应框的中心点</span><br>    <span class="hljs-comment"># box_wh对应框的宽和高</span><br>    <span class="hljs-comment"># -1,19,19,3,2; -1,19,19,3,2; -1,19,19,3,1; -1,19,19,3,80</span><br>    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats, anchors, num_classes, input_shape)<br>    <span class="hljs-comment"># 将box_xy、和box_wh调节成y_min,y_max,xmin,xmax</span><br>    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)<br>    <span class="hljs-comment"># 获得得分和box</span><br>    boxes = K.reshape(boxes, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])<br>    box_scores = box_confidence * box_class_probs<br>    box_scores = K.reshape(box_scores, [-<span class="hljs-number">1</span>, num_classes])<br>    <span class="hljs-keyword">return</span> boxes, box_scores<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   图片预测</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_eval</span>(<span class="hljs-params">yolo_outputs,</span></span><br><span class="hljs-function"><span class="hljs-params">              anchors,</span></span><br><span class="hljs-function"><span class="hljs-params">              num_classes,</span></span><br><span class="hljs-function"><span class="hljs-params">              image_shape,</span></span><br><span class="hljs-function"><span class="hljs-params">              max_boxes=<span class="hljs-number">20</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">              score_threshold=<span class="hljs-number">.6</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">              iou_threshold=<span class="hljs-number">.5</span></span>):</span><br>    <span class="hljs-comment"># 获得特征层的数量</span><br>    num_layers = <span class="hljs-built_in">len</span>(yolo_outputs)<br>    <span class="hljs-comment"># 特征层1对应的anchor是678</span><br>    <span class="hljs-comment"># 特征层2对应的anchor是345</span><br>    <span class="hljs-comment"># 特征层3对应的anchor是012</span><br>    anchor_mask = [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]]<br>    <br>    input_shape = K.shape(yolo_outputs[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] * <span class="hljs-number">32</span><br>    boxes = []<br>    box_scores = []<br>    <span class="hljs-comment"># 对每个特征层进行处理</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, image_shape)<br>        boxes.append(_boxes)<br>        box_scores.append(_box_scores)<br>    <span class="hljs-comment"># 将每个特征层的结果进行堆叠</span><br>    boxes = K.concatenate(boxes, axis=<span class="hljs-number">0</span>)<br>    box_scores = K.concatenate(box_scores, axis=<span class="hljs-number">0</span>)<br><br>    mask = box_scores &gt;= score_threshold<br>    max_boxes_tensor = K.constant(max_boxes, dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    boxes_ = []<br>    scores_ = []<br>    classes_ = []<br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_classes):<br>        <span class="hljs-comment"># 取出所有box_scores &gt;= score_threshold的框，和成绩</span><br>        class_boxes = tf.boolean_mask(boxes, mask[:, c])<br>        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])<br><br>        <span class="hljs-comment"># 非极大抑制，去掉box重合程度高的那一些，获取局部最大值</span><br>        nms_index = tf.image.non_max_suppression(<br>            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)<br><br>        <span class="hljs-comment"># 获取非极大抑制后的结果</span><br>        <span class="hljs-comment"># 下列三个分别是</span><br>        <span class="hljs-comment"># 框的位置，得分与种类</span><br>        class_boxes = K.gather(class_boxes, nms_index)<br>        class_box_scores = K.gather(class_box_scores, nms_index)<br>        classes = K.ones_like(class_box_scores, <span class="hljs-string">&#x27;int32&#x27;</span>) * c<br>        boxes_.append(class_boxes)<br>        scores_.append(class_box_scores)<br>        classes_.append(classes)<br>    boxes_ = K.concatenate(boxes_, axis=<span class="hljs-number">0</span>)<br>    scores_ = K.concatenate(scores_, axis=<span class="hljs-number">0</span>)<br>    classes_ = K.concatenate(classes_, axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">return</span> boxes_, scores_, classes_<br></code></pre></td></tr></table></figure>
<h2 id="YOLOv4新增训练技巧"><a href="#YOLOv4新增训练技巧" class="headerlink" title="YOLOv4新增训练技巧"></a>YOLOv4新增训练技巧</h2><p>为了使设计的检测器更适合于单 GPU 的训练，我们进行了如下其他设计和改进：</p>
<ol>
<li>Mosaic数据增强</li>
</ol>
<p>论文原话：Mosaic表示一种新的数据增强方法，该方法混合了4个训练图像。 因此，混合了4个不同的上下文，而CutMix仅混合了2个输入图像。 这样可以检测正常上下文之外的对象，增强模型的鲁棒性。。 此外，批量归一化从每层上的4张不同图像计算激活统计信息。 这大大减少了对大批量生产的需求。</p>
<p><img src="/img/5.3/26.png" srcset="/img/loading.gif" lazyload alt="Mosaic represents a new method of data augmentation"></p>
<p>这个Mosaic数据增强就是一次读取4张不同的图片，然后进行缩放，调整大小等操作，然后拼接成一幅图。这样能一次训练四个图像，增加了图片内容的丰富性，即丰富检测物体的背景。BN（标准化）计算的时候一下子会计算四张图片的数据</p>
<ol>
<li>自对抗训练SAT</li>
</ol>
<p>SAT是一种新的数据增强技术，该技术分前后两个阶段进行：<br>第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对自身执行一种对抗性攻击，改变原始图像，从而造成图像上没有目标的假象。<br>第二阶段，训练神经网络对修改后的图像进行正常的目标检测。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Version/">Computer Version</a>
                    
                      <a class="hover-with-bg" href="/categories/Computer-Version/Paper/">Paper</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/CV/">CV</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/05/02/YOLOv3-An-Incremental-Improvement/">
                        <span class="hidden-mobile">YOLOv3: An Incremental Improvement</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@waline/client@0.14.8/dist/Waline.min.js', function () {
        new Waline({
          el: "#waline",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: true,
          serverURL: "http://gzzyyxh.cn",
          avatarCDN: "",
          avatarForce: true,
          requiredFields: [],
          emojiCDN: "",
          emojiMaps: {},
          anonymous: null,
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-x-md">
    <div class="container custom post-custom mx-auto">
      <img src="https://octodex.github.com/images/jetpacktocat.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
    </div>
  </div>


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js" ></script>

  








  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
