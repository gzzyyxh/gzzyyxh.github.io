<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>YOLOv5原理详解</title>
    <link href="/2021/05/03/YOLOv5%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"/>
    <url>/2021/05/03/YOLOv5%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><p>对Yolov4的相关基础知识做了比较系统的梳理，但Yolov4后不久，又出现了Yolov5，虽然作者没有放上和Yolov4的直接测试对比，但在COCO数据集的测试效果还是很可观的。</p><p>很多人考虑到Yolov5的创新性不足，对算法是否能够进化，称得上Yolov5而议论纷纷。</p><p>但既然称之为Yolov5，也有很多非常不错的地方值得我们学习。不过因为Yolov5的网络结构和Yolov3、Yolov4相比，不好可视化，导致很多同学看Yolov5看的云里雾里。</p><p>因此本文，大白主要对Yolov5四种网络结构的各个细节做一个深入浅出的分析总结，和大家一些探讨学习。</p><h1 id="Yolov5-四种网络模型"><a href="#Yolov5-四种网络模型" class="headerlink" title="Yolov5 四种网络模型"></a>Yolov5 四种网络模型</h1><p>Yolov5官方代码中，给出的目标检测网络中一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。</p><p>学习一个新的算法，最好在脑海中对算法网络的整体架构有一个清晰的理解。</p><p>但比较尴尬的是，Yolov5代码中给出的网络文件是yaml格式，和原本Yolov3、Yolov4中的cfg不同。</p><p>因此无法用<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/106245563">netron</a>工具直接可视化的查看网络结构，造成有的同学不知道如何去学习这样的网络。</p><p>比如下载了Yolov5的四个pt格式的权重模型：</p><p><img src="/img/5.3/27.jpg" alt="Yolov5的四个pt格式的权重模型"></p><p><a href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础完整讲解》</a>中讲到，可以使用netron工具打开网络模型。</p><p>但因为netron对pt格式的文件兼容性并不好，直接使用netron工具打开，会发现，根本无法显示全部网络。</p><p>因此可以采用pt-&gt;onnx-&gt;netron的折中方式，先使用Yolov5代码中models/export.py脚本将pt文件转换为onnx格式，再用netron工具打开，这样就可以看全网络的整体架构了。</p><p><img src="/img/5.3/28.jpg" alt="Yolov5的四个pt格式的权重模型"></p><p>如果有同学对netron工具还不是很熟悉，这里还是放上安装netron工具的详解，如果需要安装，可以移步大白的另一篇文章：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/106245563">《网络可视化工具netron详细安装流程》</a></p><p>如需下载Yolov5整体的4个网络pt文件及onnx文件，也可<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/107852288">点击链接查看下载</a>，便于直观的学习。</p><h2 id="Yolov5网络结构图"><a href="#Yolov5网络结构图" class="headerlink" title="Yolov5网络结构图"></a>Yolov5网络结构图</h2><p>安装好netron工具，就可以可视化的打开Yolov5的网络结构。</p><p><img src="/img/5.3/29.jpg" alt="Yolov5网络结构图"></p><h2 id="网络结构可视化"><a href="#网络结构可视化" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h2><p>将四种模型pt文件的转换成对应的onnx文件后，即可使用<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/106245563">netron工具</a>查看。</p><h3 id="Yolov5s网络结构"><a href="#Yolov5s网络结构" class="headerlink" title="Yolov5s网络结构"></a>Yolov5s网络结构</h3><p>Yolov5s网络是Yolov5系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。</p><p><img src="/img/5.3/30.png" alt="Yolov5s网络结构"></p><h3 id="Yolov5m网络结构"><a href="#Yolov5m网络结构" class="headerlink" title="Yolov5m网络结构"></a>Yolov5m网络结构</h3><p><img src="/img/5.3/31.png" alt="Yolov5m网络结构"></p><h3 id="Yolov5l网络结构"><a href="#Yolov5l网络结构" class="headerlink" title="Yolov5l网络结构"></a>Yolov5l网络结构</h3><p><img src="/img/5.3/32.png" alt="Yolov5l网络结构"></p><h3 id="Yolov5x网络结构"><a href="#Yolov5x网络结构" class="headerlink" title="Yolov5x网络结构"></a>Yolov5x网络结构</h3><p><img src="/img/5.3/33.png" alt="Yolov5x网络结构"></p><h1 id="核心基础内容"><a href="#核心基础内容" class="headerlink" title="核心基础内容"></a>核心基础内容</h1><h2 id="Yolov3-amp-Yolov4网络结构图"><a href="#Yolov3-amp-Yolov4网络结构图" class="headerlink" title="Yolov3&amp;Yolov4网络结构图"></a>Yolov3&amp;Yolov4网络结构图</h2><h3 id="Yolov3网络结构图"><a href="#Yolov3网络结构图" class="headerlink" title="Yolov3网络结构图"></a>Yolov3网络结构图</h3><p>Yolov3的网络结构是比较经典的one-stage结构，分为输入端、Backbone、Neck和Prediction四个部分。</p><p>先放上YOLOv3的网络结构图</p><p><img src="/img/5.3/34.jpg" alt="YOLOv3的网络结构图"></p><h3 id="Yolov4网络结构图"><a href="#Yolov4网络结构图" class="headerlink" title="Yolov4网络结构图"></a>Yolov4网络结构图</h3><p>Yolov4在Yolov3的基础上进行了很多的创新。<br>比如输入端采用mosaic数据增强，<br>Backbone上采用了CSPDarknet53、Mish激活函数、Dropblock等方式，<br>Neck中采用了SPP、FPN+PAN的结构，<br>输出端则采用CIOU_Loss、DIOU_nms操作。</p><p><img src="/img/5.3/35.jpg" alt="Yolov4网络结构图"></p><h2 id="Yolov5核心基础内容"><a href="#Yolov5核心基础内容" class="headerlink" title="Yolov5核心基础内容"></a>Yolov5核心基础内容</h2><p>Yolov5的结构和Yolov4很相似，但也有一些不同，大白还是按照从整体到细节的方式，对每个板块进行讲解。</p><p><img src="/img/5.3/36.jpg" alt="Yolov5的网络结构图"></p><p>上图即Yolov5的网络结构图，可以看出，还是分为输入端、Backbone、Neck、Prediction四个部分。</p><p>大家可能对Yolov3比较熟悉，因此大白列举它和Yolov3的一些主要的不同点，并和Yolov4进行比较。</p><p>（1）输入端：Mosaic数据增强、自适应锚框计算、自适应图片缩放<br>（2）Backbone：Focus结构，CSP结构<br>（3）Neck：FPN+PAN结构<br>（4）Prediction：GIOU_Loss</p><p>下面丢上Yolov5作者的算法性能测试图：</p><p><img src="/img/5.3/37.jpg" alt="算法性能测试图"></p><p>Yolov5作者也是在COCO数据集上进行的测试，大白在之前的文章讲过，COCO数据集的小目标占比，因此最终的四种网络结构，性能上来说各有千秋。</p><p>Yolov5s网络最小，速度最少，AP精度也最低。但如果检测的以大目标为主，追求速度，倒也是个不错的选择。</p><p>其他的三种网络，在此基础上，不断加深加宽网络，AP精度也不断提升，但速度的消耗也在不断增加。</p><h3 id="输入端"><a href="#输入端" class="headerlink" title="输入端"></a>输入端</h3><p>1.Mosaic数据增强</p><p>Yolov5的输入端采用了和Yolov4一样的Mosaic数据增强的方式。</p><p>Mosaic数据增强提出的作者也是来自Yolov5团队的成员，不过，随机缩放、随机裁剪、随机排布的方式进行拼接，对于小目标的检测效果还是很不错的。</p><p><img src="/img/5.3/38.jpg" alt="Mosaic数据增强"></p><p>2.自适应锚框计算</p><p>在Yolo算法中，针对不同的数据集，都会有初始设定长宽的锚框。</p><p>在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新，迭代网络参数。</p><p>因此初始锚框也是比较重要的一部分，比如Yolov5在Coco数据集上初始设定的锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">- [<span class="hljs-number">116</span>,<span class="hljs-number">90</span>,<span class="hljs-number">156</span>,<span class="hljs-number">198</span>,<span class="hljs-number">373</span>,<span class="hljs-number">326</span>] <span class="hljs-comment">#P5/32</span><br>- [<span class="hljs-number">30</span>,<span class="hljs-number">61</span>,<span class="hljs-number">62</span>,<span class="hljs-number">45</span>,<span class="hljs-number">59</span>,<span class="hljs-number">119</span>] <span class="hljs-comment">#P4/16</span><br>- [<span class="hljs-number">10</span>,<span class="hljs-number">13</span>,<span class="hljs-number">16</span>,<span class="hljs-number">30</span>,<span class="hljs-number">33</span>,<span class="hljs-number">23</span>] <span class="hljs-comment">#P3/8</span><br></code></pre></td></tr></table></figure><p>在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。</p><p>但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。</p><p>当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能关闭。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">parser.add arqument(<span class="hljs-string">&#x27;--noautoanchor&#x27;</span>,action=<span class="hljs-string">&#x27;store true&#x27;</span>,<span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;disable autoanchor check&#x27;</span>)<br></code></pre></td></tr></table></figure><p>控制的代码即train.py中上面一行代码，设置成False，每次训练时，不会自动计算。</p><p>3.自适应图片缩放</p><p>在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。</p><p>比如Yolo算法中常用416<em>416，608</em>608等尺寸，比如对下面800*600的图像进行缩放。</p><p><img src="/img/5.3/39.jpg" alt="自适应图片缩放"></p><p>但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。</p><p>作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。</p><p>因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。</p><p><img src="/img/5.3/40.jpg" alt="自适应图片缩放"></p><p>图像高度上两端的黑边变少了，在推理时，计算量也会减少，即目标检测速度会得到提升。</p><p>这种方式在之前github上Yolov3中也进行了讨论：[<a href="https://github.com/ultralytics/yolov3/issues/232">https://github.com/ultralytics/yolov3/issues/232</a>]</p><p>在讨论中，通过这种简单的改进，推理速度得到了37%的提升，可以说效果很明显。</p><p>但是有的同学可能会有大大的问号？？如何进行计算的呢？大白按照Yolov5中的思路详细的讲解一下，在datasets.py的letterbox函数中也有详细的代码。</p><p>第一步：计算缩放比例</p><p><img src="/img/5.3/41.jpg" alt="自适应图片缩放"></p><p>原始缩放尺寸是416*416，都除以原始图像的尺寸后，可以得到0.52，和0.69两个缩放系数，选择小的缩放系数。</p><p>第二步：计算缩放后的尺寸</p><p><img src="/img/5.3/42.jpg" alt="自适应图片缩放"></p><p>原始图片的长宽都乘以最小的缩放系数0.52，宽变成了416，而高变成了312。</p><p>第三步：计算黑边填充数值</p><p><img src="/img/5.3/43.jpg" alt="自适应图片缩放"></p><p>将416-312=104，得到原本需要填充的高度。再采用numpy中np.mod取余数的方式，得到40个像素，再除以2，即得到图片高度两端需要填充的数值。</p><p>此外，需要注意的是：</p><p>a.这里大白填充的是黑色，即（0，0，0），而Yolov5中填充的是灰色，即（114,114,114），都是一样的效果。</p><p>b.训练时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在测试，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。</p><h3 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h3><p>1.Foucus结构</p><p><img src="/img/5.3/44.jpg" alt="Foucus结构"></p><p>Focus结构，在Yolov3&amp;Yolov4中并没有这个结构，其中比较关键是切片操作。</p><p>比如右图的切片示意图，4<em>4</em>3的图像切片后变成2<em>2</em>12的特征图。</p><p>以Yolov5s的结构为例，原始608<em>608</em>3的图像输入Focus结构，采用切片操作，先变成304<em>304</em>12的特征图，再经过一次32个卷积核的卷积操作，最终变成304<em>304</em>32的特征图。</p><p>需要注意的是：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加，先注意下，后面会讲解到四种结构的不同点。</p><p>2.CSP结构</p><p>Yolov4网络结构中，借鉴了CSPNet的设计思路，在主干网络中设计了CSP结构。</p><p><img src="/img/5.3/45.jpg" alt="CSP结构"></p><p>Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。</p><p>而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。</p><p><img src="/img/5.3/46.jpg" alt="CSP结构"></p><p>这里关于CSPNet的内容，也可以查看大白之前的<a href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础完整讲解》</a>。</p><h3 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h3><p>Yolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构，但在Yolov5刚出来时，只使用了FPN结构，后面才增加了PAN结构，此外网络中其他部分也进行了调整。</p><p>因此，大白在Yolov5刚提出时，画的很多结构图，又都重新进行了调整。</p><p><img src="/img/5.3/47.jpg" alt="Neck"></p><p>这里关于FPN+PAN的结构，大白在<a href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础知识完整讲解》</a>中，讲的很多，大家应该都有理解。</p><p>但如上面CSPNet结构中讲到，Yolov5和Yolov4的不同点在于，</p><p>Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。</p><p><img src="/img/5.3/48.jpg" alt="Neck"></p><h3 id="输出端"><a href="#输出端" class="headerlink" title="输出端"></a>输出端</h3><p>1.Bounding box损失函数</p><p>在<a href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础知识完整讲解》</a>中，大白详细的讲解了IOU_Loss，以及进化版的GIOU_Loss，DIOU_Loss，以及CIOU_Loss。</p><p>Yolov5中采用其中的GIOU_Loss做Bounding box的损失函数。</p><p><img src="/img/5.3/49.jpg" alt="Bounding box损失函数"></p><p>而Yolov4中采用CIOU_Loss作为目标Bounding box的损失。</p><script type="math/tex; mode=display">CIOU_Loss = 1-CIOU=1-(IOU-\frac{Distance_{2^2}}{Distance_{C^2}}-\frac{v^2}{(1-IOU)+v})</script><script type="math/tex; mode=display">v=\frac{4}{\pi^2}(arctan \frac{w^{gt}}{h^{gt}}-arctan \frac{w^p}{h^p})^2</script><p>2.nms非极大值抑制</p><p>在目标检测的后处理过程中，针对很多目标框的筛选，通常需要nms操作。</p><p>因为CIOU_Loss中包含影响因子v，涉及groudtruth的信息，而测试推理时，是没有groundtruth的。</p><p>所以Yolov4在DIOU_Loss的基础上采用DIOU_nms的方式，而Yolov5中采用加权nms的方式。</p><p>可以看出，采用DIOU_nms，下方中间箭头的黄色部分，原本被遮挡的摩托车也可以检出。</p><p><img src="/img/5.3/50.jpg" alt="nms非极大值抑制"></p><p>白在项目中，也采用了DIOU_nms的方式，在同样的参数情况下，将nms中IOU修改成DIOU_nms。对于一些遮挡重叠的目标，确实会有一些改进。</p><p>比如下面黄色箭头部分，原本两个人重叠的部分，在参数和普通的IOU_nms一致的情况下，修改成DIOU_nms，可以将两个目标检出。</p><p>虽然大多数状态下效果差不多，但在不增加计算成本的情况下，有稍微的改进也是好的。</p><p><img src="/img/5.3/51.jpg" alt="nms非极大值抑制"></p><h2 id="Yolov5四种网络结构的不同点"><a href="#Yolov5四种网络结构的不同点" class="headerlink" title="Yolov5四种网络结构的不同点"></a>Yolov5四种网络结构的不同点</h2><p>Yolov5代码中的四种网络，和之前的Yolov3，Yolov4中的cfg文件不同，都是以yaml的形式来呈现。</p><p>而且四个文件的内容基本上都是一样的，只有最上方的depth_multiple和width_multiple两个参数不同，很多同学看的一脸懵逼，不知道只通过两个参数是如何控制四种结构的？</p><h3 id="四种结构的参数"><a href="#四种结构的参数" class="headerlink" title="四种结构的参数"></a>四种结构的参数</h3><p>1.Yolov5s.yaml</p><p><img src="/img/5.3/52.png" alt="Yolov5s.yaml"></p><p>2.Yolov5m.yaml</p><p><img src="/img/5.3/53.png" alt="Yolov5m.yaml"></p><p>3.Yolov5l.yaml</p><p><img src="/img/5.3/54.png" alt="Yolov5l.yaml"></p><p>4.Yolov5x.yaml</p><p><img src="/img/5.3/55.png" alt="Yolov5x.yaml"></p><p>四种结构就是通过上面的两个参数，来进行控制网络的深度和宽度。其中depth_multiple控制网络的深度，width_multiple控制网络的宽度。</p><h3 id="Yolov5网络结构"><a href="#Yolov5网络结构" class="headerlink" title="Yolov5网络结构"></a>Yolov5网络结构</h3><p>四种结构的yaml文件中，下方的网络架构代码都是一样的。</p><p>为了便于讲解，大白将其中的Backbone部分提取出来，讲解如何控制网络的宽度和深度，yaml文件中的Head部分也是同样的原理。</p><p><img src="/img/5.3/56.jpg" alt="Yolov5网络结构"></p><p>在对网络结构进行解析时，yolo.py中下方的这一行代码将四种结构的depth_multiple，width_multiple提取出，赋值给gd，gw。后面主要对这gd，gw这两个参数进行讲解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">anchors,nc,gd,gw=d[<span class="hljs-string">&#x27;anchors&#x27;</span>],d[<span class="hljs-string">&#x27;nc&#x27;</span>],d[<span class="hljs-string">&#x27;depth_multiple&#x27;</span>],d[<span class="hljs-string">&#x27;width_multiple&#x27;</span>]<br></code></pre></td></tr></table></figure><p>下面再细致的剖析下，看是如何控制每种结构，深度和宽度的。</p><h3 id="Yolov5四种网络的深度"><a href="#Yolov5四种网络的深度" class="headerlink" title="Yolov5四种网络的深度"></a>Yolov5四种网络的深度</h3><p><img src="/img/5.3/57.jpg" alt="Yolov5四种网络的深度"></p><p>1.不同网络的深度</p><p>在上图中，大白画了两种CSP结构，CSP1和CSP2，其中CSP1结构主要应用于Backbone中，CSP2结构主要应用于Neck中。</p><p>需要注意的是，四种网络结构中每个CSP结构的深度都是不同的。</p><p>a.以yolov5s为例，第一个CSP1中，使用了1个残差组件，因此是CSP1_1。而在Yolov5m中，则增加了网络的深度，在第一个CSP1中，使用了2个残差组件，因此是CSP1_2。</p><p>而Yolov5l中，同样的位置，则使用了3个残差组件，Yolov5x中，使用了4个残差组件。</p><p>其余的第二个CSP1和第三个CSP1也是同样的原理。</p><p>b.在第二种CSP2结构中也是同样的方式，以第一个CSP2结构为例，Yolov5s组件中使用了2*1=2组卷积，因此是CSP2_1。</p><p>而Yolov5m中使用了2组，Yolov5l中使用了3组，Yolov5x中使用了4组。</p><p>其他的四个CSP2结构，也是同理。</p><p>Yolov5中，网络的不断加深，也在不断增加网络特征提取和特征融合的能力。</p><p>2.控制深度的代码</p><p>控制四种网络结构的核心代码是yolo.py中下面的代码，存在两个变量，n和gd。</p><p>我们再将n和gd带入计算，看每种网络的变化结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">n=<span class="hljs-built_in">max</span>(<span class="hljs-built_in">round</span>(n*gd),<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> n <span class="hljs-comment"># depth gain</span><br></code></pre></td></tr></table></figure><p>3.验证控制深度的有效性</p><p>择最小的yolov5s.yaml和中间的yolov5l.yaml两个网络结构，将gd(height_multiple)系数带入，看是否正确。</p><p><img src="/img/5.3/57.jpg" alt="验证控制深度的有效性"></p><ul><li>yolov5x.yaml</li></ul><p>其中height_multiple=0.33，即gd=0.33，而n则由上面红色框中的信息获得。</p><p>以上面网络框图中的第一个CSP1为例，即上面的第一个红色框。n等于第二个数值3。</p><p>而gd=0.33，带入（2）中的计算代码，结果n=1。因此第一个CSP1结构内只有1个残差组件，即CSP1_1。</p><p>第二个CSP1结构中，n等于第二个数值9，而gd=0.33，带入（2）中计算，结果n=3，因此第二个CSP1结构中有3个残差组件，即CSP1_3。</p><p>第三个CSP1结构也是同理，这里不多说。</p><ul><li>yolov5l.xml</li></ul><p>其中height_multiple=1，即gd=1</p><p>和上面的计算方式相同，第一个CSP1结构中，n=1，带入代码中，结果n=3，因此为CSP1_3。</p><p>下面第二个CSP1和第三个CSP1结构都是同样的原理。</p><h3 id="Yolov5四种网络的宽度"><a href="#Yolov5四种网络的宽度" class="headerlink" title="Yolov5四种网络的宽度"></a>Yolov5四种网络的宽度</h3><p><img src="/img/5.3/59.jpg" alt="Yolov5四种网络的宽度"></p><p>1.不同网络的宽度:</p><p>如上图表格中所示，四种yolov5结构在不同阶段的卷积核的数量都是不一样的，因此也直接影响卷积后特征图的第三维度，即厚度，大白这里表示为网络的宽度。</p><ul><li>以Yolov5s结构为例，第一个Focus结构中，最后卷积操作时，卷积核的数量是32个，因此经过Focus结构，特征图的大小变成304<em>304</em>32。</li></ul><p>而yolov5m的Focus结构中的卷积操作使用了48个卷积核，因此Focus结构后的特征图变成304<em>304</em>48。yolov5l，yolov5x也是同样的原理。</p><ul><li><p>第二个卷积操作时，yolov5s使用了64个卷积核，因此得到的特征图是152<em>152</em>64。而yolov5m使用96个特征图，因此得到的特征图是152<em>152</em>96。yolov5l，yolov5x也是同理。</p></li><li><p>后面三个卷积下采样操作也是同样的原理，这样大白不过多讲解。</p></li></ul><p>四种不同结构的卷积核的数量不同，这也直接影响网络中，比如CSP1，CSP2等结构，以及各个普通卷积，卷积操作时的卷积核数量也同步在调整，影响整体网络的计算量。</p><p>大家最好可以将结构图和<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/107658499">前面第一部分四个网络的特征图链接</a>，对应查看，思路会更加清晰。</p><p>当然卷积核的数量越多，特征图的厚度，即宽度越宽，网络提取特征的学习能力也越强。</p><p>2.控制宽度的代码</p><p>在yolov5的代码中，控制宽度的核心代码是yolo.py文件里面的这一行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">c2=make_divisible(c2*gw,<span class="hljs-number">8</span>) <span class="hljs-keyword">if</span> c2 != no <span class="hljs-keyword">else</span> c2<br></code></pre></td></tr></table></figure><p>它所调用的子函数make_divisible的功能是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_divisible</span>(<span class="hljs-params">x,divisor</span>):</span><br>    <span class="hljs-comment">#Returns x evenly divisble bu divisor</span><br>    <span class="hljs-keyword">return</span> math.cell(x / divisor) * divisor<br></code></pre></td></tr></table></figure><p>3.验证控制宽度的有效性</p><p>我们还是选择最小的yolov5s和中间的yolov5l两个网络结构，将width_multiple系数带入，看是否正确。</p><p><img src="/img/5.3/60.jpg" alt="验证控制宽度的有效性"></p><ul><li>yolov5x.yaml</li></ul><p>其中width_multiple=0.5，即gw=0.5。</p><p><img src="/img/5.3/61.jpg" alt="yolov5x.yaml"></p><p>以第一个卷积下采样为例，即Focus结构中下面的卷积操作。</p><p>按照上面Backbone的信息，我们知道Focus中，标准的c2=64，而gw=0.5，代入（2）中的计算公式，最后的结果=32。即Yolov5s的Focus结构中，卷积下采样操作的卷积核数量为32个。</p><p>再计算后面的第二个卷积下采样操作，标准c2的值=128，gw=0.5，代入（2）中公式，最后的结果=64，也是正确的。</p><ul><li>yolov5l.yaml</li></ul><p>其中width_multiple=1，即gw=1，而标准的c2=64，代入上面（2）的计算公式中，可以得到Yolov5l的Focus结构中，卷积下采样操作的卷积核的数量为64个，而第二个卷积下采样的卷积核数量是128个。</p><p>另外的三个卷积下采样操作，以及yolov5m，yolov5x结构也是同样的计算方式，大白这里不过多解释。</p><h1 id="Yolov5相关论文及代码"><a href="#Yolov5相关论文及代码" class="headerlink" title="Yolov5相关论文及代码"></a>Yolov5相关论文及代码</h1><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Yolov5的作者并没有发表论文，因此只能从代码角度进行分析。</p><p>Yolov5代码：[<a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a>]</p><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p>另外一篇论文，PP-Yolo，在Yolov3的原理上，采用了很多的tricks调参方式，也挺有意思。</p><p>感兴趣的话可以参照另一个博主的文章：<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/pHOFqFihkkRVTYbkSTlG4w">点击查看</a></p><h1 id="小目标分割检测"><a href="#小目标分割检测" class="headerlink" title="小目标分割检测"></a>小目标分割检测</h1><p>目标检测发展很快，但对于小目标的检测还是有一定的瓶颈，特别是大分辨率图像小目标检测。比如7920<em>2160，甚至16000</em>16000的图像。</p><p><img src="/img/5.3/62.jpg" alt="小目标分割检测"></p><p>图像的分辨率很大，但又有很多小的目标需要检测。但是如果直接输入检测网络，比如yolov3，检出效果并不好。</p><p>主要原因是：</p><p>1.小目标尺寸</p><p>以网络的输入608<em>608为例，yolov3、yolov4，yolov5中下采样都使用了5次，因此最后的特征图大小是19</em>19，38<em>38，76</em>76。</p><p>三个特征图中，最大的76<em>76负责检测小目标，而对应到608</em>608上，每格特征图的感受野是608/76=8*8大小。</p><p><img src="/img/5.3/63.jpg" alt="小目标分割检测"></p><p>再将608<em>608对应到7680</em>2160上，以最长边7680为例，7680/608*8=101。</p><p>即如果原始图像中目标的宽或高小于101像素，网络很难学习到目标的特征信息。</p><p>（PS：这里忽略多尺度训练的因素及增加网络检测分支的情况）</p><p>2.高分辨率</p><p>而在很多遥感图像中，长宽比的分辨率比7680<em>2160更大，比如上面的16000</em>16000，如果采用直接输入原图的方式，很多小目标都无法检测出。</p><p>3.显卡爆炸</p><p>很多图像分辨率很大，如果简单的进行下采样，下采样的倍数太大，容易丢失数据信息。</p><p>但是倍数太小，网络前向传播需要在内存中保存大量的特征图，极大耗尽GPU资源,很容易发生显存爆炸，无法正常的训练及推理。</p><p>因此可以借鉴<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.09512">2018年YOLT算法</a>的方式，改变一下思维，对大分辨率图片先进行分割，变成一张张小图，再进行检测。</p><p>需要注意的是：</p><p>为了避免两张小图之间，一些目标正好被分割截断，所以两个小图之间设置overlap重叠区域，比如分割的小图是960<em>960像素大小，则overlap可以设置为960</em>20%=192像素。</p><p><img src="/img/5.3/64.jpg" alt="小目标分割检测"></p><p>每个小图检测完成后，再将所有的框放到大图上，对大图整体做一次nms操作，将重叠区域的很多重复框去除。</p><p>这样操作，可以将很多小目标检出，比如16000*16000像素的遥感图像。</p><p><img src="/img/5.3/65.jpg" alt="小目标分割检测"></p><p>无人机视角下，也有很多小的目标。大白也进行了测试，效果还是不错的。</p><p>比如下图是将原始大图-&gt;416*416大小，直接使用目标检测网络输出的效果：</p><p><img src="/img/5.3/66.jpg" alt="小目标分割检测"></p><p>可以看到中间黄色框区域，很多汽车检测漏掉。</p><p>再使用分割的方式，将大图先分割成小图，再对每个小图检测，可以看出中间区域很多的汽车都被检测出来：</p><p><img src="/img/5.3/67.jpg" alt="小目标分割检测"></p><p>不过这样的方式有优点也有缺点：</p><p>优点：</p><p>1.准确性</p><p>分割后的小图，再输入目标检测网络中，对于最小目标像素的下限会大大降低。</p><p>比如分割成608<em>608大小，送入输入图像大小608</em>608的网络中，按照上面的计算方式，原始图片上，长宽大于8个像素的小目标都可以学习到特征。</p><p>2.检测方式</p><p>在大分辨率图像，比如遥感图像，或者无人机图像，如果无需考虑实时性的检测，且对小目标检测也有需求的项目，可以尝试此种方式。</p><p>缺点：</p><ol><li>增加计算量</li></ol><p>比如原本7680*2160的图像，如果使用直接大图检测的方式，一次即可检测完。</p><p>但采用分割的方式，切分成N张608*608大小的图像，再进行N次检测，会大大增加检测时间。</p><p>借鉴Yolov5的四种网络方式，我们可以采用尽量轻的网络，比如Yolov5s网络结构或者更轻的网络。</p><p>当然Yolov4和Yolov5的网络各有优势，我们也可以借鉴Yolov5的设计方式，对Yolov4进行轻量化改造，或者进行剪枝。</p><h1 id="后语"><a href="#后语" class="headerlink" title="后语"></a>后语</h1><p>综合而言，在实际测试中，Yolov4的准确性有不错的优势，但Yolov5的多种网络结构使用起来更加灵活，我们可以根据不同的项目需求，取长补短，发挥不同检测网络的优势。</p>]]></content>
    
    
    <categories>
      
      <category>Computer Version</category>
      
      <category>Paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
    <link href="/2021/05/03/YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/"/>
    <url>/2021/05/03/YOLOv4-Optimal-Speed-and-Accuracy-of-Object-Detection/</url>
    
    <content type="html"><![CDATA[<p>YOLOv4模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv4.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h1><p><img src="/img/5.3/1.jpg" alt="思维导图"></p><p>部分翻译：</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>有很多特征可以提高卷积神经网络（CNN）的准确性。需要在大型数据集上对这些特征的组合进行实际测试，并需要对结果进行理论证明。某些特征仅在某些模型上运行，并且仅在某些问题上运行，或者仅在小型数据集上运行；而某些特征（例如批归一化和残差连接）适用于大多数模型，任务和数据集。我们假设此类通用特征包括加权残差连接（WRC），跨阶段部分连接（CSP），交叉小批量标准化（CmBN），自对抗训练（SAT）和Mish激活。我们使用以下新功能：WRC，CSP，CmBN，SAT，Mish激活，马赛克数据增强，CmBN，DropBlock正则化和CIoU丢失，并结合其中的一些特征来实现最新的结果：在MS COCO数据集上利用Tesla V10以65 FPS的实时速度获得了43.5%的AP（65.7％AP50）。开源代码链接：<a href="https://github.com/AlexeyAB/darknet。">https://github.com/AlexeyAB/darknet。</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>大多数基于CNN的物体检测器仅适用于推荐系统。例如，通过慢速精确模型执行的城市摄像机搜索免费停车位，而汽车碰撞警告与快速不精确模型有关。提高实时物体检测器的精度不仅可以将它们用于提示生成推荐系统，还可以用于独立的过程管理和减少人工输入。常规图形处理单元（GPU）上的实时对象检测器操作允许以可承受的价格对其进行运行。最精确的现代神经网络无法实时运行，需要使用大量的GPU进行大量的mini-batch-size训练。我们通过创建在常规GPU上实时运行的CNN来解决此类问题，并且该培训仅需要一个传统的GPU。</p><p>这项工作的主要目标是在产生式系统中设计一个运行速度快的目标探测器，并对并行计算进行优化，而不是设计一个低计算量的理论指标(BFLOP)。我们希望所设计的对象易于训练和使用。如图1中的YOLOv4结果所示，任何人使用传统的GPU进行训练和测试，都可以获得实时、高质量和令人信服的目标检测结果。我们的贡献概括如下：</p><ol><li>我们开发了一个高效且功能强大的目标检测模型。它使每个人都可以使用1080Ti或2080TiGPU来训练超快、精确的物体探测器。</li><li>我们验证了当前最新的“免费袋”和“特殊袋”检测方法在探测器训练过程中的影响。</li><li>我们修改了最新的方法，使其更有效，更适合于单个GPU的培训，包括CBN[89]、PAN[49]、SAM[85]等。</li></ol><p><img src="/img/5.3/2.png" alt="MS COCO Object Detection"></p><h2 id="相关工作（Related-work）"><a href="#相关工作（Related-work）" class="headerlink" title="相关工作（Related work）"></a>相关工作（Related work）</h2><h3 id="目标检测算法（Object-detection-models）"><a href="#目标检测算法（Object-detection-models）" class="headerlink" title="目标检测算法（Object detection models）"></a>目标检测算法（Object detection models）</h3><p>目标检测算法一般有两部分组成：一个是在ImageNet预训练的骨架（backbone），，另一个是用来预测对象类别和边界框的头部。对于在GPU平台上运行的检测器，其骨干可以是VGG [68]，ResNet [26]，ResNeXt [86]或DenseNet [30]。对于在CPU平台上运行的那些检测器，其主干可以是SqueezeNet [31]，MobileNet [28、66、27、74]或ShuffleNet [97、53]。对于头部，通常分为两类，即一级对象检测器和二级对象检测器。最具有代表性的两级对象检测器是R-CNN [19]系列，包括fast R-CNN [18]，faster R-CNN [64]，R-FCN [9]和Libra R-CNN [ 58]。还可以使两级对象检测器成为无锚对象检测器，例如RepPoints [87]。对于一级目标检测器，最具代表性的模型是YOLO [61、62、63]，SSD [50]和RetinaNet [45]。近年来，开发了无锚的（anchor free）一级物体检测器。这类检测器是CenterNet [13]，CornerNet [37、38]，FCOS [78]等。近年来，无锚点单级目标探测器得到了发展，这类探测器有CenterNet[13]、CornerNet[37，38]、FCOS[78]等。近年来发展起来的目标探测器经常在主干和头部之间插入一些层，这些层通常用来收集不同阶段的特征图。我们可以称它为物体探测器的颈部。通常，颈部由几个自下而上的路径和几个自上而下的路径组成。具有这种机制的网络包括特征金字塔网络(FPN)[44]、路径聚集网络(PAN)[49]、BiFPN[77]和NAS-FPN[17]。除上述模型外，一些研究人员将重点放在直接构建用于检测对象的新主干（DetNet [43]，DetNAS [7]）或新的整个模型（SpineNet [12]，HitDe-tector [20]）上。</p><p>总而言之，普通的检测器由以下几个部分组成：</p><ul><li>输入：图像，斑块，图像金字塔</li><li>骨架：VGG16 [68]，ResNet-50 [26]，SpineNet [12]，EfficientNet-B0 / B7 [75]，CSPResNeXt50 [81]，CSPDarknet53 [81]</li><li><p>颈部：</p><ul><li>其他块：SPP [25]，ASPP [5]，RFB [47]，SAM [85]</li><li>路径聚合块：FPN [44]，PAN [49]，NAS-FPN [17] ]，Fully-connected FPN，BiFPN [77]，ASFF [48]，SFAM [98]</li></ul></li><li><p>Heads ：</p><ul><li>RPN[64]，SSD [50]，YOLO [61]， RetinaNet [45]（基于锚）</li><li>CornerNet[37]，CenterNet [13]，MatrixNet [60]，FCOS [78]（无锚）</li><li>密集预测（一阶段）：</li></ul></li><li><p>稀疏预测（两阶段）：</p><ul><li>Faster R-CNN [64]，R-FCN [9]，Mask R-CNN [23]（基于锚）</li><li>RepPoints[87]（无锚）</li></ul></li></ul><p><img src="/img/5.3/3.png" alt="Two-Stage Detector"></p><h3 id="Bag-of-freebies"><a href="#Bag-of-freebies" class="headerlink" title="Bag of freebies"></a>Bag of freebies</h3><p>通常，传统的物体检测器是离线训练的。因此，研究人员一直喜欢采用这种优势并开发出更好的训练方法，从而可以使目标检测器获得更好的精度而又不会增加推理成本。我们称这些仅改变培训策略或仅增加培训成本的方法为“免费赠品”。数据检测是对象检测方法经常采用的并符合免费赠品的定义。数据增强的目的是增加输入图像的可变性，从而使设计的物体检测模型对从不同环境获得的图像具有更高的鲁棒性。例如，光度失真和几何失真是两种常用的数据增强方法，它们无疑有利于物体检测任务。在处理光度失真时，我们调整图像的亮度，对比度，色相，饱和度和噪点。对于几何失真，我们添加了随机缩放，裁剪，翻转和旋转。</p><p>上面提到的数据增强方法是全像素调整，并且保留了调整区域中的所有原始像素信息。此外，一些从事数据增强的研究人员将重点放在模拟对象遮挡问题上。他们在图像分类和目标检测方面取得了良好的成果。例如，random erase[100]和CutOut [11]可以随机选择图像中的矩形区域，并填充零的随机或互补值。至于hide-and-seek[69]和grid mask[6]，他们随机或均匀地选择图像中的多个矩形区域，并将其替换为所有零。如果类似的概念应用于要素地图，则有DropOut [71]，DropConnect [80]和DropBlock [16]方法。另外，一些研究人员提出了使用多个图像一起执行数据增强的方法。例如，MixUp [92]使用两个图像对具有不同系数的图像进行乘法和叠加，然后使用这些叠加的系数来调整标签。对于CutMix [91]，它是将裁切后的图像覆盖到其他图像的矩形区域，并根据混合区域的大小调整标签。除了上述方法之外，样式转移GAN [15]还用于数据扩充，这种用法可以有效地减少CNN所学习的纹理偏差。</p><p>与上面提出的各种方法不同，其他一些免费赠品方法专用于解决数据集中语义分布可能存在偏差的问题。在处理语义分布偏向问题时，一个非常重要的问题是不同类之间存在数据不平衡的问题，这一问题通常是通过两阶段对象设计器中的难例挖掘[72]或在线难例挖掘[67]来解决的。但是实例挖掘方法不适用于一级目标检测器，因为这种检测器属于密集预测架构。因此Linet等 [45]提出了焦点损失，以解决各个类别之间存在的数据不平衡问题。另一个非常重要的问题是，很难用one-hot representation来表达不同类别之间的关联度。这种表示方法经常在执行标签时使用。[73]中提出的标签平滑是将硬标签转换为软标签以进行训练，这可以使模型更加健壮。为了获得更好的软标签，Islamet等人[33]引入知识蒸馏的概念来设计标签优化网络。</p><p>最后一袋赠品是边界框回归的目标函数。传统的目标检测器通常使用均方误差(MSE)直接对BBox的中心点坐标和高度、宽度进行回归，即{xcenter，ycenter，w，h}，或者对左上角和右下点，即{xtopleft，ytopleft，xbottomright，ybottomright}进行回归。对于基于锚点的方法，是估计相应的偏移量，例如{xcenterOffset，ycenterOffset，wOffset，hoffset}和{xtopleftoffset，ytopleftoffset，xbottomright toffset，ybottomright toffset}，例如{xtopleftoffset，ytopleftoffset}和{xtopleftoffset，ytopleftoffset}。然而，直接估计BBox中每个点的坐标值是将这些点作为独立变量来处理，而实际上并没有考虑对象本身的完整性。为了更好地处理这一问题，最近一些研究人员提出了IoU loss[90]，将预测BBOX区域的覆盖率和地面真实BBOX区域的覆盖率考虑在内。IOU loss计算过程将触发BBOX的四个坐标点的计算，方法是执行具有地面实况的借条，然后将生成的结果连接到一个完整的代码中。由于IOU是一种标度不变的表示，它可以解决传统方法计算{x，y，w，h}的l1或l2个损失时，损失会随着尺度的增大而增大的问题。最近，搜索者不断改善欠条损失。例如，GIOU损失[65]除了包括覆盖区域外，还包括对象的形状和方向。他们提出找出能同时覆盖预测BBOX和实际BBOX的最小面积BBOX，并用这个BBOX作为分母来代替原来在欠条损失中使用的分母。对于DIoU loss[99]，它另外考虑了物体中心的距离，而CIoU损失[99]则同时考虑了重叠面积、中心点之间的距离和纵横比。在求解BBox回归问题时，Ciou可以达到较好的收敛速度和精度。</p><h3 id="Bag-of-spedials"><a href="#Bag-of-spedials" class="headerlink" title="Bag of spedials"></a>Bag of spedials</h3><p>对于那些仅增加少量推理成本但可以显着提高对象检测准确性的插件模块和后处理方法，我们将其称为“特价商品”。一般而言，这些插件模块用于增强模型中的某些属性，例如扩大接受域，引入注意力机制或增强特征集成能力等，而后处理是用于筛选模型预测结果的方法。</p><p>可以用来增强接收域的通用模块是SPP [25]，ASPP [5]和RFB [47]。SPP模块起源于空间金字塔匹配（SPM）[39]，SPM的原始方法是将功能图分割成若干x不等的块，其中{1,2,3，…}可以是空间金字塔，然后提取词袋特征。SPP将SPM集成到CNN中，并使用最大池操作而不是词袋运算。由于Heet等人提出了SPP模块。[25]将输出一维特征向量，在全卷积网络（FCN）中应用是不可行的。因此，在YOLOv3的设计中[63]，Redmon和Farhadi将SPP模块改进为内核大小为k×k的最大池输出的串联，其中k = {1,5,9,13}，步长等于1。在这种设计下，相对大k×kmax池有效地增加了骨干特征的接受范围。在添加了改进版本的SPP模块之后，YOLOv3-608在MS COCOobject检测任务上将AP50升级了2.7％，而额外的计算费用为0.5％。ASPP[5]模块和改进的SPP模块之间的操作差异主要来自于原始k× kkernel大小，最大卷积步长等于1到3×3内核大小，膨胀比等于tok，步长等于1。RFB模块将使用k×kkernel的几个扩张卷积，扩张比率equalstok和步幅等于1来获得比ASPP更全面的空间覆盖范围。RFB [47]仅花费7％的额外推断时间即可将MS COCO上SSD的AP50提高5.7％。</p><p>物体检测中常用的注意模块主要分为通道式注意和点式注意，这两种注意模型的代表分别是挤压激发(SE)[29]和空间注意模块(SAM)[85]。虽然SE模块在Im-ageNet图像分类任务中可以提高1%的TOP-1准确率，但是在GPU上通常会增加10%左右的推理时间，因此更适合在移动设备上使用，虽然SE模块在Im-ageNet图像分类任务中可以提高1%的TOP-1准确率，但是在GPU上通常会增加10%左右的推理时间。而对于SAM，它只需要额外支付0.1%的计算量，在ImageNet图像分类任务上可以提高ResNet50-SE 0.5%的TOP-1准确率。最棒的是，它完全不影响GPU上的推理速度。</p><p>在特征集成方面，早期的实践是使用KIP连接[51]或超列[22]将低级物理特征集成到高级语义特征。随着模糊神经网络等多尺度预测方法的普及，人们提出了许多集成不同特征金字塔的轻量级模块。这种类型的模块包括SfAM[98]、ASFF[48]和BiFPN[77]。SfAM的主要思想是使用SE模块对多比例尺拼接的特征地图进行通道级的加权。ASFF采用Softmax作为逐点层次加权，然后添加不同尺度的特征地图；BiFPN采用多输入加权残差连接进行尺度层次重新加权，再添加不同尺度的特征地图。</p><p>在深度学习的研究中，有些人专注于寻找良好的激活功能。良好的激活函数可以使梯度更有效地传播，同时不会引起过多的计算成本。在2010年，Nair和Hin-ton [56]提出了ReLU，以基本上解决传统tanh和sigmoid激活函数中经常遇到的梯度消失问题。随后，LReLU [54]，PReLU [24]，ReLU6 [28]，比例指数线性单位（SELU）[35]，Swish [59]，hard-Swish [27]和Mish [55]等，它们也是已经提出了用于解决梯度消失问题的方法。LReLU和PReLU的主要目的是解决当输出小于零时ReLU的梯度为零的问题。至于ReLU6和hard-Swish，它们是专门为量化网络设计的。为了对神经网络进行自归一化，提出了SELU激活函数来满足这一目标。要注意的一件事是，Swish和Mishare都具有连续可区分的激活功能。</p><p>在基于深度学习的对象检测中通常使用的后处理方法是NMS，它可以用于过滤那些无法预测相同对象的BBox，并仅保留具有较高响应速度的候选BBox。NMS尝试改进的方法与优化目标函数的方法一致。NMS提出的原始方法没有考虑上下文信息，因此Girshicket等人。[19]在R-CNN中添加了分类置信度得分作为参考，并且根据置信度得分的顺序，从高分到低分的顺序执行贪婪的NMS。对于软网络管理系统[1]，考虑了一个问题，即物体的遮挡可能会导致带有IoU评分的贪婪的网络管理系统的置信度得分下降。DIoU NMS [99]开发人员的思维方式是在softNMS的基础上将中心距离的信息添加到BBox筛选过程中。值得一提的是，由于上述后处理方法均未直接涉及捕获的图像功能，因此在随后的无锚方法开发中不再需要后处理。</p><h2 id="方法（Methodology）"><a href="#方法（Methodology）" class="headerlink" title="方法（Methodology）"></a>方法（Methodology）</h2><p>其基本目标是在生产系统中对神经网络进行快速操作，并针对并行计算进行优化，而不是低计算量理论指示器（BFLOP）。我们提供了实时神经网络的两种选择：</p><ul><li>对于GPU，我们使用少量的组（1-8）卷积层：CSPResNeXt50 / CSPDarknet53</li><li>对于VPU-我们使用分组卷积，但是我们不再使用挤压和激发（SE）块-特别是这包括以下模型：EfficientNet-lite / MixNet [76] / GhostNet [21] / Mo-bileNetV3</li></ul><h3 id="Selection-of-architecture"><a href="#Selection-of-architecture" class="headerlink" title="Selection of architecture"></a>Selection of architecture</h3><p>我们的目标是在输入网络分辨率，卷积层数，参数数（filtersize2 过滤器通道/组）和层输出（过滤器）数目之间找到最佳平衡。例如，大量研究表明，在ILSVRC2012（ImageNet）数据集的对象分类方面，CSPResNext50比CSPDarknet53更好。然而，相反，就检测MS COCO数据集上的对象而言，CSPDarknet53比CSPResNext50更好。</p><p>下一个目标是为不同的检测器级别从不同的主干级别中选择其他块以增加接收场和参数聚集的最佳方法：FPN，PAN，ASFF，BiFPN。</p><p>对于分类最佳的参考模型对于检测器并非总是最佳的。与分类器相比，检测器需要满足以下条件：</p><ul><li>更大的网络输入，用于检测小目标</li><li>更多的层-以获得更大的感受野来覆盖增大的输入图像</li><li>更多的参数-为了增强从单张图像中检测出不同大小的多个对象的能力</li></ul><p>假设可以选择接受场较大(卷积层数为3×3)和参数数较多的模型作为主干。表1显示了CSPResNeXt50、CSPDarknet53和Effi-cientNet B3的信息。CSPResNext50只包含16个卷积层3×3，425×425感受野和20.6M参数，而CSPDarknet53包含29个卷积层3×3，725×725感受野和27.6M参数。这一理论证明，再加上我们的大量实验，表明CSPDarknet53神经网络是<strong>两者作为探测器骨干的最佳模型</strong>。</p><p><img src="/img/5.3/4.png" alt="Parameters of neural networks for image classification"></p><p>不同大小的感受野对检测效果的影响如下所示：</p><ul><li>最大对象大小-允许查看整个对象</li><li>最大网络大小-允许查看对象周围的上下文</li><li>超过网络大小-增加图像点和最终激活之间的连接</li></ul><p>我们在CSPDarknet53上添加SPP块，因为它显著增加了接受场，分离出最重要的上下文特征，并且几乎不会降低网络操作速度。我们使用PANET代替YOLOv3中使用的FPN作为不同骨级的参数聚合方法，用于不同的检测器级别。</p><p>最后，我们选择了CSPDarknet53主干、SPP附加模块、PANET路径聚合Neck和YOLOv3(基于锚点的)头部作为YOLOv4的体系结构。</p><p>将来，我们计划显着扩展检测器的赠品袋（BoF）的内容，从理论上讲，它可以解决一些问题并提高检测器的准确性，并以实验方式依次检查每个功能的影响。</p><p>我们不使用跨GPU批量标准化（CGBNor SyncBN）或昂贵的专用设备。这使任何人都可以在传统的图形处理器上重现我们的最新成果，例如GTX 1080Ti或RTX2080Ti。</p><h3 id="Selection-of-BoF-and-BoS"><a href="#Selection-of-BoF-and-BoS" class="headerlink" title="Selection of BoF and BoS"></a>Selection of BoF and BoS</h3><p>为了改进目标检测训练，CNN通常使用以下方法:</p><ul><li>激活：ReLU，leaky-ReLU，parameter-ReLU，ReLU6，SELU，Swish或Mish</li><li>边界框回归损失：MSE，IoU，GIoU，CIoU，DIoU</li><li>数据增强：CutOut，MixUp，CutMix</li><li>正则化方法：DropOut， DropPath [36]，Spatial DropOut [79]或DropBlock</li><li>通过均值和方差对网络激活进行归一化：Batch Normalization (BN) [32],Cross-GPU Batch Normalization (CGBN or SyncBN)[93], Filter Response Normalization (FRN) [70], orCross-Iteration Batch Normalization (CBN) [89]</li><li>跨连接：Residual connections, Weightedresidual connections, Multi-input weighted residualconnections, or Cross stage partial connections (CSP)</li></ul><p>至于训练激活功能，由于PReLU和SELU更难训练，并且ReLU6是专门为量化网络设计的，因此我们从候选列表中删除了上述激活功能。在重新量化方法中，发布Drop-Block的人们将自己的方法与其他方法进行了详细的比较，而其正则化方法赢得了很多。因此，我们毫不犹豫地选择了DropBlock作为我们的正则化方法。至于标准化方法的选择，由于我们专注于仅使用一个GPU的训练策略，因此不考虑syncBN。</p><h3 id="Additional-improvements"><a href="#Additional-improvements" class="headerlink" title="Additional improvements"></a>Additional improvements</h3><p>为了使设计的检测器更适合在单个GPU上进行训练，我们进行了以下附加设计和改进：</p><ul><li>我们引入了一种新的数据增强方法：Mosaic, and Self-Adversarial Training (SAT)</li><li>在应用遗传算法时，我们选择最优的超参数</li><li>我们修改了一些现有方法，使我们的设计适合进行有效的训练和检测-改进的SAM，改进的PAN ，以及跨小批量标准化（CmBN）</li></ul><p>Mosaic是一种新的混合4幅训练图像的数据增强方法。所以四个不同的上下文信息被混合，而CutMix只混合了2种。</p><p>这允许检测其正常上下文之外的对象。此外，批量归一化从每层上的4个不同的图像计算激活统计。这极大地减少了对large mini-batch-size的需求。</p><p>自对抗训练(SAT)也代表了一种新的数据增强技术，它在两个前向后向阶段运行。在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对其自身执行对抗性攻击，改变原始图像，以制造图像上没有所需对象的欺骗。在第二阶段，训练神经网络，以正常的方式在修改后的图像上检测目标。</p><p>CmBN表示CBN的修改版本，如图4所示，定义为交叉小批量规范化(Cross mini-Batch Normalization，CMBN)。这仅在单个批次内的小批次之间收集统计信息。</p><p><img src="/img/5.3/5.png" alt="Cross mini-Batch Normalization"></p><p>我们将SAM从空间注意修改为点注意，并将PAN的快捷连接分别替换为串联:</p><p><img src="/img/5.3/6.png" alt="Modified SAN"></p><h3 id="YOLOv4"><a href="#YOLOv4" class="headerlink" title="YOLOv4"></a>YOLOv4</h3><p>YOLOv4的组成：</p><ul><li>Backbone:CSPDarknet53[81]</li><li>Neck:SPP[25],PAN[49]</li><li>Head:YOLOv3[63]</li></ul><p>YOLOv4的使用：</p><p><img src="/img/5.3/7.png" alt="YOLOv4 uses"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>我们在ImageNet(ILSVRC 2012 Val)数据集上测试了不同训练改进技术对分类器精度的影响，然后在MS Coco(test-dev 2017)数据集上测试了不同训练改进技术对检测器精度的影响。</p><h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p>在ImageNet图像分类实验中，缺省超参数如下：训练步数为800万步；batch size和mini-batch size分别为128和32；采用多项式衰减学习率调度策略，初始学习率为0.1；预热步数为1000步；动量和权值分别设置为0.9和0.005。我们所有的BoS实验都使用与默认设置相同的超参数，并且在BoF实验中，我们增加了50%的训练步骤。在BoF实验中，我们验证了MixUp、CutMix、Mosaic、Bluring数据增强和标记平滑正则化方法。在BoS实验中，我们比较了LReLU、SWISH和MISHISH激活函数的效果。所有实验均使用1080Ti或2080Ti GPU进行训练。</p><p>在MS COCO目标检测实验中，缺省超参数如下：训练步数为500,500；采用阶跃衰减学习率调度策略，初始学习率为0.01，在400，000步和450，000步分别乘以因子0.1；动量衰减和权重衰减分别设置为0.9和0.0005。所有的体系结构都使用单个GPU来执行批处理大小为64的多尺度训练，而小批处理大小为8或4，具体取决于体系结构和GPU内存的限制。除采用遗传算法进行超参数搜索实验外，其余实验均采用默认设置。遗传算法使用YOLOv3-SPP算法在有GIoU损失的情况下进行训练，在300个历元中搜索Min-Val5k集。我们采用搜索学习率0.00261，动量0.949，IOU阈值分配地面真值0.213，损失归一化0.07%进行遗传算法实验。我们验证了大量的BoF算法，包括网格敏感度消除、moSAIC数据增强、IOU阈值、遗传算法、类标签平滑、交叉小批量归一化、自对抗训练、余弦退火调度器、动态小批量大小、DropBlock、优化锚点、不同类型的IOU损失。我们还在不同的BoS上进行了实验，包括MISH、SPP、SAM、RFB、BiFPN和Gaus-Sian YOLO[8]。对于所有的实验，我们只使用一个GPU进行训练，因此不使用诸如优化多个GPU的syncBN之类的技术。</p><h3 id="Influence-of-different-features-on-Classifier-training"><a href="#Influence-of-different-features-on-Classifier-training" class="headerlink" title="Influence of different features on Classifier training"></a>Influence of different features on Classifier training</h3><p>首先，我们研究了不同特征对分类器训练的影响；具体地说，类标签平滑的影响，不同数据扩充技术的影响，双边模糊，混合，CutMix和马赛克，如图7所示，以及不同活动的影响，如Leaky-relu(默认情况下)，SWISH和MISH。</p><p><img src="/img/5.3/8.png" alt="Various method of data augmentation"></p><p>在我们的实验中，如表2所示，通过引入诸如：CutMix和Mosaic数据增强、类标签平滑和Mish激活等特征，提高了分类器的精度。因此，我们用于分类器训练的BoF-Backbone(Bag Of Freebies)包括以下内容：CutMix和Mosaic数据增强以及类标签平滑。此外，我们还使用MISH激活作为补充选项，如表2和表3所示。</p><p><img src="/img/5.3/9.png" alt="Influence of BoF and Mish on the CSPResNeXt-50 classifier accuracy"></p><h3 id="Influence-of-different-features-on-Detector-training"><a href="#Influence-of-different-features-on-Detector-training" class="headerlink" title="Influence of different features on Detector training"></a>Influence of different features on Detector training</h3><p>进一步的研究涉及不同的免费袋(BOF探测器)对探测器训练精度的影响，如表4所示。我们通过研究在不影响FPS的情况下提高探测器精度的不同特性，显著扩展了BOF列表：</p><p><img src="/img/5.3/10.png" alt="BOF列表"></p><p>进一步研究了不同的专业袋(BOS检测器)对检测器训练精度的影响，包括PAN、RFB、SAM、高斯YOLO(G)和ASFF，如表5所示。在我们的实验中，当使用SPP、PAN和SAM时，检测器的性能最佳。</p><p><img src="/img/5.3/11.png" alt="Ablation Studies of Bag-of-Freebies."></p><h3 id="Influence-of-different-backbones-and-pretrained-weightings-on-Detector-training"><a href="#Influence-of-different-backbones-and-pretrained-weightings-on-Detector-training" class="headerlink" title="Influence of different backbones and pretrained weightings on Detector training"></a>Influence of different backbones and pretrained weightings on Detector training</h3><p>进一步，我们研究了不同主干模型对检测器精度的影响，如表6所示。请注意，具有最佳分类精度的模型在检测器精度方面并不总是最佳。</p><p><img src="/img/5.3/12.png" alt="Using different classifier pre-trained weightings for detector training"></p><p>首先，尽管与CSPDarknet53模型相比，经过不同功能训练的CSPResNeXt-50模型的分类准确性更高，但CSPDarknet53模型在对象检测方面显示出更高的准确性。</p><p>其次，使用BoF和Mish进行CSPResNeXt50分类器训练会提高其分类准确性，但是将这些预先训练的权重进一步应用于检测器训练会降低检测器准确性。但是，将BoF和Mish用于CSPDarknet53分类器训练可以提高分类器和使用该分类器预训练加权的检测器的准确性。结果是，与CSPResNeXt50相比，主干CSPDarknet53更适合于检测器。</p><p>我们观察到，由于各种改进，CSPDarknet53模型具有更大的能力来提高检测器精度。</p><h3 id="Influence-of-different-mini-batch-size-on-Detector-training"><a href="#Influence-of-different-mini-batch-size-on-Detector-training" class="headerlink" title="Influence of different mini-batch size on Detector training"></a>Influence of different mini-batch size on Detector training</h3><p>最后，我们分析了在不同最小批量大小下训练的模型所获得的结果，结果如表7所示。从表7所示的结果中，我们发现在添加BoF和BoS训练策略之后，最小批量大小几乎没有影响在检测器的性能上。该结果表明，在引入BoF和BoS之后，不再需要使用昂贵的GPU进行训练。换句话说，任何人都只能使用传统的GPU来训练出色的探测器。</p><p><img src="/img/5.3/13.png" alt="Using defferernt mini-batch size for detector training"></p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>与其他最先进的物体探测器获得的结果比较如图8所示。YOLOv4在速度和准确性方面均优于最快，为最准确的探测器。</p><p><img src="/img/5.3/14.png" alt="Comparison of the speed and accuracy of differert object detectors"></p><p>由于不同的方法使用不同体系结构的GPU进行推理时间验证，我们在常用的Maxwell、Pascal和VoltaArchitecture体系结构的GPU上运行YOLOv4，并将它们与其他先进的方法进行了比较。表8列出了使用Maxwell GPU的帧率比较结果，可以是GTX Titan X(Maxwell)或Tesla M40 GPU。表9列出了使用Pascal GPU的帧率比较结果，它可以是Titan X(Pascal)、Titan XP、GTX 1080 Ti或Tesla P100 GPU。表10列出了使用VoltaGPU的帧率对比结果，可以是Titan Volta，也可以是Tesla V100 GPU。</p><p><img src="/img/5.3/15.png" alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p><p><img src="/img/5.3/16.png" alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p><p><img src="/img/5.3/17.png" alt="Comparison of the speed and accuracy of differernt object detectors on the MS COCO dataset"></p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>我们提供最先进的检测器，其速度（FPS）和准确度（MS COCO AP50 … 95和AP50）比所有可用的替代检测器都高。所描述的检测器可以在具有8-16GB-VRAM的常规GPU上进行训练和使用，这使得它的广泛使用成为可能。一阶段基于锚的探测器的原始概念已证明其可行性。我们已经验证了大量特征，并选择使用其中的一些特征以提高分类器和检测器的准确性。这些功能可以用作将来研究和开发的最佳实践。</p><h2 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h2><p>作者要感谢Glenn Jocher进行Mosaic数据增强的想法，通过使用遗传算法选择超参数并解决网格敏感性问题的方法<a href="https://github.com/ultralytics/yolov3.10。">https://github.com/ultralytics/yolov3.10。</a></p><h1 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h1><h2 id="YOLOv4结构"><a href="#YOLOv4结构" class="headerlink" title="YOLOv4结构"></a>YOLOv4结构</h2><p><img src="/img/5.3/18.png" alt="YOLOv4"></p><p>YOLOv4 consists of:</p><ul><li>Backbone:CSPDarknet53[81]</li><li>Neck:SPP[25],PAN[49]</li><li>Head:YOLOv3[63]</li></ul><p>上图为论文原图，形象将模型分为四部分：input，Backbone、Neck、Head</p><h3 id="结构讲解："><a href="#结构讲解：" class="headerlink" title="结构讲解："></a>结构讲解：</h3><p>假设输入图像大小为416×416，可以看到主干网络Darknet53–&gt;CSPDarknet53，激活函数由yolov3的Leak-Relu–&gt;Mish，Mish激活函数公式与图像如下：</p><p><img src="/img/5.3/19.png" alt="Mish"></p><p>分析一下网络结构：</p><ol><li>主干网络Backbone</li></ol><ul><li>首先是输入层输入(416，416，3)的图，经过DarknetConv2D_BN_Mish（图中的Conv2d_BN_Mish）卷积块，卷积块由DarknetConv2D、BatchNormalization（BN）、Mish三部分组成，即该卷积结构包括l2正则化、批标准化、Mish激活函数这一系列操作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   单次卷积</span><br><span class="hljs-meta">@wraps(<span class="hljs-params">Conv2D</span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    darknet_conv_kwargs = &#123;<span class="hljs-string">&#x27;kernel_regularizer&#x27;</span>: l2(<span class="hljs-number">5e-4</span>)&#125;<br>    darknet_conv_kwargs[<span class="hljs-string">&#x27;padding&#x27;</span>] = <span class="hljs-string">&#x27;valid&#x27;</span> <span class="hljs-keyword">if</span> kwargs.get(<span class="hljs-string">&#x27;strides&#x27;</span>)==(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;same&#x27;</span><br>    darknet_conv_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> Conv2D(*args, **darknet_conv_kwargs)<br><br><span class="hljs-comment">#   卷积块 DarknetConv2D + BatchNormalization + Mish</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D_BN_Mish</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    no_bias_kwargs = &#123;<span class="hljs-string">&#x27;use_bias&#x27;</span>: <span class="hljs-literal">False</span>&#125;<br>    no_bias_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> compose(<br>        DarknetConv2D(*args, **no_bias_kwargs),<br>        BatchNormalization(),<br>        Mish())<br></code></pre></td></tr></table></figure><ul><li>然后后面有5个CSPResblockBody，分别为×1，×2，×8，×8，×4，表示重复多少次resblock_body，可以看到该残差结构块中先有个ZeroPadding2D对二维矩阵的四周填充0，即零填充层，然后三个DarknetConv2D_BN_Mish卷积块，第二个DarknetConv2D_BN_Mish卷积块会生成一个大的残差边（后面会解释残差边），然后有个for循环对通道进行整合[compose]与特征提取，这个循环多少次就是图中×1，×2，×8等。，在后面进行一次1x1DarknetConv2D_BN_Mish卷积，再concatenate堆叠拼接，最后再对通道数进行整合并返回。可以看到，resblockbody中有大量的DarknetConv2D_BN_Mish卷积块进行卷积提取特征。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   CSPdarknet的结构块</span><br><span class="hljs-comment">#   存在一个大残差边</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resblock_body</span>(<span class="hljs-params">x, num_filters, num_blocks, all_narrow=<span class="hljs-literal">True</span></span>):</span><br>    <span class="hljs-comment"># 进行长和宽的压缩</span><br>    preconv1 = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(x)<br>    preconv1 = DarknetConv2D_BN_Mish(num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(preconv1)<br><br>    <span class="hljs-comment"># 生成一个大的残差边 </span><br>    shortconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(preconv1)<br><br>    <span class="hljs-comment"># 主干部分的卷积</span><br>    mainconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(preconv1)<br>    <span class="hljs-comment"># 1x1卷积对通道数进行整合-&gt;3x3卷积提取特征，使用残差结构</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks):<br>        y = compose(<br>                DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)))(mainconv)<br>        mainconv = Add()([mainconv,y])<br>    <span class="hljs-comment"># 1x1卷积后和残差边堆叠</span><br>    postconv = DarknetConv2D_BN_Mish(num_filters//<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> all_narrow <span class="hljs-keyword">else</span> num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(mainconv)<br>    route = Concatenate()([postconv, shortconv])<br><br>    <span class="hljs-comment"># 最后对通道数进行整合</span><br>    <span class="hljs-keyword">return</span> DarknetConv2D_BN_Mish(num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(route)<br></code></pre></td></tr></table></figure><p>CSPDarknet（yolov4）与Darknet（yolov3）的resbodyblock不一样地方就是使用了CSPnet结构</p><p><img src="/img/5.3/20.png" alt="net结构"></p><p>CSPnet结构将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：<br>主干部分还是进行残差快的堆叠（Part 2）<br>另一部分则像是一条长边一样，经过少量处理直接连接到最后。<br>因此可以认为CSP中存在一个大的残差边。<br>残差边的作用：残差结构可以不通过卷积与堆叠，直接从前面一个特征层映射到后面的特征层（跳跃连接），有助于训练，也有助于特征的提取。<br>主干网络代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#   CSPdarknet53 的主体部分</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet_body</span>(<span class="hljs-params">x</span>):</span><br>    x = DarknetConv2D_BN_Mish(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(x)<br>    x = resblock_body(x, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-literal">False</span>)<br>    x = resblock_body(x, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>)<br>    x = resblock_body(x, <span class="hljs-number">256</span>, <span class="hljs-number">8</span>)<br>    feat1 = x<br>    x = resblock_body(x, <span class="hljs-number">512</span>, <span class="hljs-number">8</span>)<br>    feat2 = x<br>    x = resblock_body(x, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span>)<br>    feat3 = x<br>    <span class="hljs-keyword">return</span> feat1,feat2,feat3<br></code></pre></td></tr></table></figure><p>darknet_body(x)中的x即为输入传递来的图像数组(416，416，3)，然后图像先通过 DarknetConv2D_BN_Mish卷积块进行卷积，得到shape为(416,416,32)，再进行5次resblockbody，每一次宽高都减半，深度翻倍，其中feat1，feat2，feat3是对当时的特征层做一个记录，因为后面还需用到，变化如下图。</p><p><img src="/img/5.3/21.png" alt="网络结构"></p><ol><li>特征金字塔部分SSP与PAN</li></ol><p>相比YOLOv3，YOLOV4结合了两种改进:</p><ul><li>使用了SPP结构。</li><li>使用了PANet结构。</li></ul><p>注：除去CSPDarknet53主干网络和Yolo Head的结构外，都是特征金字塔的结构。</p><ul><li>SSP结构</li></ul><p>首先SSP结构进行了三次DarknetConv2D_BN_Leaky卷积，你没看错，这里不是Mish，分别利用四个不同尺度的最大池化进行处理，最大池化的池化核大小分别为13x13、9x9、5x5、1x1（1x1–无处理），该结构能分离出最显著的上下文特征，是强有力的特征提取，池化后，再进行堆叠。如下图</p><p><img src="/img/5.3/22.png" alt="SSP结构"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成darknet53的主干模型</span><br>    feat1,feat2,feat3 = darknet_body(inputs)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat3)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br>    <span class="hljs-comment"># 使用了SPP结构，即不同尺度的最大池化后堆叠。</span><br>    maxpool1 = MaxPooling2D(pool_size=(<span class="hljs-number">13</span>,<span class="hljs-number">13</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    maxpool2 = MaxPooling2D(pool_size=(<span class="hljs-number">9</span>,<span class="hljs-number">9</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    maxpool3 = MaxPooling2D(pool_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>)(P5)<br>    P5 = Concatenate()([maxpool1, maxpool2, maxpool3, P5])<br></code></pre></td></tr></table></figure><p>简单描述下代码，首先darknet_body(inputs)可以获得返回的三个参数feat1，feat2，feat3，其中feat3为主干网络经过5个网络输出的特征层，然后对它进行3次 DarknetConv2D_BN_Leaky卷积，再经过三个最大池化分离特征，一个1×1池化即不变，还是P5，最后用 Concatenate()将四个进行堆叠。</p><ul><li>PANet</li></ul><p>PANet是2018年发表的一种实例分割算法，它可以反复提取特征。</p><p><img src="/img/5.3/23.png" alt="PAN结构"></p><p><img src="/img/5.3/24.png" alt="PAN结构"></p><p>整个过程大致来讲就是上采样、再堆叠卷积重复，之后再进行下采样、堆叠，如上图，上采样是放大，下采样是压缩。</p><ol><li>输出结果yolo head</li></ol><p>与yolov3中一样，内部是一个3×3卷积，一个是1×1卷积，进行通道调整<br>最后得到三个输出(52,52,75)，(26,26,75)，(13,13,75)<br>注：假设基于VOC数据集</p><p><img src="/img/5.3/25.png" alt="PAN结构"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 池化堆叠后的四次卷积，注意compose中还有一次</span><br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5 = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5)<br><br><span class="hljs-comment"># 上采样</span><br>    P5_upsample = compose(DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), UpSampling2D(<span class="hljs-number">2</span>))(P5)<br>    <br>    <span class="hljs-comment"># 与左边主干网络的特征层feat2进行拼接</span><br>    P4 = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat2)<br>    P4 = Concatenate()([P4, P5_upsample])<br>    <span class="hljs-comment"># Conv2d×5</span><br>    P4 = make_five_convs(P4,<span class="hljs-number">256</span>)<br><br><span class="hljs-comment"># 上采样</span><br>    P4_upsample = compose(DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), UpSampling2D(<span class="hljs-number">2</span>))(P4)<br>    <br>    <span class="hljs-comment"># 与左边主干网络的特征层feat1进行拼接</span><br>    P3 = DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(feat1)<br>    P3 = Concatenate()([P3, P4_upsample])<br>    <span class="hljs-comment"># Conv2d×5</span><br>    P3 = make_five_convs(P3,<span class="hljs-number">128</span>)<br><br>    <span class="hljs-comment"># 52x52的输出yolo head1，若是VOC数据集，则3×(20+5)=75，shape为(52,52,75)</span><br>    <span class="hljs-comment"># conv2d 3*3</span><br>    P3_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P3)<br>    <span class="hljs-comment"># con2d 1*1</span><br>    P3_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P3_output)<br><br><span class="hljs-comment"># 注意一下，这里下采样有个零填充即可</span><br>    P3_downsample = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(P3)<br>    P3_downsample = DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(P3_downsample)<br>    P4 = Concatenate()([P3_downsample, P4])<br>    P4 = make_five_convs(P4,<span class="hljs-number">256</span>)<br>    <br>    <span class="hljs-comment"># 38x38的out</span><br>    P4_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P4)<br>    P4_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P4_output)<br>    <br><br>    P4_downsample = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(P4)<br>    P4_downsample = DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(P4_downsample)<br>    P5 = Concatenate()([P4_downsample, P5])<br>    P5 = make_five_convs(P5,<span class="hljs-number">512</span>)<br>    <br>    <span class="hljs-comment"># 19x19的out</span><br>    P5_output = DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(P5)<br>    P5_output = DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))(P5_output)<br>    <span class="hljs-keyword">return</span> Model(inputs, [P5_output, P4_output, P3_output])<br></code></pre></td></tr></table></figure><p>输出三个yolo head后还需对其进行解码，因为这个yolo head预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。这部分在yolov3文章的第二、三步已经写了，可以直接看2、3步，链接如下：<br><a href="https://blog.csdn.net/weixin_39615182/article/details/109752498">yolov3原理详解</a></p><p>下面讲下实现预测结果解码与得分排序、非极大抑制筛选两部分代码<br>解码是为了得到真实的边界框，但是不只一个，通过得分排序和非极大抑制筛选得到众多边框中最准确的一个，即最优的一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   将预测值的每个特征层调成真实值</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_head</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, calc_loss=<span class="hljs-literal">False</span></span>):</span><br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    <span class="hljs-comment"># [1, 1, 1, num_anchors, 2]</span><br>    anchors_tensor = K.reshape(K.constant(anchors), [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, num_anchors, <span class="hljs-number">2</span>])<br><br>    <span class="hljs-comment"># 获得x，y的网格</span><br>    <span class="hljs-comment"># (13, 13, 1, 2)</span><br>    grid_shape = K.shape(feats)[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] <span class="hljs-comment"># height, width</span><br>    grid_y = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">0</span>]), [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid_x = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">1</span>]), [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [grid_shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid = K.concatenate([grid_x, grid_y])<br>    grid = K.cast(grid, K.dtype(feats))<br><br>    <span class="hljs-comment"># (batch_size,19,19,3,85)</span><br>    feats = K.reshape(feats, [-<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">0</span>], grid_shape[<span class="hljs-number">1</span>], num_anchors, num_classes + <span class="hljs-number">5</span>])<br><br>    <span class="hljs-comment"># 将预测值调成真实值</span><br>    <span class="hljs-comment"># box_xy对应框的中心点</span><br>    <span class="hljs-comment"># box_wh对应框的宽和高</span><br>    box_xy = (K.sigmoid(feats[..., :<span class="hljs-number">2</span>]) + grid) / K.cast(grid_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))<br>    box_wh = K.exp(feats[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]) * anchors_tensor / K.cast(input_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))<br>    box_confidence = K.sigmoid(feats[..., <span class="hljs-number">4</span>:<span class="hljs-number">5</span>])<br>    box_class_probs = K.sigmoid(feats[..., <span class="hljs-number">5</span>:])<br><br>    <span class="hljs-comment"># 在计算loss的时候返回如下参数</span><br>    <span class="hljs-keyword">if</span> calc_loss == <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">return</span> grid, feats, box_xy, box_wh<br>    <span class="hljs-keyword">return</span> box_xy, box_wh, box_confidence, box_class_probs<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   对box进行调整，使其符合真实图片的样子</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_correct_boxes</span>(<span class="hljs-params">box_xy, box_wh, input_shape, image_shape</span>):</span><br>    box_yx = box_xy[..., ::-<span class="hljs-number">1</span>]<br>    box_hw = box_wh[..., ::-<span class="hljs-number">1</span>]<br>    <br>    input_shape = K.cast(input_shape, K.dtype(box_yx))<br>    image_shape = K.cast(image_shape, K.dtype(box_yx))<br><br>    new_shape = K.<span class="hljs-built_in">round</span>(image_shape * K.<span class="hljs-built_in">min</span>(input_shape/image_shape))<br>    offset = (input_shape-new_shape)/<span class="hljs-number">2.</span>/input_shape<br>    scale = input_shape/new_shape<br><br>    box_yx = (box_yx - offset) * scale<br>    box_hw *= scale<br><br>    box_mins = box_yx - (box_hw / <span class="hljs-number">2.</span>)<br>    box_maxes = box_yx + (box_hw / <span class="hljs-number">2.</span>)<br>    boxes =  K.concatenate([<br>        box_mins[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_min</span><br>        box_mins[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>],  <span class="hljs-comment"># x_min</span><br>        box_maxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_max</span><br>        box_maxes[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>]  <span class="hljs-comment"># x_max</span><br>    ])<br><br>    boxes *= K.concatenate([image_shape, image_shape])<br>    <span class="hljs-keyword">return</span> boxes<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   获取每个box和它的得分</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_boxes_and_scores</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, image_shape</span>):</span><br>    <span class="hljs-comment"># 将预测值调成真实值</span><br>    <span class="hljs-comment"># box_xy对应框的中心点</span><br>    <span class="hljs-comment"># box_wh对应框的宽和高</span><br>    <span class="hljs-comment"># -1,19,19,3,2; -1,19,19,3,2; -1,19,19,3,1; -1,19,19,3,80</span><br>    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats, anchors, num_classes, input_shape)<br>    <span class="hljs-comment"># 将box_xy、和box_wh调节成y_min,y_max,xmin,xmax</span><br>    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)<br>    <span class="hljs-comment"># 获得得分和box</span><br>    boxes = K.reshape(boxes, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])<br>    box_scores = box_confidence * box_class_probs<br>    box_scores = K.reshape(box_scores, [-<span class="hljs-number">1</span>, num_classes])<br>    <span class="hljs-keyword">return</span> boxes, box_scores<br><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-comment">#   图片预测</span><br><span class="hljs-comment">#---------------------------------------------------#</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_eval</span>(<span class="hljs-params">yolo_outputs,</span></span><br><span class="hljs-function"><span class="hljs-params">              anchors,</span></span><br><span class="hljs-function"><span class="hljs-params">              num_classes,</span></span><br><span class="hljs-function"><span class="hljs-params">              image_shape,</span></span><br><span class="hljs-function"><span class="hljs-params">              max_boxes=<span class="hljs-number">20</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">              score_threshold=<span class="hljs-number">.6</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">              iou_threshold=<span class="hljs-number">.5</span></span>):</span><br>    <span class="hljs-comment"># 获得特征层的数量</span><br>    num_layers = <span class="hljs-built_in">len</span>(yolo_outputs)<br>    <span class="hljs-comment"># 特征层1对应的anchor是678</span><br>    <span class="hljs-comment"># 特征层2对应的anchor是345</span><br>    <span class="hljs-comment"># 特征层3对应的anchor是012</span><br>    anchor_mask = [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]]<br>    <br>    input_shape = K.shape(yolo_outputs[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] * <span class="hljs-number">32</span><br>    boxes = []<br>    box_scores = []<br>    <span class="hljs-comment"># 对每个特征层进行处理</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, image_shape)<br>        boxes.append(_boxes)<br>        box_scores.append(_box_scores)<br>    <span class="hljs-comment"># 将每个特征层的结果进行堆叠</span><br>    boxes = K.concatenate(boxes, axis=<span class="hljs-number">0</span>)<br>    box_scores = K.concatenate(box_scores, axis=<span class="hljs-number">0</span>)<br><br>    mask = box_scores &gt;= score_threshold<br>    max_boxes_tensor = K.constant(max_boxes, dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    boxes_ = []<br>    scores_ = []<br>    classes_ = []<br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_classes):<br>        <span class="hljs-comment"># 取出所有box_scores &gt;= score_threshold的框，和成绩</span><br>        class_boxes = tf.boolean_mask(boxes, mask[:, c])<br>        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])<br><br>        <span class="hljs-comment"># 非极大抑制，去掉box重合程度高的那一些，获取局部最大值</span><br>        nms_index = tf.image.non_max_suppression(<br>            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)<br><br>        <span class="hljs-comment"># 获取非极大抑制后的结果</span><br>        <span class="hljs-comment"># 下列三个分别是</span><br>        <span class="hljs-comment"># 框的位置，得分与种类</span><br>        class_boxes = K.gather(class_boxes, nms_index)<br>        class_box_scores = K.gather(class_box_scores, nms_index)<br>        classes = K.ones_like(class_box_scores, <span class="hljs-string">&#x27;int32&#x27;</span>) * c<br>        boxes_.append(class_boxes)<br>        scores_.append(class_box_scores)<br>        classes_.append(classes)<br>    boxes_ = K.concatenate(boxes_, axis=<span class="hljs-number">0</span>)<br>    scores_ = K.concatenate(scores_, axis=<span class="hljs-number">0</span>)<br>    classes_ = K.concatenate(classes_, axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">return</span> boxes_, scores_, classes_<br></code></pre></td></tr></table></figure><h2 id="YOLOv4新增训练技巧"><a href="#YOLOv4新增训练技巧" class="headerlink" title="YOLOv4新增训练技巧"></a>YOLOv4新增训练技巧</h2><p>为了使设计的检测器更适合于单 GPU 的训练，我们进行了如下其他设计和改进：</p><ol><li>Mosaic数据增强</li></ol><p>论文原话：Mosaic表示一种新的数据增强方法，该方法混合了4个训练图像。 因此，混合了4个不同的上下文，而CutMix仅混合了2个输入图像。 这样可以检测正常上下文之外的对象，增强模型的鲁棒性。。 此外，批量归一化从每层上的4张不同图像计算激活统计信息。 这大大减少了对大批量生产的需求。</p><p><img src="/img/5.3/26.png" alt="Mosaic represents a new method of data augmentation"></p><p>这个Mosaic数据增强就是一次读取4张不同的图片，然后进行缩放，调整大小等操作，然后拼接成一幅图。这样能一次训练四个图像，增加了图片内容的丰富性，即丰富检测物体的背景。BN（标准化）计算的时候一下子会计算四张图片的数据</p><ol><li>自对抗训练SAT</li></ol><p>SAT是一种新的数据增强技术，该技术分前后两个阶段进行：<br>第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对自身执行一种对抗性攻击，改变原始图像，从而造成图像上没有目标的假象。<br>第二阶段，训练神经网络对修改后的图像进行正常的目标检测。</p>]]></content>
    
    
    <categories>
      
      <category>Computer Version</category>
      
      <category>Paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLOv3: An Incremental Improvement</title>
    <link href="/2021/05/02/YOLOv3-An-Incremental-Improvement/"/>
    <url>/2021/05/02/YOLOv3-An-Incremental-Improvement/</url>
    
    <content type="html"><![CDATA[<p>YOLOv3模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv3.pdf" width="100%" height="550" type="application/pdf"></div><p>YOLO是一种端到端的目标检测模型。YOLO算法的基本思想是：首先通过特征提取网络提取输入特征，得到特定大小的特征图输出。输入图像分成13×13的网格单元，接着如果真实框中某个对象的中心坐标落在某个网格中，那么就由该网格来预测该对象。每个对象有固定数量的边界框，YOLO v3中有三个边界框，使用逻辑回归确定用来预测的回归框.</p><h1 id="YOLO结构"><a href="#YOLO结构" class="headerlink" title="YOLO结构"></a>YOLO结构</h1><p>Yolo v3整个结构，不包括池化层和全连接层。Yolo主干结构是Darknet-53网络，还有 Yolo预测支路采用的都是全卷积的结构。</p><p><img src="/img/5.2/1.png" alt="YOLO v3 结构图"></p><p>DBL是YOLO v3的基本组件, 主干网络中使用5个resn结构,n代表数字,表示这个给res_block里含有n个res_unit.YOLO v3在YOLO v2 的基础上开始借鉴ResNet 残差网络结构,这种结构可以让网络结构更深。</p><p>在预测支路上有张量拼接（concat）操作。其实现方法是将darknet中间层和中间层后某一层的上采样进行拼接。值得注意的是，张量拼接和Res_unit结构的add的操作是不一样的，张量拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</p><h1 id="Darknet-53-特征提取网络"><a href="#Darknet-53-特征提取网络" class="headerlink" title="Darknet-53 特征提取网络"></a>Darknet-53 特征提取网络</h1><p>Yolo v3中使用了一个53层的卷积网络，这个网络由残差单元叠加而成。Joseph Redmon的实验表明，在分类准确度上与效率的平衡上，Darknet-53模型比ResNet-101、 ResNet-152和Darknet-19表现得更好。Yolo v3并没有那么追求速度，而是在保证实时性(fps&gt;60)的基础上追求performance。</p><p>一方面，Darknet-53网络采用全卷积结构，Yolo v3前向传播过程中，张量的尺寸变换是通过改变卷积核的步长来实现的。卷积的步长为2，每次经过卷积之后，图像边长缩小一半。如图2.1中所示，Darknet-53中有5次卷积的步长为2。经过5次缩小，特征图缩小为原输入尺寸的1/32。所以网络输入图片的尺寸为32的倍数，取为416×416。Yolo v2中对于前向过程中张量尺寸变换，都是通过最大池化来进行，一共有5次。而v3是通过卷积核增大步长来进行，也是5次。</p><p>另一方面，Darknet-53网络引入了residual结构。Yolo v2中还是类似VGG那样直筒型的网络结构，层数太多训起来会有梯度问题，所以Darknet-19也就19层。得益于ResNet的residual结构，训练深层网络的难度大大减小。因此Darknet-53网络做到53层，精度提升比较明显。</p><p><img src="/img/5.2/2.png" alt="Darknet-53骨干结构"></p><p>Darknet-53网络只是特征提取层，源码中只使用了pooling层前面的卷积层来提取特征，因此multi-scale的特征融合和预测支路并没有在该网络结构中体现。</p><h1 id="边界框的预测"><a href="#边界框的预测" class="headerlink" title="边界框的预测"></a>边界框的预测</h1><p>Yolo v3关于bounding box的初始尺寸还是采用Yolo v2 中的k-means聚类的方式来做，这种先验知识对于bounding box的初始化帮助还是很大的，毕竟过多的bounding box虽然对于效果来说有保障，但是对于算法速度影响还是比较大的。</p><p>Yolo v2借鉴了faster R-CNN的RPN的anchor机制，不同的是，采用k-means聚类的方法来确定默认框的尺寸。Joseph Redmon修改了k-means算法中关于距离的定义，使用的是IOU距离。同样地，YOLO v3选择的默认框有9个。其尺寸可以通过k-means算法在数据集上聚类得到。在COCO数据集上，9个聚类是：（10×13）;（16×30）;（33×23）;（30×61）;（62×45）; （59×119）; （116×90）; （156×198）; （373×326）。默认框与不同尺寸特征图的对应关系是：13×13的特征图对应[（116×90），（156×198），（373×326）],26×26的特征图对应[（30×61），（62×45），（59×119）],52×52的特征图对应[（10×13），（16×30），（33×23）]。其原因是：特征图越大，感受野越小。对小目标越敏感，所以选用小的anchor box。特征图越小，感受野越大。对大目标越敏感，所以选用大的anchor box。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">YOLO_Kmeans</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, cluster_number, filename</span>):</span><br>        self.cluster_number = cluster_number<br>        self.filename = <span class="hljs-string">&quot;2012_train.txt&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">iou</span>(<span class="hljs-params">self, boxes, clusters</span>):</span>  <span class="hljs-comment"># 1 box -&gt; k clusters </span><br>        n = boxes.shape[<span class="hljs-number">0</span>]<br>        k = self.cluster_number<br>        box_area = boxes[:, <span class="hljs-number">0</span>] * boxes[:, <span class="hljs-number">1</span>]<br>        box_area = box_area.repeat(k)<br>        box_area = np.reshape(box_area, (n, k))<br>        cluster_area = clusters[:, <span class="hljs-number">0</span>] * clusters[:, <span class="hljs-number">1</span>]<br>        cluster_area = np.tile(cluster_area, [<span class="hljs-number">1</span>, n])<br>        cluster_area = np.reshape(cluster_area, (n, k))<br>        box_w_matrix = np.reshape(boxes[:, <span class="hljs-number">0</span>].repeat(k), (n, k))<br>        cluster_w_matrix = np.reshape(np.tile(clusters[:, <span class="hljs-number">0</span>], (<span class="hljs-number">1</span>, n)), (n, k))<br>        min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix)<br>        box_h_matrix = np.reshape(boxes[:, <span class="hljs-number">1</span>].repeat(k), (n, k))<br>        cluster_h_matrix = np.reshape(np.tile(clusters[:, <span class="hljs-number">1</span>], (<span class="hljs-number">1</span>, n)), (n, k))<br>        min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix)<br>        inter_area = np.multiply(min_w_matrix, min_h_matrix)<br><span class="hljs-comment"># 计算IOU值</span><br>        result = inter_area / (box_area + cluster_area - inter_area)<br>        <span class="hljs-keyword">return</span> result<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">avg_iou</span>(<span class="hljs-params">self, boxes, clusters</span>):</span><br>        accuracy = np.mean([np.<span class="hljs-built_in">max</span>(self.iou(boxes, clusters), axis=<span class="hljs-number">1</span>)])<br>        <span class="hljs-keyword">return</span> accuracy<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kmeans</span>(<span class="hljs-params">self, boxes, k, dist=np.median</span>):</span><br><span class="hljs-comment">#聚类问题</span><br>        box_number = boxes.shape[<span class="hljs-number">0</span>]<br>        distances = np.empty((box_number, k))<br>        last_nearest = np.zeros((box_number,))<br>        np.random.seed()<br>        clusters = boxes[np.random.choice(<br>            box_number, k, replace=<span class="hljs-literal">False</span>)]  <span class="hljs-comment"># init k clusters</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br><span class="hljs-comment">#此处没有使用欧氏距离，较大的box会比较小的box产生更多的错误。自定义的距离度量公式为：</span><br><span class="hljs-comment">#d(box,centroid)=1-IOU(box,centroid)。到聚类中心的距离越小越好，但IOU值是越大越好，所以使用 #1 - IOU，这样就保证距离越小，IOU值越大。</span><br>            distances = <span class="hljs-number">1</span> - self.iou(boxes, clusters)  <br>            current_nearest = np.argmin(distances, axis=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> (last_nearest == current_nearest).<span class="hljs-built_in">all</span>():<br>                <span class="hljs-keyword">break</span>  <span class="hljs-comment"># clusters won&#x27;t change</span><br>            <span class="hljs-keyword">for</span> cluster <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>                clusters[cluster] = dist(  <span class="hljs-comment"># update clusters</span><br>                    boxes[current_nearest == cluster], axis=<span class="hljs-number">0</span>)<br>            last_nearest = current_nearest<br>        <span class="hljs-keyword">return</span> clusters<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">result2txt</span>(<span class="hljs-params">self, data</span>):</span><br>        f = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;yolo_anchors.txt&quot;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>        row = np.shape(data)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(row):<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<br>                x_y = <span class="hljs-string">&quot;%d,%d&quot;</span> % (data[i][<span class="hljs-number">0</span>], data[i][<span class="hljs-number">1</span>])<br>            <span class="hljs-keyword">else</span>:<br>                x_y = <span class="hljs-string">&quot;, %d,%d&quot;</span> % (data[i][<span class="hljs-number">0</span>], data[i][<span class="hljs-number">1</span>])<br>            f.write(x_y)<br>        f.close()<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">txt2boxes</span>(<span class="hljs-params">self</span>):</span><br>        f = <span class="hljs-built_in">open</span>(self.filename, <span class="hljs-string">&#x27;r&#x27;</span>)<br>        dataSet = []<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>            infos = line.split(<span class="hljs-string">&quot; &quot;</span>)<br>            length = <span class="hljs-built_in">len</span>(infos)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, length):<br>                width = <span class="hljs-built_in">int</span>(infos[i].split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">2</span>]) - \<br>                    <span class="hljs-built_in">int</span>(infos[i].split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>])<br>                height = <span class="hljs-built_in">int</span>(infos[i].split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">3</span>]) - \<br>                    <span class="hljs-built_in">int</span>(infos[i].split(<span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">1</span>])<br>                dataSet.append([width, height])<br>        result = np.array(dataSet)<br>        f.close()<br>        <span class="hljs-keyword">return</span> result<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">txt2clusters</span>(<span class="hljs-params">self</span>):</span><br>        all_boxes = self.txt2boxes()<br>        result = self.kmeans(all_boxes, k=self.cluster_number)<br>        result = result[np.lexsort(result.T[<span class="hljs-number">0</span>, <span class="hljs-literal">None</span>])]<br>        self.result2txt(result)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;K anchors:\n &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(result))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: &#123;:.2f&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(<br>            self.avg_iou(all_boxes, result) * <span class="hljs-number">100</span>))<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    cluster_number = <span class="hljs-number">9</span><br>    filename = <span class="hljs-string">&quot;2012_train.txt&quot;</span><br>    kmeans = YOLO_Kmeans(cluster_number, filename)<br>    kmeans.txt2clusters()<br></code></pre></td></tr></table></figure><p>Yolo v3采用直接预测相对位置的方法。预测出b-box中心点相对于网格单元左上角的相对坐标。直接预测出<script type="math/tex">(t_x,t_y,t_w,t_h,t_0)</script>，然后通过以下坐标偏移公式计算得到b-box的位置大小和confidence。</p><script type="math/tex; mode=display">b_x=\sigma(t_x)+c_x</script><script type="math/tex; mode=display">b_y=\sigma(t_y)+c_y</script><script type="math/tex; mode=display">b_\omega=p_\omega e^{t_w}</script><script type="math/tex; mode=display">b_h=p_he^{t_h}</script><script type="math/tex; mode=display">p_r(object)\times IOU(b,ogject)=\sigma(t_0)</script><p><script type="math/tex">t_x</script>、<script type="math/tex">t_y</script>、<script type="math/tex">t_w</script>、<script type="math/tex">t_h</script>就是模型的预测输出。<script type="math/tex">c_x</script>和<script type="math/tex">c_y</script>表示网格单元的坐标，比如某层的特征图大小是13×13，那么网格单元就有13×13个，第0行第1列的网格单元的坐标<script type="math/tex">c_x</script>就是0，<script type="math/tex">c_y</script>就是1。<script type="math/tex">p_w</script>和<script type="math/tex">p_h</script>表示预测前边界框的大小。<script type="math/tex">b_x</script>、<script type="math/tex">b_y</script>、<script type="math/tex">b_w</script>和<script type="math/tex">b_h</script>就是预测得到的边界框的中心的坐标和大小。在训练这几个坐标值的时候采用了sum of squared error loss（平方和误差损失），因为这种方式的误差可以很快的计算出来。</p><p>Yolo v3使用逻辑回归预测每个边界框的分数。如果边界框与真实框的重叠度比之前的任何其他边界框都要好，则该值应该为1。如果边界框不是最好的，但确实与真实对象的重叠超过某个阈值(Yolo v3中这里设定的阈值是0.5)，那么就忽略这次预测。Yolo v3只为每个真实对象分配一个边界框，如果边界框与真实对象不吻合，则不会产生坐标或类别预测损失，只会产生物体预测损失。</p><h1 id="类别预测"><a href="#类别预测" class="headerlink" title="类别预测"></a>类别预测</h1><p>类别预测方面主要是将原来的单标签分类改进为多标签分类，因此网络结构上就将原来用于单标签多分类的softmax层换成用于多标签多分类的Logistic分类器。Yolo v2网络中的Softmax分类器，认为一个目标只属于一个类别，通过输出Score大小，使得每个框分配到Score最大的一个类别。但在一些复杂场景下，一个目标可能属于多个类（有重叠的类别标签），因此Yolo v3用多个独立的Logistic分类器替代Softmax层解决多标签分类问题，且准确率不会下降。举例说明，原来分类网络中的softmax层都是假设一张图像或一个object只属于一个类别，但是在一些复杂场景下，一个object可能属于多个类，比如你的类别中有woman和person这两个类，那么如果一张图像中有一个woman，那么你检测的结果中类别标签就要同时有woman和person两个类，这就是 多标签分类，需要用Logistic分类器来对每个类别做二分类。 Logistic分类器主要用到sigmoid函数，该函数可以将输入约束在0到1的范围内，因此当一张图像经过特征提取后的某一类输出经过sigmoid函数约束后如果大于0.5，就表示该边界框负责的目标属于该类。</p><h1 id="多尺度预测"><a href="#多尺度预测" class="headerlink" title="多尺度预测"></a>多尺度预测</h1><p>Yolo v3采用多个尺度融合的方式做预测。原来的Yolo v2有一个层叫：passthrough layer，该层作用是为了加强Yolo算法对小目标检测的精确度。这个思想在Yolo v3中得到了进一步加强，在Yolo v3中采用类似FPN(feature pyramid networks)的上采样和融合做法（最后融合了3个尺度，其他两个尺度的大小分别是26×26和52×52），在多个尺度的特征图上做检测，越精细的网格就可以检测出越精细的物体。对于小目标的检测效果提升明显。</p><p>在结构图1中可以看出，Yolo v3设定的是每个网格单元预测3个box，所以每个box需要有<script type="math/tex">(x, y, w, h, confidence)</script>五个基本参数。Yolo v3输出了3个不同尺度的特征图，如图1所示的<script type="math/tex">y_1</script>, <script type="math/tex">y_2</script>, <script type="math/tex">y_3</script>。<script type="math/tex">y_1</script>,<script type="math/tex">y_2</script>和<script type="math/tex">y_3</script>的深度都是255，边长的规律是13:26:52。</p><p>每个预测任务得到的特征大小都为<script type="math/tex">N\times N\times [3\times (4+1+80)]</script> ，N为格子大小，3为每个格子得到的边界框数量， 4是边界框坐标数量，1是目标预测值，80是类别数量。对于COCO类别而言，有80个类别的概率，所以每个box应该对每个种类都输出一个概率。所以3×(5 + 80) = 255。这个255就是这么来的。</p><p>Yolo v3用上采样的方法来实现这种多尺度的特征图。在Darknet-53得到的特征图的基础上，经过六个DBL结构和最后一层卷积层得到第一个特征图谱，在这个特征图谱上做第一次预测。Y1支路上，从后向前的倒数第3个卷积层的输出，经过一个DBL结构和一次（2,2）上采样，将上采样特征与第2个Res8结构输出的卷积特征张量连接，经过六个DBL结构和最后一层卷积层得到第二个特征图谱，在这个特征图谱上做第二次预测。Y2支路上，从后向前倒数第3个卷积层的输出，经过一个DBL结构和一次（2,2）上采样，将上采样特征与第1个Res8结构输出的卷积特征张量连接，经过六个DBL结构和最后一层卷积层得到第三个特征图谱，在这个特征图谱上做第三次预测。</p><p>就整个网络而言，Yolo v3多尺度预测输出的特征图尺寸为<script type="math/tex">y_1：(13×13)，y_2：(26×26)，y_3：(52×52)</script>。网络接收一张（416×416）的图，经过5个步长为2的卷积来进行降采样（416 / 2ˆ5 = 13，<script type="math/tex">y_1</script>输出（13×13）。从<script type="math/tex">y_1</script>的倒数第二层的卷积层上采样<script type="math/tex">(x_2，up sampling)</script>再与最后一个26×26大小的特征图张量连接，<script type="math/tex">y_2</script>输出（26×26）。从<script type="math/tex">y_2</script>的倒数第二层的卷积层上采样<script type="math/tex">(x_2，up sampling)</script>再与最后一个52×52大小的特征图张量连接，<script type="math/tex">y_3</script>输出（52×52）</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>在Yolo v1中使用了一种叫sum-square error的损失计算方法，只是简单的差方相加。我们知道，在目标检测任务里，有几个关键信息是需要确定的:(x,y),(w,h),class,confidence 。根据关键信息的特点可以分为上述四类，损失函数应该由各自特点确定。最后加到一起就可以组成最终的loss function了，也就是一个loss function搞定端到端的训练。</p><p><img src="/img/5.2/3.png" alt="Loss Function"></p><p>类别预测:判断是否有对象中心落在网格中</p><p>下面从代码中分析v3的损失函数.keras框架描述的Yolo v3 的loss function代码，在附录yolo3.model。忽略恒定系数不看，可以从代码中看出:<br>除了w, h的损失函数依然采用总方误差之外，其他部分的损失函数用的是二值交叉熵。最后加到一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,<span class="hljs-number">0</span>:<span class="hljs-number">2</span>], from_logits=<span class="hljs-literal">True</span>)<br>wh_loss = object_mask * box_loss_scale * <span class="hljs-number">0.5</span> * K.square(raw_true_wh-raw_pred[...,<span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br><span class="hljs-comment"># 置信度</span><br>confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,<span class="hljs-number">4</span>:<span class="hljs-number">5</span>], from_logits=<span class="hljs-literal">True</span>)+ (<span class="hljs-number">1</span>-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,<span class="hljs-number">4</span>:<span class="hljs-number">5</span>], from_logits=<span class="hljs-literal">True</span>) * ignore_mask<br><span class="hljs-comment"># 分类</span><br>class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,<span class="hljs-number">5</span>:], from_logits=<span class="hljs-literal">True</span>)<br>xy_loss = K.<span class="hljs-built_in">sum</span>(xy_loss) / mf<br>wh_loss = K.<span class="hljs-built_in">sum</span>(wh_loss) / mf<br>confidence_loss = K.<span class="hljs-built_in">sum</span>(confidence_loss) / mf<br>class_loss = K.<span class="hljs-built_in">sum</span>(class_loss) / mf<br>loss += xy_loss + wh_loss + confidence_loss + class_loss<br></code></pre></td></tr></table></figure><p><img src="/img/5.2/4.png" alt="实验目录结构"></p><p><img src="/img/5.2/5.png" alt="仿真过程流程"></p><h1 id="使用官方模型检测"><a href="#使用官方模型检测" class="headerlink" title="使用官方模型检测"></a>使用官方模型检测</h1><p>Yolo v3的作者训练的网络基于coco数据集。下载作者的权值文件，yolov3.weights。经convert.py转换为keras的网络结构和权值文件。执行以下命令，完成模型的转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python convert.py -w yolov3.cfg yolov3.weights model_data/yolo.h5<br></code></pre></td></tr></table></figure><p>需要注意的是 然后使用yolo_video.py检测图像或视频中的目标。yolov3.cfg是模型控制文件，yolov3.weights是模型权重文件，model_data/yolo.h5是输出的keras权重文件。<br>YOLO v3.cfg中的部分参数说明:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python">[net]<br><span class="hljs-comment"># Testing            ### 测试模式                                          </span><br>batch=<span class="hljs-number">1</span><br>subdivisions=<span class="hljs-number">1</span><br><span class="hljs-comment"># Training           ### 训练模式，每次前向的图片数目 = batch/subdivisions </span><br><span class="hljs-comment"># batch=64</span><br><span class="hljs-comment"># subdivisions=16</span><br>width=<span class="hljs-number">416</span>            <span class="hljs-comment">### 网络的输入宽、高、通道数                          </span><br>height=<span class="hljs-number">416</span><br>channels=<span class="hljs-number">3</span><br>momentum=<span class="hljs-number">0.9</span>         <span class="hljs-comment">### 动量                                              </span><br>decay=<span class="hljs-number">0.0005</span>         <span class="hljs-comment">### 权重衰减                                          </span><br>angle=<span class="hljs-number">0</span><br>saturation = <span class="hljs-number">1.5</span>     <span class="hljs-comment">### 饱和度                                            </span><br>exposure = <span class="hljs-number">1.5</span>       <span class="hljs-comment">### 曝光度                                            </span><br>hue=<span class="hljs-number">.1</span>               <span class="hljs-comment">### 色调                                              </span><br><br>learning_rate=<span class="hljs-number">0.001</span>  <span class="hljs-comment">### 学习率                                            </span><br>burn_in=<span class="hljs-number">1000</span>         <span class="hljs-comment">### 学习率控制的参数</span><br>max_batches = <span class="hljs-number">50200</span>  <span class="hljs-comment">### 迭代次数                                          </span><br>policy=steps         <span class="hljs-comment">### 学习率策略                                       </span><br>steps=<span class="hljs-number">40000</span>,<span class="hljs-number">45000</span>    <span class="hljs-comment">### 学习率变动步长                                   </span><br>scales=<span class="hljs-number">.1</span>,<span class="hljs-number">.1</span>         <span class="hljs-comment">### 学习率变动因子                                   </span><br><br>[convolutional]<br>batch_normalize=<span class="hljs-number">1</span>    <span class="hljs-comment">### BN</span><br>filters=<span class="hljs-number">32</span>           <span class="hljs-comment">### 卷积核数目</span><br>size=<span class="hljs-number">3</span>               <span class="hljs-comment">### 卷积核尺寸</span><br>stride=<span class="hljs-number">1</span>             <span class="hljs-comment">### 卷积核步长</span><br>pad=<span class="hljs-number">1</span>                <span class="hljs-comment">### pad</span><br>activation=leaky     <span class="hljs-comment">### 激活函数</span><br>……<br>[convolutional]<br>size=<span class="hljs-number">1</span><br>stride=<span class="hljs-number">1</span><br>pad=<span class="hljs-number">1</span><br>filters=<span class="hljs-number">255</span>              <span class="hljs-comment">### 3x(classes + 4coor + 1prob) = 3x(20+4+1) = 75              </span><br>activation=linear<br><br>[yolo]<br>mask = <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>            <span class="hljs-comment">### mask序号                        </span><br>anchors = <span class="hljs-number">10</span>,<span class="hljs-number">13</span>,  <span class="hljs-number">16</span>,<span class="hljs-number">30</span>,  <span class="hljs-number">33</span>,<span class="hljs-number">23</span>,  <span class="hljs-number">30</span>,<span class="hljs-number">61</span>,  <span class="hljs-number">62</span>,<span class="hljs-number">45</span>,  <span class="hljs-number">59</span>,<span class="hljs-number">119</span>,  <span class="hljs-number">116</span>,<span class="hljs-number">90</span>,  <span class="hljs-number">156</span>,<span class="hljs-number">198</span>,  <span class="hljs-number">373</span>,<span class="hljs-number">326</span>  <br>classes=<span class="hljs-number">80</span>              <span class="hljs-comment">### 类比数目                        </span><br>num=<span class="hljs-number">9</span><br>jitter=<span class="hljs-number">.3</span>               <span class="hljs-comment">### 数据扩充的抖动操作              </span><br>ignore_thresh = <span class="hljs-number">.5</span>      <span class="hljs-comment">### 文章中的阈值1                   </span><br>truth_thresh = <span class="hljs-number">1</span>        <span class="hljs-comment">### 文章中的阈值2                   </span><br>random=<span class="hljs-number">1</span>                <span class="hljs-comment">### 多尺度训练开关   </span><br></code></pre></td></tr></table></figure><p>检测使用的脚本分析放在使用自己的数据集训练并检测的部分。这里先给出使用官方权重文件的检测结果。使用python yolo_video.py –image命令，输入类别为狗，鸟，人的图片各一张，得到图片的检测结果。</p><p><img src="/img/5.2/6.png" alt="检测结果"></p><p>使用以下命令得到视频的检测结果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python yolo_video.py [video_path] [output_path (optional)]<br></code></pre></td></tr></table></figure><p><img src="/img/5.2/7.png" alt="检测结果"></p><p>自己训练数据集并检测 用于Keras-yolo训练的数据集格式为VOC格式。本文使用的数据集是由监控摄像头拍摄得到的视频随机截取得到的帧图像。训练集包含 people（人），front（车前），side（车侧身）和back（车尾）四个类别。 首先，构建图7.6所示的数据集目录结构。将数据集图片都复制到JPEGImages目录下。使用Labelimg工具，人工标注训练集，把标注工具输出的文件复制到Annotations目录下。</p><p><img src="/img/5.2/8.png" alt="VOC 数据集目录结构"></p><p>在VOC下新建名为test.py的python文件。该python文件，读入’Annotations’目录下的xml文件，将90%的图片划分为训练集，10%的图片用作训练-验证集。训练-验证集的10%用作验证集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br>trainval_percent = <span class="hljs-number">0.1</span> <span class="hljs-comment">#验证集比例</span><br>train_percent = <span class="hljs-number">0.9</span> <span class="hljs-comment">#训练集比例</span><br>xmlfilepath = <span class="hljs-string">&#x27;Annotations&#x27;</span><br>txtsavepath = <span class="hljs-string">&#x27;ImageSets\Main&#x27;</span><br>total_xml = os.listdir(xmlfilepath)<br>num = <span class="hljs-built_in">len</span>(total_xml)<br><span class="hljs-built_in">list</span> = <span class="hljs-built_in">range</span>(num)<br>tv = <span class="hljs-built_in">int</span>(num * trainval_percent)<br>tr = <span class="hljs-built_in">int</span>(tv * train_percent)<br>trainval = random.sample(<span class="hljs-built_in">list</span>, tv)<br>train = random.sample(trainval, tr)<br>ftrainval = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ImageSets/Main/trainval.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>ftest = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ImageSets/Main/test.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>ftrain = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ImageSets/Main/train.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>fval = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;ImageSets/Main/val.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>:<br>    name = total_xml[i][:-<span class="hljs-number">4</span>] + <span class="hljs-string">&#x27;\n&#x27;</span><br>    <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> trainval:<br>        ftrainval.write(name)  <span class="hljs-comment">#10%的图片用作训练-验证集</span><br>        <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> train:<br>            ftest.write(name)  <span class="hljs-comment">#训练-验证集的90%用作测试集</span><br>        <span class="hljs-keyword">else</span>:<br>            fval.write(name)  <span class="hljs-comment">#训练-验证集的10%用作验证集</span><br>    <span class="hljs-keyword">else</span>:<br>        ftrain.write(name)   <span class="hljs-comment">#90%的图片作为训练集</span><br>ftrainval.close()<br>ftrain.close()<br>fval.close()<br>ftest.close()<br></code></pre></td></tr></table></figure><p>最后将划分好的图片名称分别保存到’ImageSets\Main’目录下，trainval.txt，test.txt,train.txt和val.txt四个文件中。<br>该工程中使用的数据格式是: image_file_path box1 box2 … boxN; 边界框格式是: x_min,y_min,x_max,y_max,class_id (no space)。对于VOC数据集，需要使用voc_annotation.py脚本进行转换。在主目录下生成test.txt,train.txt和val.txt，包含上一步生成的训练集、验证集和测试集的图片的路径和（x，y，w，h，class）真实值信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xml.etree.ElementTree <span class="hljs-keyword">as</span> ET<br><span class="hljs-keyword">from</span> os <span class="hljs-keyword">import</span> getcwd<br>sets=[(<span class="hljs-string">&#x27;2007&#x27;</span>, <span class="hljs-string">&#x27;train&#x27;</span>), (<span class="hljs-string">&#x27;2007&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>), (<span class="hljs-string">&#x27;2007&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>)]<br>classes = [<span class="hljs-string">&quot;people&quot;</span>,<span class="hljs-string">&quot;front&quot;</span>,<span class="hljs-string">&quot;side&quot;</span>,<span class="hljs-string">&quot;back&quot;</span>] <span class="hljs-comment">#数据集中所标记的四个类别</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convert_annotation</span>(<span class="hljs-params">year, image_id, list_file</span>):</span><br>    in_file =<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/home/fengzicai/Documents/keras-yolo3/VOC%s/Annotations/%s.xml&#x27;</span>%(year, image_id))<br>    tree=ET.parse(in_file)<br>    root = tree.getroot()<br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> root.<span class="hljs-built_in">iter</span>(<span class="hljs-string">&#x27;object&#x27;</span>):<br>        difficult = obj.find(<span class="hljs-string">&#x27;difficult&#x27;</span>).text<br>        cls = obj.find(<span class="hljs-string">&#x27;name&#x27;</span>).text<br>        <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> classes <span class="hljs-keyword">or</span> <span class="hljs-built_in">int</span>(difficult)==<span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">continue</span><br>        cls_id = classes.index(cls)<br>        xmlbox = obj.find(<span class="hljs-string">&#x27;bndbox&#x27;</span>)<br>        b = (<span class="hljs-built_in">int</span>(xmlbox.find(<span class="hljs-string">&#x27;xmin&#x27;</span>).text), <span class="hljs-built_in">int</span>(xmlbox.find(<span class="hljs-string">&#x27;ymin&#x27;</span>).text), <span class="hljs-built_in">int</span>(xmlbox.find(<span class="hljs-string">&#x27;xmax&#x27;</span>).text), <span class="hljs-built_in">int</span>(xmlbox.find(<span class="hljs-string">&#x27;ymax&#x27;</span>).text))<br>        list_file.write(<span class="hljs-string">&quot; &quot;</span> + <span class="hljs-string">&quot;,&quot;</span>.join([<span class="hljs-built_in">str</span>(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> b]) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(cls_id))<br>wd = getcwd()<br><span class="hljs-keyword">for</span> year, image_set <span class="hljs-keyword">in</span> sets:<br>    image_ids = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;/home/fengzicai/Documents/keras-yolo3/VOC%s/ImageSets/Main/%s.txt&#x27;</span>%(year, image_set)).read().strip().split()<br>    list_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;%s_%s.txt&#x27;</span>%(year, image_set), <span class="hljs-string">&#x27;w&#x27;</span>)<br>    <span class="hljs-keyword">for</span> image_id <span class="hljs-keyword">in</span> image_ids:<br>        list_file.write(<span class="hljs-string">&#x27;%s/VOC%s/JPEGImages/%s.jpg&#x27;</span>%(wd, year, image_id))<br>        convert_annotation(year, image_id, list_file)<br>        list_file.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    list_file.close()<br><span class="hljs-comment">#至此，VOC格式的数据集就准备好了。然后将四类标签名写入model_data/coco_classes.txt和model/voc_classes.txt中。model_data/ yolo_anchors.txt填写通过K聚类得到的9个anchor。</span><br></code></pre></td></tr></table></figure><p>下一步开始准备训练。训练过程函数调用关系如图:</p><p><img src="/img/5.2/9.png" alt="训练过程函数调用关系"></p><p>训练脚本train.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Retrain the YOLO model for your own dataset.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> keras.backend <span class="hljs-keyword">as</span> K<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, Lambda<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> TensorBoard, ModelCheckpoint, EarlyStopping<br><br><span class="hljs-keyword">from</span> yolo3.model <span class="hljs-keyword">import</span> preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss<br><span class="hljs-keyword">from</span> yolo3.utils <span class="hljs-keyword">import</span> get_random_data<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_main</span>():</span><br>    annotation_path = <span class="hljs-string">&#x27;train.txt&#x27;</span><br>    log_dir = <span class="hljs-string">&#x27;logs/000/&#x27;</span>  <span class="hljs-comment">#保存权重文件的路径</span><br>    classes_path = <span class="hljs-string">&#x27;model_data/voc_classes.txt&#x27;</span>  <span class="hljs-comment">#保存分类信息文件的路径</span><br>    anchors_path = <span class="hljs-string">&#x27;model_data/yolo_anchors.txt&#x27;</span>  <span class="hljs-comment">#保存默认框信息的路径</span><br>    class_names = get_classes(classes_path)<br>    anchors = get_anchors(anchors_path)<br>    input_shape = (<span class="hljs-number">416</span>,<span class="hljs-number">416</span>) <span class="hljs-comment"># multiple of 32, hw</span><br>    model = create_model(input_shape, anchors, <span class="hljs-built_in">len</span>(class_names) )<br>    train(model, annotation_path, input_shape, anchors, <span class="hljs-built_in">len</span>(class_names), log_dir=log_dir)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">model, annotation_path, input_shape, anchors, num_classes, log_dir=<span class="hljs-string">&#x27;logs/&#x27;</span></span>):</span><br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=&#123;<br>        <span class="hljs-string">&#x27;yolo_loss&#x27;</span>: <span class="hljs-keyword">lambda</span> y_true, y_pred: y_pred&#125;)<br>    logging = TensorBoard(log_dir=log_dir)<br>    checkpoint = ModelCheckpoint(log_dir + <span class="hljs-string">&quot;ep&#123;epoch:03d&#125;-loss&#123;loss:.3f&#125;-val_loss&#123;val_loss:.3f&#125;.h5&quot;</span>,<br>        monitor=<span class="hljs-string">&#x27;val_loss&#x27;</span>, save_weights_only=<span class="hljs-literal">True</span>, save_best_only=<span class="hljs-literal">True</span>, period=<span class="hljs-number">1</span>)<br>    batch_size = <span class="hljs-number">10</span><br>    val_split = <span class="hljs-number">0.1</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(annotation_path) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    np.random.shuffle(lines)<br>    num_val = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(lines)*val_split)<br>    num_train = <span class="hljs-built_in">len</span>(lines) - num_val<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train on &#123;&#125; samples, val on &#123;&#125; samples, with batch size &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(num_train, num_val, batch_size))<br><br>    model.fit_generator(data_generator_wrap(lines[:num_train], batch_size, input_shape, anchors, num_classes),<br>            steps_per_epoch=<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, num_train//batch_size),<br>            validation_data=data_generator_wrap(lines[num_train:], batch_size, input_shape, anchors, num_classes),<br>            validation_steps=<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, num_val//batch_size),<br>            epochs=<span class="hljs-number">500</span>,<br>            initial_epoch=<span class="hljs-number">0</span>)<br>    model.save_weights(log_dir + <span class="hljs-string">&#x27;trained_weights.h5&#x27;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_classes</span>(<span class="hljs-params">classes_path</span>):</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(classes_path) <span class="hljs-keyword">as</span> f:<br>        class_names = f.readlines()<br>    class_names = [c.strip() <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> class_names]<br>    <span class="hljs-keyword">return</span> class_names<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_anchors</span>(<span class="hljs-params">anchors_path</span>):</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(anchors_path) <span class="hljs-keyword">as</span> f:<br>        anchors = f.readline()<br>    anchors = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> anchors.split(<span class="hljs-string">&#x27;,&#x27;</span>)]<br><span class="hljs-keyword">return</span> np.array(anchors).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-comment">#该函数用于创建模型</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_model</span>(<span class="hljs-params">input_shape, anchors, num_classes, load_pretrained=<span class="hljs-literal">False</span>, freeze_body=<span class="hljs-literal">False</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">            weights_path=<span class="hljs-string">&#x27;model_data/yolo_weights.h5&#x27;</span></span>):</span><br>    K.clear_session() <span class="hljs-comment"># get a new session</span><br>    image_input = Input(shape=(<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-number">3</span>))<br>    h, w = input_shape<br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    y_true = [Input(shape=(h//&#123;<span class="hljs-number">0</span>:<span class="hljs-number">32</span>, <span class="hljs-number">1</span>:<span class="hljs-number">16</span>, <span class="hljs-number">2</span>:<span class="hljs-number">8</span>&#125;[l], w//&#123;<span class="hljs-number">0</span>:<span class="hljs-number">32</span>, <span class="hljs-number">1</span>:<span class="hljs-number">16</span>, <span class="hljs-number">2</span>:<span class="hljs-number">8</span>&#125;[l], \<br>        num_anchors//<span class="hljs-number">3</span>, num_classes+<span class="hljs-number">5</span>)) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]<br><span class="hljs-comment">#预测每个尺度的3个框，所以对于4个边界框偏移量，1个目标性预测和4个类别预测，张量为#N×N×[3 *（4 + 1 + 4）]，默认参数下：y_true[l]的shape为（batch,H,W,3,num_classes+5)</span><br>model_body = yolo_body(image_input, num_anchors//<span class="hljs-number">3</span>, num_classes)<br><span class="hljs-comment"># yolo_body()函数从yolo3.model中引入</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Create YOLOv3 model with &#123;&#125; anchors and &#123;&#125; classes.&#x27;</span>.<span class="hljs-built_in">format</span>(num_anchors, num_classes))<br><br>    <span class="hljs-keyword">if</span> load_pretrained:<br>        model_body.load_weights(weights_path, by_name=<span class="hljs-literal">True</span>, skip_mismatch=<span class="hljs-literal">True</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Load weights &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(weights_path))<br>        <span class="hljs-keyword">if</span> freeze_body:<br>            <span class="hljs-comment"># Do not freeze 3 output layers.</span><br>            num = <span class="hljs-built_in">len</span>(model_body.layers)-<span class="hljs-number">7</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num): model_body.layers[i].trainable = <span class="hljs-literal">False</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Freeze the first &#123;&#125; layers of total &#123;&#125; layers.&#x27;</span>.<span class="hljs-built_in">format</span>(num, <span class="hljs-built_in">len</span>(model_body.layers)))<br><span class="hljs-comment">#生成模型损失</span><br>    model_loss = Lambda(yolo_loss, output_shape=(<span class="hljs-number">1</span>,), name=<span class="hljs-string">&#x27;yolo_loss&#x27;</span>,<br>        arguments=&#123;<span class="hljs-string">&#x27;anchors&#x27;</span>: anchors, <span class="hljs-string">&#x27;num_classes&#x27;</span>: num_classes, <span class="hljs-string">&#x27;ignore_thresh&#x27;</span>: <span class="hljs-number">0.5</span>&#125;)(<br>        [*model_body.output, *y_true])<br>    model = Model([model_body.<span class="hljs-built_in">input</span>, *y_true], model_loss)<br><span class="hljs-keyword">return</span> model<br><span class="hljs-comment">#通过train.py(data_generator)生成数据</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_generator</span>(<span class="hljs-params">annotation_lines, batch_size, input_shape, anchors, num_classes</span>):</span><br>    n = <span class="hljs-built_in">len</span>(annotation_lines)<br>    np.random.shuffle(annotation_lines)<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        image_data = []<br>        box_data = []<br>        <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>            i %= n<br>            image, box = get_random_data(annotation_lines[i], input_shape, random=<span class="hljs-literal">True</span>)<br>            image_data.append(image)<br>            box_data.append(box)<br>            i += <span class="hljs-number">1</span><br>        image_data = np.array(image_data)<br>        box_data = np.array(box_data)<br>        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)<br>        <span class="hljs-keyword">yield</span> [image_data, *y_true], np.zeros(batch_size)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_generator_wrap</span>(<span class="hljs-params">annotation_lines, batch_size, input_shape, anchors, num_classes</span>):</span><br>    n = <span class="hljs-built_in">len</span>(annotation_lines)<br>    <span class="hljs-keyword">if</span> n==<span class="hljs-number">0</span> <span class="hljs-keyword">or</span> batch_size&lt;=<span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">return</span> data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>_main()<br></code></pre></td></tr></table></figure><p>使用自己的数据集进行检测过程与使用官方权重文件进行检测过程相同。使用python yolo_img.py –image执行检测脚本。检测脚本yolo_img.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">from</span> yolo <span class="hljs-keyword">import</span> YOLO, detect_video<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> glob<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">detect_img</span>(<span class="hljs-params">yolo</span>):</span><br>    path = <span class="hljs-string">&quot;/home/fengzicai/Documents/keras-yolo3/VOC2007/JPEGImages/*.jpg&quot;</span> <span class="hljs-comment">#要读入的图片路径</span><br>    outdir = <span class="hljs-string">&quot;/home/fengzicai/Documents/keras-yolo3/VOC2007/SegmentationClass&quot;</span> <span class="hljs-comment">#将检测的结果全保#存到outdir路径</span><br>    <span class="hljs-keyword">for</span> jpgfile <span class="hljs-keyword">in</span> glob.glob(path):<br>        img = Image.<span class="hljs-built_in">open</span>(jpgfile)<br>        img = yolo.detect_image(img)  <span class="hljs-comment">#调用yolo类中的detect_image函数，对图片进行检测，见#yolo脚本</span><br>        img.save(os.path.join(outdir, os.path.basename(jpgfile)))<br>    yolo.close_session()<br>FLAGS = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># class YOLO defines the default value, so suppress any default here</span><br>    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Command line options</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    parser.add_argument(<br>        <span class="hljs-string">&#x27;--model&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;path to model weight file, default &#x27;</span> + YOLO.get_defaults(<span class="hljs-string">&quot;model_path&quot;</span>)<br>    )<br>    parser.add_argument(<br>        <span class="hljs-string">&#x27;--anchors&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;path to anchor definitions, default &#x27;</span> + YOLO.get_defaults(<span class="hljs-string">&quot;anchors_path&quot;</span>)<br>    )<br><br>    parser.add_argument(<br>        <span class="hljs-string">&#x27;--classes&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;path to class definitions, default &#x27;</span> + YOLO.get_defaults(<span class="hljs-string">&quot;classes_path&quot;</span>)<br>    )<br>    parser.add_argument(<br>        <span class="hljs-string">&#x27;--gpu_num&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Number of GPU to use, default &#x27;</span> + <span class="hljs-built_in">str</span>(YOLO.get_defaults(<span class="hljs-string">&quot;gpu_num&quot;</span>))<br>    )<br>    parser.add_argument(<br>        <span class="hljs-string">&#x27;--image&#x27;</span>, default=<span class="hljs-literal">False</span>, action=<span class="hljs-string">&quot;store_true&quot;</span>,<br>        <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Image detection mode, will ignore all positional arguments&#x27;</span><br>    )<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Command line positional arguments -- for video detection mode</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    parser.add_argument(<br>        <span class="hljs-string">&quot;--input&quot;</span>, nargs=<span class="hljs-string">&#x27;?&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,required=<span class="hljs-literal">False</span>,default=<span class="hljs-string">&#x27;./path2your_video&#x27;</span>,<br>        <span class="hljs-built_in">help</span> = <span class="hljs-string">&quot;Video input path&quot;</span><br>    )<br>    parser.add_argument(<br>        <span class="hljs-string">&quot;--output&quot;</span>, nargs=<span class="hljs-string">&#x27;?&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;&quot;</span>,<br>        <span class="hljs-built_in">help</span> = <span class="hljs-string">&quot;[Optional] Video output path&quot;</span><br>    )<br>    FLAGS = parser.parse_args()<br>    <span class="hljs-keyword">if</span> FLAGS.image:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Image detection mode, disregard any remaining command line arguments</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Image detection mode&quot;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;input&quot;</span> <span class="hljs-keyword">in</span> FLAGS:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot; Ignoring remaining command line arguments: &quot;</span> + FLAGS.<span class="hljs-built_in">input</span> + <span class="hljs-string">&quot;,&quot;</span> + FLAGS.output)<br>        detect_img(YOLO(**<span class="hljs-built_in">vars</span>(FLAGS)))<br>    <span class="hljs-keyword">elif</span> <span class="hljs-string">&quot;input&quot;</span> <span class="hljs-keyword">in</span> FLAGS:<br>        detect_video(YOLO(**<span class="hljs-built_in">vars</span>(FLAGS)), FLAGS.<span class="hljs-built_in">input</span>, FLAGS.output)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Must specify at least video_input_path.  See usage with --help.&quot;</span>)<br></code></pre></td></tr></table></figure><p>Yolo_img.py在执行时，导入了yolo.py脚本，包含图像和视频中YOLO v3模型检测的类定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Class definition of YOLO_v3 style detection model on image and video</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> colorsys<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> timeit <span class="hljs-keyword">import</span> default_timer <span class="hljs-keyword">as</span> timer<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageFont, ImageDraw<br><br><span class="hljs-keyword">from</span> yolo3.model <span class="hljs-keyword">import</span> yolo_eval, yolo_body, tiny_yolo_body<br><span class="hljs-keyword">from</span> yolo3.utils <span class="hljs-keyword">import</span> letterbox_image<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> multi_gpu_model<br><span class="hljs-comment">#YOLO类的初始化参数</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">YOLO</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    _defaults = &#123;<br>        <span class="hljs-comment">#&quot;model_path&quot;: &#x27;model_data/yolo.h5&#x27;,</span><br>        <span class="hljs-string">&quot;model_path&quot;</span>: <span class="hljs-string">&#x27;logs/001/trained_weights.h5&#x27;</span>, <span class="hljs-comment">#训练好的模型</span><br>        <span class="hljs-string">&quot;anchors_path&quot;</span>: <span class="hljs-string">&#x27;model_data/yolo_anchors.txt&#x27;</span>, <span class="hljs-comment">#有9个anchor box，从小到大排列</span><br>        <span class="hljs-string">&quot;classes_path&quot;</span>: <span class="hljs-string">&#x27;model_data/coco_classes.txt&#x27;</span>, <span class="hljs-comment">#类别数目</span><br>        <span class="hljs-string">&quot;score&quot;</span> : <span class="hljs-number">0.3</span>, <span class="hljs-comment">#score阈值</span><br>        <span class="hljs-string">&quot;iou&quot;</span> : <span class="hljs-number">0.45</span>, <span class="hljs-comment">#iou 阈值</span><br>        <span class="hljs-string">&quot;model_image_size&quot;</span> : (<span class="hljs-number">416</span>, <span class="hljs-number">416</span>), <span class="hljs-comment">#输入图像尺寸</span><br>        <span class="hljs-string">&quot;gpu_num&quot;</span> : <span class="hljs-number">1</span>, <span class="hljs-comment">#gpu数量</span><br>    &#125;<br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_defaults</span>(<span class="hljs-params">cls, n</span>):</span><br>        <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> cls._defaults:<br>            <span class="hljs-keyword">return</span> cls._defaults[n]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Unrecognized attribute name &#x27;&quot;</span> + n + <span class="hljs-string">&quot;&#x27;&quot;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, **kwargs</span>):</span><br>        self.__dict__.update(self._defaults) <span class="hljs-comment"># set up default values</span><br>        self.__dict__.update(kwargs) <span class="hljs-comment"># and update with user overrides</span><br>        self.class_names = self._get_class()<br>        self.anchors = self._get_anchors()<br>        self.sess = K.get_session()<br>        self.boxes, self.scores, self.classes = self.generate()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_class</span>(<span class="hljs-params">self</span>):</span><br>        classes_path = os.path.expanduser(self.classes_path)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(classes_path) <span class="hljs-keyword">as</span> f:<br>            class_names = f.readlines()<br>        class_names = [c.strip() <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> class_names]<br>        <span class="hljs-keyword">return</span> class_names<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_anchors</span>(<span class="hljs-params">self</span>):</span><br>        anchors_path = os.path.expanduser(self.anchors_path)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(anchors_path) <span class="hljs-keyword">as</span> f:<br>            anchors = f.readline()<br>        anchors = [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> anchors.split(<span class="hljs-string">&#x27;,&#x27;</span>)]<br>        <span class="hljs-keyword">return</span> np.array(anchors).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate</span>(<span class="hljs-params">self</span>):</span> <span class="hljs-comment">#yolo_img.py中调用了该函数</span><br>        model_path = os.path.expanduser(self.model_path)  <span class="hljs-comment">#获取model路径</span><br>        <span class="hljs-keyword">assert</span> model_path.endswith(<span class="hljs-string">&#x27;.h5&#x27;</span>), <span class="hljs-string">&#x27;Keras model or weights must be a .h5 file.&#x27;</span> <br><span class="hljs-comment">#判断model是否以h5结尾</span><br>        <span class="hljs-comment"># Load model, or construct model and load weights.</span><br>        num_anchors = <span class="hljs-built_in">len</span>(self.anchors) <span class="hljs-comment">#num_anchors = 9。yolov3有9个先验框</span><br>        num_classes = <span class="hljs-built_in">len</span>(self.class_names) <span class="hljs-comment">#num_cliasses = 4。一共有四个类别</span><br>        is_tiny_version = num_anchors==<span class="hljs-number">6</span> <span class="hljs-comment"># default setting</span><br>        <span class="hljs-keyword">try</span>:<br>            self.yolo_model = load_model(model_path, <span class="hljs-built_in">compile</span>=<span class="hljs-literal">False</span>) <span class="hljs-comment">#下载model</span><br>        <span class="hljs-keyword">except</span>:<br>            self.yolo_model=tiny_yolo_body(Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>)), num_anchors//<span class="hljs-number">2</span>, <br>num_classes) \<br>            <span class="hljs-keyword">if</span> is_tiny_version <span class="hljs-keyword">else</span> yolo_body(Input(shape=(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-number">3</span>)), num_anchors//<span class="hljs-number">3</span>, num_classes)<br>            self.yolo_model.load_weights(self.model_path) <span class="hljs-comment"># 确保model和anchor classes 对应</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> self.yolo_model.layers[-<span class="hljs-number">1</span>].output_shape[-<span class="hljs-number">1</span>] == \<br><span class="hljs-comment"># model.layer[-1]:网络最后一层输出。 output_shape[-1]:输出维度的最后一维。 -&gt; (?,13,13,27)</span><br>                num_anchors/<span class="hljs-built_in">len</span>(self.yolo_model.output) * (num_classes + <span class="hljs-number">5</span>), \<br><span class="hljs-comment">#27 = 9/3*(4+5). 9/3:每层网格对应3个anchor box 4：4个类别 5:4+1,框的4个值+1个置信度</span><br>                <span class="hljs-string">&#x27;Mismatch between model and given anchor and class sizes&#x27;</span><br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125; model, anchors, and classes loaded.&#x27;</span>.<span class="hljs-built_in">format</span>(model_path))<br><br>        <span class="hljs-comment"># 生成绘制边框的颜色</span><br>        hsv_tuples = [(x / <span class="hljs-built_in">len</span>(self.class_names), <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>)<br><span class="hljs-comment">#h(色调）：x/len(self.class_names)  s(饱和度）：1.0  v(明亮）：1.0</span><br>                      <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.class_names))]<br>        self.colors = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: colorsys.hsv_to_rgb(*x), hsv_tuples))  <span class="hljs-comment">#hsv转换为rgb</span><br>        self.colors = <span class="hljs-built_in">list</span>(<br>            <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: (<span class="hljs-built_in">int</span>(x[<span class="hljs-number">0</span>] * <span class="hljs-number">255</span>), <span class="hljs-built_in">int</span>(x[<span class="hljs-number">1</span>] * <span class="hljs-number">255</span>), <span class="hljs-built_in">int</span>(x[<span class="hljs-number">2</span>] * <span class="hljs-number">255</span>)),<br>                self.colors))<br><span class="hljs-comment">#hsv取值范围在[0,1]，而RBG取值范围在[0,255]，所以乘上255</span><br>        np.random.seed(<span class="hljs-number">10101</span>)  <span class="hljs-comment"># np.random.seed():产生随机种子。固定种子为一致的颜色</span><br>        np.random.shuffle(self.colors)  <span class="hljs-comment"># 调整颜色来装饰相邻的类。</span><br>        np.random.seed(<span class="hljs-literal">None</span>)  <span class="hljs-comment"># 重置种子为默认</span><br><br>        <span class="hljs-comment">#为过滤的边界框生成输出张量目标。</span><br>        self.input_image_shape = K.placeholder(shape=(<span class="hljs-number">2</span>, )) <span class="hljs-comment">#K.placeholder:keras中的占位符</span><br>        <span class="hljs-keyword">if</span> self.gpu_num&gt;=<span class="hljs-number">2</span>:<br>            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)<br>        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,<br>                <span class="hljs-built_in">len</span>(self.class_names), self.input_image_shape,<br>                score_threshold=self.score, iou_threshold=self.iou) <span class="hljs-comment">#yolo_eval():yolo评估函数</span><br>        <span class="hljs-keyword">return</span> boxes, scores, classes<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">detect_image</span>(<span class="hljs-params">self, image</span>):</span> <span class="hljs-comment"># yolo_img.py中调用了该函数</span><br>        start = timer()<br>        <span class="hljs-keyword">if</span> self.model_image_size != (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>): <span class="hljs-comment">#判断图片是否存在</span><br>            <span class="hljs-keyword">assert</span> self.model_image_size[<span class="hljs-number">0</span>]%<span class="hljs-number">32</span> == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Multiples of 32 required&#x27;</span><br>            <span class="hljs-keyword">assert</span> self.model_image_size[<span class="hljs-number">1</span>]%<span class="hljs-number">32</span> == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Multiples of 32 required&#x27;</span><br><span class="hljs-comment">#assert断言语句的语法格式 model_image_size[0][1]指图像的w和h，且必须是32的整数倍</span><br>            boxed_image = letterbox_image(image, <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">reversed</span>(self.model_image_size)))<br><span class="hljs-comment"># letterbox_image()定义见附录中的yolo3.utils。输入参数（图像 ,(w=416,h=416)),</span><br><span class="hljs-comment">#输出一张使用填充来调整图像的纵横比不变的新图。</span><br>        <span class="hljs-keyword">else</span>:<br>            new_image_size = (image.width - (image.width % <span class="hljs-number">32</span>),<br>                              image.height - (image.height % <span class="hljs-number">32</span>))<br>            boxed_image = letterbox_image(image, new_image_size)<br>        image_data = np.array(boxed_image, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br>        <span class="hljs-built_in">print</span>(image_data.shape) <span class="hljs-comment">#(416,416,3)</span><br>        image_data /= <span class="hljs-number">255.</span> <span class="hljs-comment">#归一化</span><br>        image_data = np.expand_dims(image_data, <span class="hljs-number">0</span>)  <span class="hljs-comment"># Add batch dimension.</span><br><span class="hljs-comment">#添加批量维度为 (1,416,416,3)，使输入网络的张量满足(bitch, w, h, c)的格式</span><br>        out_boxes, out_scores, out_classes = self.sess.run(<br>            [self.boxes, self.scores, self.classes],<br><span class="hljs-comment">#目的为了求boxes,scores,classes，具体计算方式定义在generate（）函数内。在yolo.py中</span><br>            feed_dict=&#123; <br>                self.yolo_model.<span class="hljs-built_in">input</span>: image_data,  <span class="hljs-comment">#图像数据</span><br>                self.input_image_shape: [image.size[<span class="hljs-number">1</span>], image.size[<span class="hljs-number">0</span>]], <span class="hljs-comment">#图像尺寸</span><br>                K.learning_phase(): <span class="hljs-number">0</span> <span class="hljs-comment">#学习模式 0：测试模型。1：训练模式</span><br>            &#125;)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Found &#123;&#125; boxes for &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(out_boxes), <span class="hljs-string">&#x27;img&#x27;</span>))<br><span class="hljs-comment">#绘制边框，自动设置边框宽度，绘制边框和类别文字，使用pillow绘图库。</span><br>        font = ImageFont.truetype(font=<span class="hljs-string">&#x27;font/FiraMono-Medium.otf&#x27;</span>,<br>                    size=np.floor(<span class="hljs-number">3e-2</span> * image.size[<span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)) <span class="hljs-comment">#设置字体</span><br>        thickness = (image.size[<span class="hljs-number">0</span>] + image.size[<span class="hljs-number">1</span>]) // <span class="hljs-number">300</span> <span class="hljs-comment">#设置厚度</span><br><br>        <span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">enumerate</span>(out_classes))):<br>            predicted_class = self.class_names[c] <span class="hljs-comment">#类别</span><br>            box = out_boxes[i] <span class="hljs-comment">#框</span><br>            score = out_scores[i] <span class="hljs-comment">#置信度</span><br><br>            label = <span class="hljs-string">&#x27;&#123;&#125; &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(predicted_class, score) <span class="hljs-comment">#标签</span><br>            draw = ImageDraw.Draw(image) <span class="hljs-comment">#画图</span><br>            label_size = draw.textsize(label, font) <span class="hljs-comment">#标签文字</span><br><br>            top, left, bottom, right = box<br>            top = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, np.floor(top + <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&#x27;int32&#x27;</span>))<br>            left = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, np.floor(left + <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&#x27;int32&#x27;</span>))<br>            bottom = <span class="hljs-built_in">min</span>(image.size[<span class="hljs-number">1</span>], np.floor(bottom + <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&#x27;int32&#x27;</span>))<br>            right = <span class="hljs-built_in">min</span>(image.size[<span class="hljs-number">0</span>], np.floor(right + <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&#x27;int32&#x27;</span>))<br>            <span class="hljs-built_in">print</span>(label, (left, top), (right, bottom)) <span class="hljs-comment">#边框</span><br><br>            <span class="hljs-keyword">if</span> top - label_size[<span class="hljs-number">1</span>] &gt;= <span class="hljs-number">0</span>: <span class="hljs-comment">#标签文字</span><br>                text_origin = np.array([left, top - label_size[<span class="hljs-number">1</span>]])<br>            <span class="hljs-keyword">else</span>:<br>                text_origin = np.array([left, top + <span class="hljs-number">1</span>])<br><br>            <span class="hljs-comment"># My kingdom for a good redistributable image drawing library.</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(thickness): <span class="hljs-comment">#画边框</span><br>                draw.rectangle(<br>                    [left + i, top + i, right - i, bottom - i],<br>                    outline=self.colors[c])<br>            draw.rectangle( <span class="hljs-comment">#文字背景</span><br>                [<span class="hljs-built_in">tuple</span>(text_origin), <span class="hljs-built_in">tuple</span>(text_origin + label_size)],<br>                fill=self.colors[c])<br>            draw.text(text_origin, label, fill=(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), font=font)<br>            <span class="hljs-keyword">del</span> draw<br><br>        end = timer()<br>        <span class="hljs-built_in">print</span>(end - start)<br>        <span class="hljs-keyword">return</span> image<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">close_session</span>(<span class="hljs-params">self</span>):</span><br>        self.sess.close()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">detect_video</span>(<span class="hljs-params">yolo, video_path, output_path=<span class="hljs-string">&quot;&quot;</span></span>):</span><br>    <span class="hljs-keyword">import</span> cv2<br>    vid = cv2.VideoCapture(video_path)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> vid.isOpened():<br>        <span class="hljs-keyword">raise</span> IOError(<span class="hljs-string">&quot;Couldn&#x27;t open webcam or video&quot;</span>)<br>    video_FourCC    = <span class="hljs-built_in">int</span>(vid.get(cv2.CAP_PROP_FOURCC))<br>    video_fps       = vid.get(cv2.CAP_PROP_FPS)<br>    video_size      = (<span class="hljs-built_in">int</span>(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),<br>                        <span class="hljs-built_in">int</span>(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))<br>    isOutput = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> output_path != <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> isOutput:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;!!! TYPE:&quot;</span>, <span class="hljs-built_in">type</span>(output_path), <span class="hljs-built_in">type</span>(video_FourCC), <span class="hljs-built_in">type</span>(video_fps), <span class="hljs-built_in">type</span>(video_size))<br>        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)<br>    accum_time = <span class="hljs-number">0</span><br>    curr_fps = <span class="hljs-number">0</span><br>    fps = <span class="hljs-string">&quot;FPS: ??&quot;</span><br>    prev_time = timer()<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        return_value, frame = vid.read()<br>        <span class="hljs-comment">#frame_array = np.asarray(frame)</span><br>        image = Image.fromarray(frame)<br>        image = yolo.detect_image(image)<br>        result = np.asarray(image)<br>        curr_time = timer()<br>        exec_time = curr_time - prev_time<br>        prev_time = curr_time<br>        accum_time = accum_time + exec_time<br>        curr_fps = curr_fps + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> accum_time &gt; <span class="hljs-number">1</span>:<br>            accum_time = accum_time - <span class="hljs-number">1</span><br>            fps = <span class="hljs-string">&quot;FPS: &quot;</span> + <span class="hljs-built_in">str</span>(curr_fps)<br>            curr_fps = <span class="hljs-number">0</span><br>        cv2.putText(result, text=fps, org=(<span class="hljs-number">3</span>, <span class="hljs-number">15</span>), fontFace=cv2.FONT_HERSHEY_SIMPLEX,<br>                    fontScale=<span class="hljs-number">0.50</span>, color=(<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), thickness=<span class="hljs-number">2</span>)<br>        cv2.namedWindow(<span class="hljs-string">&quot;result&quot;</span>, cv2.WINDOW_NORMAL)<br>        cv2.imshow(<span class="hljs-string">&quot;result&quot;</span>, result)<br>        <span class="hljs-keyword">if</span> isOutput:<br>            out.write(result)<br>        <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xFF</span> == <span class="hljs-built_in">ord</span>(<span class="hljs-string">&#x27;q&#x27;</span>):<br>            <span class="hljs-keyword">break</span><br>yolo.close_session()<br></code></pre></td></tr></table></figure><p><img src="/img/5.2/10.png" alt="检测结果"></p><p><img src="/img/5.2/11.png" alt="检测结果"></p><p>附录A</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br></pre></td><td class="code"><pre><code class="hljs python">训练和检测都导入了yolo3.model：<br><span class="hljs-string">&quot;&quot;&quot;YOLO_v3 Model Defined in Keras.&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> wraps<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D<br><span class="hljs-keyword">from</span> keras.layers.advanced_activations <span class="hljs-keyword">import</span> LeakyReLU<br><span class="hljs-keyword">from</span> keras.layers.normalization <span class="hljs-keyword">import</span> BatchNormalization<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> keras.regularizers <span class="hljs-keyword">import</span> l2<br><br><span class="hljs-keyword">from</span> yolo3.utils <span class="hljs-keyword">import</span> compose<br><br><span class="hljs-comment"># DarknetConv2D()，DarknetConv2D_BN_Leaky()，resblock_body()三个函数构成了darknet_body()卷积层框#架</span><br><span class="hljs-meta">@wraps(<span class="hljs-params">Conv2D</span>)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Wrapper to set Darknet parameters for Convolution2D.&quot;&quot;&quot;</span><br>    darknet_conv_kwargs = &#123;<span class="hljs-string">&#x27;kernel_regularizer&#x27;</span>: l2(<span class="hljs-number">5e-4</span>)&#125;<br>    darknet_conv_kwargs[<span class="hljs-string">&#x27;padding&#x27;</span>] = <span class="hljs-string">&#x27;valid&#x27;</span> <span class="hljs-keyword">if</span> kwargs.get(<span class="hljs-string">&#x27;strides&#x27;</span>)==(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;same&#x27;</span><br>    darknet_conv_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> Conv2D(*args, **darknet_conv_kwargs)<br><span class="hljs-comment">#注意 DARKNET卷积这里激活函数是LEAKYRELU</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DarknetConv2D_BN_Leaky</span>(<span class="hljs-params">*args, **kwargs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Darknet Convolution2D followed by BatchNormalization and LeakyReLU.&quot;&quot;&quot;</span><br>    no_bias_kwargs = &#123;<span class="hljs-string">&#x27;use_bias&#x27;</span>: <span class="hljs-literal">False</span>&#125;<br>    no_bias_kwargs.update(kwargs)<br>    <span class="hljs-keyword">return</span> compose(<br>        DarknetConv2D(*args, **no_bias_kwargs),<br>        BatchNormalization(),<br>        LeakyReLU(alpha=<span class="hljs-number">0.1</span>))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resblock_body</span>(<span class="hljs-params">x, num_filters, num_blocks</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;A series of resblocks starting with a downsampling Convolution2D&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># Darknet uses left and top padding instead of &#x27;same&#x27; mode</span><br><span class="hljs-comment"># Darknet使用向左和向上填充代替same模式。</span><br><span class="hljs-comment">#DARKNET每块之间，使用了，（1，0，1，0）的PADDING层。</span><br>    x = ZeroPadding2D(((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)))(x)<br>    x = DarknetConv2D_BN_Leaky(num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))(x)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks):<br>        y = compose(<br>                DarknetConv2D_BN_Leaky(num_filters//<span class="hljs-number">2</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                DarknetConv2D_BN_Leaky(num_filters, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)))(x)<br>        x = Add()([x,y])<br><span class="hljs-keyword">return</span> x<br><span class="hljs-comment">#创建darknet网络结构，有52层卷积层。包含五个resblock</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet_body</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Darknent body having 52 Convolution2D layers&#x27;&#x27;&#x27;</span><br>    x = DarknetConv2D_BN_Leaky(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))(x)<br>    x = resblock_body(x, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>)<br>    x = resblock_body(x, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>)<br>    x = resblock_body(x, <span class="hljs-number">256</span>, <span class="hljs-number">8</span>)<br>    x = resblock_body(x, <span class="hljs-number">512</span>, <span class="hljs-number">8</span>)<br>    x = resblock_body(x, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span>)<br>    <span class="hljs-keyword">return</span> x<br><span class="hljs-comment">#Convs由make_last_layers函数来实现。</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_last_layers</span>(<span class="hljs-params">x, num_filters, out_filters</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer&#x27;&#x27;&#x27;</span><br>    x = compose(<br>            DarknetConv2D_BN_Leaky(num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            DarknetConv2D_BN_Leaky(num_filters*<span class="hljs-number">2</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D_BN_Leaky(num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            DarknetConv2D_BN_Leaky(num_filters*<span class="hljs-number">2</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D_BN_Leaky(num_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))(x)<br>    y = compose(<br>            DarknetConv2D_BN_Leaky(num_filters*<span class="hljs-number">2</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D(out_filters, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))(x)<br>    <span class="hljs-keyword">return</span> x, y<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_body</span>(<span class="hljs-params">inputs, num_anchors, num_classes</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Create YOLO_V3 model CNN body in Keras.&quot;&quot;&quot;</span><br>darknet = Model(inputs, darknet_body(inputs)) <span class="hljs-comment"># darknet_body(inputs)创建一个darknet网络</span><br><span class="hljs-comment">#以下语句是特征金字塔（FPN）的具体实现。 </span><br>    x, y1 = make_last_layers(darknet.output, <span class="hljs-number">512</span>, num_anchors*(num_classes+<span class="hljs-number">5</span>))<br><span class="hljs-comment">#compose函数，从左向右评估函数</span><br>    x = compose(<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            UpSampling2D(<span class="hljs-number">2</span>))(x)<br>    x = Concatenate()([x,darknet.layers[<span class="hljs-number">152</span>].output])<br>    x, y2 = make_last_layers(x, <span class="hljs-number">256</span>, num_anchors*(num_classes+<span class="hljs-number">5</span>))<br><br>    x = compose(<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            UpSampling2D(<span class="hljs-number">2</span>))(x)<br>    x = Concatenate()([x,darknet.layers[<span class="hljs-number">92</span>].output])<br>    x, y3 = make_last_layers(x, <span class="hljs-number">128</span>, num_anchors*(num_classes+<span class="hljs-number">5</span>))<br><br>    <span class="hljs-keyword">return</span> Model(inputs, [y1,y2,y3])<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tiny_yolo_body</span>(<span class="hljs-params">inputs, num_anchors, num_classes</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Create Tiny YOLO_v3 model CNN body in keras.&#x27;&#x27;&#x27;</span><br>    x1 = compose(<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">16</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)))(inputs)<br>    x2 = compose(<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), padding=<span class="hljs-string">&#x27;same&#x27;</span>),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">1024</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))(x1)<br>    y1 = compose(<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))(x2)<br><br>    x2 = compose(<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">128</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>            UpSampling2D(<span class="hljs-number">2</span>))(x2)<br>    y2 = compose(<br>            Concatenate(),<br>            DarknetConv2D_BN_Leaky(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            DarknetConv2D(num_anchors*(num_classes+<span class="hljs-number">5</span>), (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))([x2,x1])<br><br>    <span class="hljs-keyword">return</span> Model(inputs, [y1,y2])<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_head</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, calc_loss=<span class="hljs-literal">False</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Convert final layer features to bounding box parameters.&quot;&quot;&quot;</span><br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)  <span class="hljs-comment">#num_anchors = 3</span><br>    <span class="hljs-comment"># Reshape to batch, height, width, num_anchors, box_params.</span><br>    anchors_tensor = K.reshape(K.constant(anchors), [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, num_anchors, <span class="hljs-number">2</span>])  <span class="hljs-comment">#reshape -&gt;(1,1,1,3,2)</span><br>grid_shape = K.shape(feats)[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]   <span class="hljs-comment"># height, width   (?,13,13,27) -&gt; (13,13)</span><br><span class="hljs-comment">#grid_y和grid_x用于生成网格grid，通过arange、reshape、tile的组合， 创建y轴的0~12的组合#grid_y，再创建x轴的0~12的组合grid_x，将两者拼接concatenate，就是grid；</span><br>    grid_y = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">0</span>]), [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid_x = K.tile(K.reshape(K.arange(<span class="hljs-number">0</span>, stop=grid_shape[<span class="hljs-number">1</span>]), [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),<br>        [grid_shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    grid = K.concatenate([grid_x, grid_y])<br>    grid = K.cast(grid, K.dtype(feats))  <span class="hljs-comment">#K.cast():把grid中值的类型变为和feats中值的类型一样</span><br><br>    feats = K.reshape(<br>        feats, [-<span class="hljs-number">1</span>, grid_shape[<span class="hljs-number">0</span>], grid_shape[<span class="hljs-number">1</span>], num_anchors, num_classes + <span class="hljs-number">5</span>])<br><span class="hljs-comment">#将feats的最后一维展开，将anchors与其他数据（类别数+4个框值+框置信度）分离</span><br><br><span class="hljs-comment"># Adjust preditions to each spatial grid point and anchor size.</span><br><span class="hljs-comment">#xywh的计算公式，见边界框回归公式。</span><br><span class="hljs-comment">#tx、ty、tw和th是feats值，而bx、by、bw和bh是输出值</span><br>    box_xy = (K.sigmoid(feats[..., :<span class="hljs-number">2</span>]) + grid) / K.cast(grid_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))  <span class="hljs-comment">#sigmoid:σ</span><br>    box_wh = K.exp(feats[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]) * anchors_tensor / K.cast(input_shape[::-<span class="hljs-number">1</span>], K.dtype(feats))<br>    box_confidence = K.sigmoid(feats[..., <span class="hljs-number">4</span>:<span class="hljs-number">5</span>])<br>box_class_probs = K.sigmoid(feats[..., <span class="hljs-number">5</span>:])<br><span class="hljs-comment"># ...操作符，在Python中，“...”(ellipsis)操作符，表示其他维度不变，只操作最前或最后1维；</span><br>    <span class="hljs-keyword">if</span> calc_loss == <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">return</span> grid, feats, box_xy, box_wh<br><span class="hljs-comment"># 将box_xy，box_xy 从OUTPUT的预测数据转为真实坐标。</span><br>    <span class="hljs-keyword">return</span> box_xy, box_wh, box_confidence, box_class_probs<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_correct_boxes</span>(<span class="hljs-params">box_xy, box_wh, input_shape, image_shape</span>):</span>  <span class="hljs-comment">#得到正确的x,y,w,h</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Get corrected boxes&#x27;&#x27;&#x27;</span><br>    box_yx = box_xy[..., ::-<span class="hljs-number">1</span>]  <span class="hljs-comment">#“::-1”是颠倒数组的值</span><br>    box_hw = box_wh[..., ::-<span class="hljs-number">1</span>]   <br>    input_shape = K.cast(input_shape, K.dtype(box_yx))<br>    image_shape = K.cast(image_shape, K.dtype(box_yx))<br>    new_shape = K.<span class="hljs-built_in">round</span>(image_shape * K.<span class="hljs-built_in">min</span>(input_shape/image_shape))<br>    offset = (input_shape-new_shape)/<span class="hljs-number">2.</span>/input_shape<br>    scale = input_shape/new_shape<br>    box_yx = (box_yx - offset) * scale<br>    box_hw *= scale<br><br>    box_mins = box_yx - (box_hw / <span class="hljs-number">2.</span>)<br>    box_maxes = box_yx + (box_hw / <span class="hljs-number">2.</span>)<br>    boxes =  K.concatenate([<br>        box_mins[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_min</span><br>        box_mins[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>],  <span class="hljs-comment"># x_min</span><br>        box_maxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">1</span>],  <span class="hljs-comment"># y_max</span><br>        box_maxes[..., <span class="hljs-number">1</span>:<span class="hljs-number">2</span>]  <span class="hljs-comment"># x_max</span><br>    ])<br><br>    <span class="hljs-comment"># Scale boxes back to original image shape.</span><br>    boxes *= K.concatenate([image_shape, image_shape])<br>    <span class="hljs-keyword">return</span> boxes<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_boxes_and_scores</span>(<span class="hljs-params">feats, anchors, num_classes, input_shape, image_shape</span>):</span><br><span class="hljs-comment"># feats:输出的shape，-&gt;(?,13,13,27); anchors:每层对应的3个anchor box</span><br><span class="hljs-comment"># num_classes: 类别数（4）; input_shape:（416,416）; image_shape:图像尺寸</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Process Conv layer output&#x27;&#x27;&#x27;</span><br>    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,<br>        anchors, num_classes, input_shape)<br><span class="hljs-comment">#yolo_head():box_xy是box的中心坐标，(0~1)相对位置；box_wh是box的宽高，(0~1)相对值；</span><br><span class="hljs-comment">#box_confidence是框中物体置信度；box_class_probs是类别置信度；</span><br>boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)<br><span class="hljs-comment">#将box_xy和box_wh的(0~1)相对值，转换为真实坐标，输出boxes是(y_min,x_min,y_max,x_max)的#值</span><br>boxes = K.reshape(boxes, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment">#reshape,将不同网格的值转换为框的列表。即（?,13,13,3,4）-&gt;(?,4)  ？：框的数目</span><br>box_scores = box_confidence * box_class_probs<br><span class="hljs-comment">#框的得分=框的置信度*类别置信度</span><br>    box_scores = K.reshape(box_scores, [-<span class="hljs-number">1</span>, num_classes])<br><span class="hljs-comment">#reshape,将框的得分展平，变为(?,4); ?:框的数目</span><br>    <span class="hljs-keyword">return</span> boxes, box_scores<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_eval</span>(<span class="hljs-params">yolo_outputs, </span></span><br><span class="hljs-function"><span class="hljs-params"><span class="hljs-comment">#模型输出，格式如下[（?，13,13,27）（?，26,26,27）（?,52,52,27）] ?:bitch size; 13-26-52:多尺度预测； 27：预测值（3*（4+5））</span></span></span><br><span class="hljs-function"><span class="hljs-params">              anchors,</span></span><br><span class="hljs-function"><span class="hljs-params"><span class="hljs-comment">#[(10,13), (16,30), (33,23), (30,61), (62,45), (59,119), (116,90), (156,198),(373,326)]</span></span></span><br><span class="hljs-function"><span class="hljs-params">              num_classes, <span class="hljs-comment"># 类别个数，此数据集有4类 </span></span></span><br><span class="hljs-function"><span class="hljs-params">              image_shape, <span class="hljs-comment">#placeholder类型的TF参数，默认(416, 416)；</span></span></span><br><span class="hljs-function"><span class="hljs-params">              max_boxes=<span class="hljs-number">20</span>,</span></span><br><span class="hljs-function"><span class="hljs-params"><span class="hljs-comment">#每张图每类最多检测到20个框同类别框的IoU阈值，大于阈值的重叠框被删除，重叠物体较多，则调高阈值，重叠物体较少，则调低阈值</span></span></span><br><span class="hljs-function"><span class="hljs-params">              score_threshold=<span class="hljs-number">.6</span>,</span></span><br><span class="hljs-function"><span class="hljs-params"><span class="hljs-comment">#框置信度阈值，小于阈值的框被删除，需要的框较多，则调低阈值，需要的框较少，则调高阈值；</span></span></span><br><span class="hljs-function"><span class="hljs-params">              iou_threshold=<span class="hljs-number">.5</span></span>):</span><br><span class="hljs-comment">#同类别框的IoU阈值，大于阈值的重叠框被删除，重叠物体较多，则调高阈值，重叠物体较少，则调低阈值</span><br>    <span class="hljs-string">&quot;&quot;&quot;Evaluate YOLO model on given input and return filtered boxes.&quot;&quot;&quot;</span><br>num_layers = <span class="hljs-built_in">len</span>(yolo_outputs) <span class="hljs-comment">#yolo的输出层数；num_layers = 3  -&gt; 13-26-52</span><br><span class="hljs-comment"># 不同的欺骗对应不同的ANCHOR大小。</span><br>    anchor_mask = [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]] <span class="hljs-keyword">if</span> num_layers==<span class="hljs-number">3</span> <span class="hljs-keyword">else</span> [[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]] <span class="hljs-comment"># default setting</span><br><span class="hljs-comment">#每层分配3个anchor box.如13*13分配到[6,7,8]即[（116,90）（156,198）（373,326）]</span><br>input_shape = K.shape(yolo_outputs[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] * <span class="hljs-number">32</span><br><span class="hljs-comment">#输入shape(?,13,13,255);即第一维和第二维分别乘32，输出的图片尺寸为（416,416）</span><br>    boxes = []<br>    box_scores = []<br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], <br><span class="hljs-comment"># yolo_boxes_and_scores()函数见附录yolo3.model</span><br>            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)<br>        boxes.append(_boxes)<br>        box_scores.append(_box_scores)<br>    boxes = K.concatenate(boxes, axis=<span class="hljs-number">0</span>) <span class="hljs-comment">#K.concatenate:将数据展平 -&gt;(?,4)</span><br>    box_scores = K.concatenate(box_scores, axis=<span class="hljs-number">0</span>) <span class="hljs-comment"># -&gt;(?,)</span><br><br>    mask = box_scores &gt;= score_threshold <br><span class="hljs-comment">#MASK掩码，过滤小于score阈值的值，只保留大于阈值的值</span><br>    max_boxes_tensor = K.constant(max_boxes, dtype=<span class="hljs-string">&#x27;int32&#x27;</span>) <span class="hljs-comment">#最大检测框数20</span><br>    boxes_ = []<br>    scores_ = []<br>    classes_ = []<br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_classes):<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> use keras backend instead of tf.</span><br>        class_boxes = tf.boolean_mask(boxes, mask[:, c]) <span class="hljs-comment">#通过掩码MASK和类别C筛选框boxes</span><br>        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c]) <br><span class="hljs-comment">#通过掩码MASK和类别C筛选scores</span><br>        nms_index = tf.image.non_max_suppression(  <span class="hljs-comment">#运行非极大抑制</span><br>            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)<br>        class_boxes = K.gather(class_boxes, nms_index)<br><span class="hljs-comment">#K.gather:根据索引nms_index选择class_boxes</span><br>        class_box_scores = K.gather(class_box_scores, nms_index)<br><span class="hljs-comment">#根据索引nms_index选择class_box_score)</span><br>        classes = K.ones_like(class_box_scores, <span class="hljs-string">&#x27;int32&#x27;</span>) * c <span class="hljs-comment">#计算类的框得分</span><br>        boxes_.append(class_boxes)<br>        scores_.append(class_box_scores)<br>        classes_.append(classes)<br>boxes_ = K.concatenate(boxes_, axis=<span class="hljs-number">0</span>)<br><span class="hljs-comment">#K.concatenate().将相同维度的数据连接在一起；把boxes_展平。  -&gt; 变成格式:(?,4);  ?:框的个#数；4：（x,y,w,h）</span><br>    scores_ = K.concatenate(scores_, axis=<span class="hljs-number">0</span>)<br>    classes_ = K.concatenate(classes_, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> boxes_, scores_, classes_<br><span class="hljs-comment">#图片缩放到固定大小之后就是生成对应的数据</span><br><span class="hljs-comment">#通过model.py(preprocess_true_boxes实现box框的框定</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_true_boxes</span>(<span class="hljs-params">true_boxes, input_shape, anchors, num_classes</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Preprocess true boxes to training input format</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    true_boxes: array, shape=(m, T, 5)</span><br><span class="hljs-string">        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.</span><br><span class="hljs-string">    input_shape: array-like, hw, multiples of 32</span><br><span class="hljs-string">    anchors: array, shape=(N, 2), wh</span><br><span class="hljs-string">    num_classes: integer</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    y_true: list of array, shape like yolo_outputs, xywh are reletive value</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">assert</span> (true_boxes[..., <span class="hljs-number">4</span>]&lt;num_classes).<span class="hljs-built_in">all</span>(), <span class="hljs-string">&#x27;class id must be less than num_classes&#x27;</span><br>num_layers = <span class="hljs-built_in">len</span>(anchors)//<span class="hljs-number">3</span> <span class="hljs-comment"># default setting</span><br><span class="hljs-comment"># 不同的欺骗对应不同的ANCHOR大小。</span><br>    anchor_mask = [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]] <span class="hljs-keyword">if</span> num_layers==<span class="hljs-number">3</span> <span class="hljs-keyword">else</span> [[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]]<br><br>    true_boxes = np.array(true_boxes, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>    input_shape = np.array(input_shape, dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    boxes_xy = (true_boxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">2</span>] + true_boxes[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]) // <span class="hljs-number">2</span><br>    boxes_wh = true_boxes[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>] - true_boxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">2</span>]<br><span class="hljs-comment"># 生成true_box做了类似归一化的处理，因此，true_box小于1，box_loss_scale一定大于0.</span><br>    true_boxes[..., <span class="hljs-number">0</span>:<span class="hljs-number">2</span>] = boxes_xy/input_shape[::-<span class="hljs-number">1</span>]<br>    true_boxes[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>] = boxes_wh/input_shape[::-<span class="hljs-number">1</span>]<br><br>    m = true_boxes.shape[<span class="hljs-number">0</span>]<br>    grid_shapes = [input_shape//&#123;<span class="hljs-number">0</span>:<span class="hljs-number">32</span>, <span class="hljs-number">1</span>:<span class="hljs-number">16</span>, <span class="hljs-number">2</span>:<span class="hljs-number">8</span>&#125;[l] <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]<br>    y_true = [np.zeros((m,grid_shapes[l][<span class="hljs-number">0</span>],grid_shapes[l][<span class="hljs-number">1</span>],<span class="hljs-built_in">len</span>(anchor_mask[l]),<span class="hljs-number">5</span>+num_classes),<br>        dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]<br><br>    <span class="hljs-comment"># Expand dim to apply broadcasting.</span><br>    anchors = np.expand_dims(anchors, <span class="hljs-number">0</span>)<br>    anchor_maxes = anchors / <span class="hljs-number">2.</span><br>    anchor_mins = -anchor_maxes<br>    valid_mask = boxes_wh[..., <span class="hljs-number">0</span>]&gt;<span class="hljs-number">0</span><br><span class="hljs-comment">#每个图片都需要单独处理。</span><br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>        <span class="hljs-comment"># Discard zero rows.</span><br>        wh = boxes_wh[b, valid_mask[b]]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(wh)==<span class="hljs-number">0</span>: <span class="hljs-keyword">continue</span><br>        <span class="hljs-comment"># Expand dim to apply broadcasting.</span><br>        wh = np.expand_dims(wh, -<span class="hljs-number">2</span>)<br>        box_maxes = wh / <span class="hljs-number">2.</span><br>        box_mins = -box_maxes<br><br>        intersect_mins = np.maximum(box_mins, anchor_mins)<br>        intersect_maxes = np.minimum(box_maxes, anchor_maxes)<br>        intersect_wh = np.maximum(intersect_maxes - intersect_mins, <span class="hljs-number">0.</span>)<br>        intersect_area = intersect_wh[..., <span class="hljs-number">0</span>] * intersect_wh[..., <span class="hljs-number">1</span>]<br>        box_area = wh[..., <span class="hljs-number">0</span>] * wh[..., <span class="hljs-number">1</span>]<br>        anchor_area = anchors[..., <span class="hljs-number">0</span>] * anchors[..., <span class="hljs-number">1</span>]<br>        iou = intersect_area / (box_area + anchor_area - intersect_area)<br><br>        <span class="hljs-comment"># Find best anchor for each true box</span><br><span class="hljs-comment"># 9个设定的ANCHOR去框定每个输入的BOX。</span><br>        best_anchor = np.argmax(iou, axis=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">for</span> t, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(best_anchor):<br>            <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>                <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> anchor_mask[l]:<br>                    i = np.floor(true_boxes[b,t,<span class="hljs-number">0</span>]*grid_shapes[l][<span class="hljs-number">1</span>]).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br>                    j = np.floor(true_boxes[b,t,<span class="hljs-number">1</span>]*grid_shapes[l][<span class="hljs-number">0</span>]).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br>                    k = anchor_mask[l].index(n)<br>                    c = true_boxes[b,t, <span class="hljs-number">4</span>].astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br> <span class="hljs-comment"># 设定数据</span><br> <span class="hljs-comment"># 将T个box的标的数据统一放置到3*B*W*H*3的维度上。</span><br>                    y_true[l][b, j, i, k, <span class="hljs-number">0</span>:<span class="hljs-number">4</span>] = true_boxes[b,t, <span class="hljs-number">0</span>:<span class="hljs-number">4</span>]<br>                    y_true[l][b, j, i, k, <span class="hljs-number">4</span>] = <span class="hljs-number">1</span><br>                    y_true[l][b, j, i, k, <span class="hljs-number">5</span>+c] = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> y_true<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">box_iou</span>(<span class="hljs-params">b1, b2</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Return iou tensor</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    b1: tensor, shape=(i1,...,iN, 4), xywh</span><br><span class="hljs-string">    b2: tensor, shape=(j, 4), xywh</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    iou: tensor, shape=(i1,...,iN, j)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-comment"># Expand dim to apply broadcasting.</span><br>    b1 = K.expand_dims(b1, -<span class="hljs-number">2</span>)<br>    b1_xy = b1[..., :<span class="hljs-number">2</span>]<br>    b1_wh = b1[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]<br>    b1_wh_half = b1_wh/<span class="hljs-number">2.</span><br>    b1_mins = b1_xy - b1_wh_half<br>    b1_maxes = b1_xy + b1_wh_half<br><br>    <span class="hljs-comment"># Expand dim to apply broadcasting.</span><br>    b2 = K.expand_dims(b2, <span class="hljs-number">0</span>)<br>    b2_xy = b2[..., :<span class="hljs-number">2</span>]<br>    b2_wh = b2[..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>]<br>    b2_wh_half = b2_wh/<span class="hljs-number">2.</span><br>    b2_mins = b2_xy - b2_wh_half<br>    b2_maxes = b2_xy + b2_wh_half<br><br>    intersect_mins = K.maximum(b1_mins, b2_mins)<br>    intersect_maxes = K.minimum(b1_maxes, b2_maxes)<br>    intersect_wh = K.maximum(intersect_maxes - intersect_mins, <span class="hljs-number">0.</span>)<br>    intersect_area = intersect_wh[..., <span class="hljs-number">0</span>] * intersect_wh[..., <span class="hljs-number">1</span>]<br>    b1_area = b1_wh[..., <span class="hljs-number">0</span>] * b1_wh[..., <span class="hljs-number">1</span>]<br>    b2_area = b2_wh[..., <span class="hljs-number">0</span>] * b2_wh[..., <span class="hljs-number">1</span>]<br>    iou = intersect_area / (b1_area + b2_area - intersect_area)<br><br>    <span class="hljs-keyword">return</span> iou<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">yolo_loss</span>(<span class="hljs-params">args, anchors, num_classes, ignore_thresh=<span class="hljs-number">.5</span>, print_loss=<span class="hljs-literal">False</span></span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Return yolo_loss tensor </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body</span><br><span class="hljs-string">    y_true: list of array, the output of preprocess_true_boxes</span><br><span class="hljs-string">    anchors: array, shape=(N, 2), wh</span><br><span class="hljs-string">    num_classes: integer</span><br><span class="hljs-string">    ignore_thresh: float, the iou threshold whether to ignore object confidence loss</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    loss: tensor, shape=(1,)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    num_layers = <span class="hljs-built_in">len</span>(anchors)//<span class="hljs-number">3</span> <span class="hljs-comment"># default setting</span><br>    yolo_outputs = args[:num_layers]<br>y_true = args[num_layers:]<br><span class="hljs-comment"># 不同的欺骗对应不同的ANCHOR大小。</span><br>anchor_mask = [[<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]] <span class="hljs-keyword">if</span> num_layers==<span class="hljs-number">3</span> <span class="hljs-keyword">else</span> [[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]]<br><span class="hljs-comment"># 根据模型返回的OUTPUT计算输入图片SHAPE以及3个LAYER下，3个切片的大小。</span><br>    input_shape = K.cast(K.shape(yolo_outputs[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>] * <span class="hljs-number">32</span>, K.dtype(y_true[<span class="hljs-number">0</span>]))<br>    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>], K.dtype(y_true[<span class="hljs-number">0</span>])) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]<br>    loss = <span class="hljs-number">0</span><br>m = K.shape(yolo_outputs[<span class="hljs-number">0</span>])[<span class="hljs-number">0</span>] <span class="hljs-comment"># batch size, tensor #m表示采样batch_size</span><br>    mf = K.cast(m, K.dtype(yolo_outputs[<span class="hljs-number">0</span>]))<br><span class="hljs-comment"># loss是需要三层分别计算的</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>   <span class="hljs-comment"># 置信率</span><br>        object_mask = y_true[l][..., <span class="hljs-number">4</span>:<span class="hljs-number">5</span>]<br>        <span class="hljs-comment"># 分类</span><br>        true_class_probs = y_true[l][..., <span class="hljs-number">5</span>:]<br><span class="hljs-comment"># raw_pred是yolo_outputs[l]，经过yolo_head函数后，raw_pred数据并没有改变。</span><br>        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],<br>             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=<span class="hljs-literal">True</span>)<br>        pred_box = K.concatenate([pred_xy, pred_wh])<br><br>        <span class="hljs-comment"># Darknet raw box to calculate loss.</span><br><span class="hljs-comment"># Darknet原始盒子来计算损失。</span><br>        raw_true_xy = y_true[l][..., :<span class="hljs-number">2</span>]*grid_shapes[l][::-<span class="hljs-number">1</span>] - grid<br>        raw_true_wh = K.log(y_true[l][..., <span class="hljs-number">2</span>:<span class="hljs-number">4</span>] / anchors[anchor_mask[l]] * input_shape[::-<span class="hljs-number">1</span>])<br>        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) <span class="hljs-comment"># avoid log(0)=-inf</span><br>        box_loss_scale = <span class="hljs-number">2</span> - y_true[l][...,<span class="hljs-number">2</span>:<span class="hljs-number">3</span>]*y_true[l][...,<span class="hljs-number">3</span>:<span class="hljs-number">4</span>]<br><br>        <span class="hljs-comment"># Find ignore mask, iterate over each of batch.</span><br>        ignore_mask = tf.TensorArray(K.dtype(y_true[<span class="hljs-number">0</span>]), size=<span class="hljs-number">1</span>, dynamic_size=<span class="hljs-literal">True</span>)<br>        object_mask_bool = K.cast(object_mask, <span class="hljs-string">&#x27;bool&#x27;</span>)<br><span class="hljs-comment"># loop_body计算batch_size内最大的IOU</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loop_body</span>(<span class="hljs-params">b, ignore_mask</span>):</span><br><span class="hljs-comment"># tf.boolean_mask Apply boolean mask to tensor. Numpy equivalent is tensor[mask]. 根据y_true的置信度标识，来框定y_true的坐标系参数是否有效。</span><br>            true_box = tf.boolean_mask(y_true[l][b,...,<span class="hljs-number">0</span>:<span class="hljs-number">4</span>], object_mask_bool[b,...,<span class="hljs-number">0</span>])<br>            iou = box_iou(pred_box[b], true_box)<br>            best_iou = K.<span class="hljs-built_in">max</span>(iou, axis=-<span class="hljs-number">1</span>)<br><span class="hljs-comment">#当一张图片的最大IOU低于ignore_thresh，则认为图片内是没有目标</span><br>            ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box)))<br>            <span class="hljs-keyword">return</span> b+<span class="hljs-number">1</span>, ignore_mask<br>_, ignore_mask = K.control_flow_ops.while_loop(<span class="hljs-keyword">lambda</span> b,*args: b&lt;m, loop_body, [<span class="hljs-number">0</span>, <br>ignore_mask])<br>        ignore_mask = ignore_mask.stack()<br>        ignore_mask = K.expand_dims(ignore_mask, -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># K.binary_crossentropy is helpful to avoid exp overflow.</span><br>        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,<span class="hljs-number">0</span>:<span class="hljs-number">2</span>], from_logits=<span class="hljs-literal">True</span>)<br>        wh_loss = object_mask * box_loss_scale * <span class="hljs-number">0.5</span> * K.square(raw_true_wh-raw_pred[...,<span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br><span class="hljs-comment"># 置信度</span><br>        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,<span class="hljs-number">4</span>:<span class="hljs-number">5</span>], from_logits=<span class="hljs-literal">True</span>)+ (<span class="hljs-number">1</span>-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,<span class="hljs-number">4</span>:<span class="hljs-number">5</span>], from_logits=<span class="hljs-literal">True</span>) * ignore_mask<br><span class="hljs-comment"># 分类</span><br>class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,<span class="hljs-number">5</span>:], from_logits=<span class="hljs-literal">True</span>)<br><br>        xy_loss = K.<span class="hljs-built_in">sum</span>(xy_loss) / mf<br>        wh_loss = K.<span class="hljs-built_in">sum</span>(wh_loss) / mf<br>        confidence_loss = K.<span class="hljs-built_in">sum</span>(confidence_loss) / mf<br>        class_loss = K.<span class="hljs-built_in">sum</span>(class_loss) / mf<br>        loss += xy_loss + wh_loss + confidence_loss + class_loss<br>        <span class="hljs-keyword">if</span> print_loss:<br>loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.<span class="hljs-built_in">sum</span>(ignore_mask)], message=<span class="hljs-string">&#x27;loss: &#x27;</span>)<br><span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><p>附录B<br>训练和检测都导入了yolo3.utils：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#yolo3.utils中是其他使用函数，主要用于keras-yolo数据增强的一些方法。</span><br><span class="hljs-string">&quot;&quot;&quot;Miscellaneous utility functions.&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> reduce<br><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> rgb_to_hsv, hsv_to_rgb<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compose</span>(<span class="hljs-params">*funcs</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Compose arbitrarily many functions, evaluated left to right.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Reference: https://mathieularose.com/function-composition-in-python/</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># return lambda x: reduce(lambda v, f: f(v), funcs, x)</span><br>    <span class="hljs-keyword">if</span> funcs:<br>        <span class="hljs-keyword">return</span> reduce(<span class="hljs-keyword">lambda</span> f, g: <span class="hljs-keyword">lambda</span> *a, **kw: g(f(*a, **kw)), funcs)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Composition of empty sequence not supported.&#x27;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">letterbox_image</span>(<span class="hljs-params">image, size</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;resize image with unchanged aspect ratio using padding&#x27;&#x27;&#x27;</span><br>    iw, ih = image.size<br>    w, h = size<br>    scale = <span class="hljs-built_in">min</span>(w/iw, h/ih)<br>    nw = <span class="hljs-built_in">int</span>(iw*scale)<br>    nh = <span class="hljs-built_in">int</span>(ih*scale)<br><br>    image = image.resize((nw,nh), Image.BICUBIC)<br>    new_image = Image.new(<span class="hljs-string">&#x27;RGB&#x27;</span>, size, (<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>))<br>    new_image.paste(image, ((w-nw)//<span class="hljs-number">2</span>, (h-nh)//<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> new_image<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rand</span>(<span class="hljs-params">a=<span class="hljs-number">0</span>, b=<span class="hljs-number">1</span></span>):</span><br>    <span class="hljs-keyword">return</span> np.random.rand()*(b-a) + a<br><span class="hljs-comment">#在utils.py(get_random_data)函数中实现数据处理</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_random_data</span>(<span class="hljs-params">annotation_line, input_shape, random=<span class="hljs-literal">True</span>, max_boxes=<span class="hljs-number">20</span>, jitter=<span class="hljs-number">.3</span>, hue=<span class="hljs-number">.1</span>, sat=<span class="hljs-number">1.5</span>, val=<span class="hljs-number">1.5</span>, proc_img=<span class="hljs-literal">True</span></span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;random preprocessing for real-time data augmentation&#x27;&#x27;&#x27;</span><br>    line = annotation_line.split()<br>    image = Image.<span class="hljs-built_in">open</span>(line[<span class="hljs-number">0</span>])<br>    iw, ih = image.size<br>    h, w = input_shape<br>    box = np.array([np.array(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>,box.split(<span class="hljs-string">&#x27;,&#x27;</span>)))) <span class="hljs-keyword">for</span> box <span class="hljs-keyword">in</span> line[<span class="hljs-number">1</span>:]])<br><span class="hljs-comment">#not random的实现</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> random:<br>        <span class="hljs-comment"># resize image</span><br>        <span class="hljs-comment">#缩放大小</span><br>        scale = <span class="hljs-built_in">min</span>(w/iw, h/ih)<br>        nw = <span class="hljs-built_in">int</span>(iw*scale)<br>        nh = <span class="hljs-built_in">int</span>(ih*scale)<br>        <span class="hljs-comment">#中心点</span><br>        dx = (w-nw)//<span class="hljs-number">2</span><br>        dy = (h-nh)//<span class="hljs-number">2</span><br>        image_data=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> proc_img:<br>            image = image.resize((nw,nh), Image.BICUBIC)<br>   <span class="hljs-comment">#背景</span><br>            new_image = Image.new(<span class="hljs-string">&#x27;RGB&#x27;</span>, (w,h), (<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>))<br>   <span class="hljs-comment">#黏贴图pain</span><br>            new_image.paste(image, (dx, dy))<br>   <span class="hljs-comment">#归一化</span><br>            image_data = np.array(new_image)/<span class="hljs-number">255.</span><br><br>        <span class="hljs-comment"># correct boxes</span><br>        box_data = np.zeros((max_boxes,<span class="hljs-number">5</span>))<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(box)&gt;<span class="hljs-number">0</span>:<br>            np.random.shuffle(box)<br>   <span class="hljs-comment"># 最大20个BOX。</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(box)&gt;max_boxes: box = box[:max_boxes]<br>   <span class="hljs-comment">#根据缩放大小，生成新图中的BOX位置</span><br>            box[:, [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]] = box[:, [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]]*scale + dx<br>            box[:, [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]] = box[:, [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]]*scale + dy<br>            box_data[:<span class="hljs-built_in">len</span>(box)] = box<br><br>        <span class="hljs-keyword">return</span> image_data, box_data<br><br><span class="hljs-comment"># resize image</span><br><span class="hljs-comment"># 随机生成宽高比</span><br>new_ar = w/h * rand(<span class="hljs-number">1</span>-jitter,<span class="hljs-number">1</span>+jitter)/rand(<span class="hljs-number">1</span>-jitter,<span class="hljs-number">1</span>+jitter)<br><span class="hljs-comment"># 随机生成缩放比例。</span><br>scale = rand(<span class="hljs-number">.25</span>, <span class="hljs-number">2</span>)<br><span class="hljs-comment"># 生成新的高宽数据，可能放大2倍。</span><br>    <span class="hljs-keyword">if</span> new_ar &lt; <span class="hljs-number">1</span>:<br>        nh = <span class="hljs-built_in">int</span>(scale*h)<br>        nw = <span class="hljs-built_in">int</span>(nh*new_ar)<br>    <span class="hljs-keyword">else</span>:<br>        nw = <span class="hljs-built_in">int</span>(scale*w)<br>        nh = <span class="hljs-built_in">int</span>(nw/new_ar)<br>    image = image.resize((nw,nh), Image.BICUBIC)<br><br><span class="hljs-comment"># place image</span><br><span class="hljs-comment"># 随机水平位移</span><br>    dx = <span class="hljs-built_in">int</span>(rand(<span class="hljs-number">0</span>, w-nw))<br>    dy = <span class="hljs-built_in">int</span>(rand(<span class="hljs-number">0</span>, h-nh))<br>    new_image = Image.new(<span class="hljs-string">&#x27;RGB&#x27;</span>, (w,h), (<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>))<br>    new_image.paste(image, (dx, dy))<br>    image = new_image<br><br><span class="hljs-comment"># flip image or not</span><br><span class="hljs-comment"># 翻转</span><br>    flip = rand()&lt;<span class="hljs-number">.5</span><br>    <span class="hljs-keyword">if</span> flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)<br><br><span class="hljs-comment"># distort image</span><br><span class="hljs-comment"># HSV抖动</span><br>    hue = rand(-hue, hue)<br>    sat = rand(<span class="hljs-number">1</span>, sat) <span class="hljs-keyword">if</span> rand()&lt;<span class="hljs-number">.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>/rand(<span class="hljs-number">1</span>, sat)<br>val = rand(<span class="hljs-number">1</span>, val) <span class="hljs-keyword">if</span> rand()&lt;<span class="hljs-number">.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>/rand(<span class="hljs-number">1</span>, val)<br><span class="hljs-comment"># 归一化处理</span><br><span class="hljs-comment"># 内部函数，通过公式转化。具体函数不介绍。</span><br>    x = rgb_to_hsv(np.array(image)/<span class="hljs-number">255.</span>)<br>    x[..., <span class="hljs-number">0</span>] += hue<br>    x[..., <span class="hljs-number">0</span>][x[..., <span class="hljs-number">0</span>]&gt;<span class="hljs-number">1</span>] -= <span class="hljs-number">1</span><br>    x[..., <span class="hljs-number">0</span>][x[..., <span class="hljs-number">0</span>]&lt;<span class="hljs-number">0</span>] += <span class="hljs-number">1</span><br>    x[..., <span class="hljs-number">1</span>] *= sat<br>x[..., <span class="hljs-number">2</span>] *= val<br><span class="hljs-comment"># 避免S/V CHANNEL越界</span><br>    x[x&gt;<span class="hljs-number">1</span>] = <span class="hljs-number">1</span><br>    x[x&lt;<span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>    image_data = hsv_to_rgb(x) <span class="hljs-comment"># numpy array, 0 to 1</span><br><br>    <span class="hljs-comment"># correct boxes</span><br>    box_data = np.zeros((max_boxes,<span class="hljs-number">5</span>))<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(box)&gt;<span class="hljs-number">0</span>:<br>        np.random.shuffle(box)<br>        box[:, [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]] = box[:, [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]]*nw/iw + dx<br>        box[:, [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]] = box[:, [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>]]*nh/ih + dy<br>        <span class="hljs-comment">### 左右翻转</span><br>        <span class="hljs-keyword">if</span> flip: box[:, [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]] = w - box[:, [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]]<br>        <span class="hljs-comment">### 定义边界</span><br>        box[:, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>][box[:, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>]&lt;<span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>        box[:, <span class="hljs-number">2</span>][box[:, <span class="hljs-number">2</span>]&gt;w] = w<br>        box[:, <span class="hljs-number">3</span>][box[:, <span class="hljs-number">3</span>]&gt;h] = h<br>        <span class="hljs-comment">### 计算新的长宽</span><br>        box_w = box[:, <span class="hljs-number">2</span>] - box[:, <span class="hljs-number">0</span>]<br>        box_h = box[:, <span class="hljs-number">3</span>] - box[:, <span class="hljs-number">1</span>]<br>        box = box[np.logical_and(box_w&gt;<span class="hljs-number">1</span>, box_h&gt;<span class="hljs-number">1</span>)] <span class="hljs-comment"># discard invalid box</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(box)&gt;max_boxes: box = box[:max_boxes]<br>        box_data[:<span class="hljs-built_in">len</span>(box)] = box<br>    <span class="hljs-keyword">return</span> image_data, box_data<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Computer Version</category>
      
      <category>Paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO9000: Better, Faster, Stronger</title>
    <link href="/2021/04/29/YOLO9000-Better-Faster-Stronger/"/>
    <url>/2021/04/29/YOLO9000-Better-Faster-Stronger/</url>
    
    <content type="html"><![CDATA[<p>YOLOv2模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv2.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="What’s-YOLOv2"><a href="#What’s-YOLOv2" class="headerlink" title="What’s YOLOv2?"></a>What’s YOLOv2?</h1><p>YOLOv2的论文全名为YOLO9000: Better, Faster, Stronger，它斩获了CVPR 2017 Best Paper Honorable Mention。在这篇文章中，作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。所以，这篇文章其实包含两个模型：YOLOv2和YOLO9000，不过后者是在前者基础上提出的，两者模型主体结构是一致的。YOLOv2相比YOLOv1做了很多方面的改进，这也使得YOLOv2的mAP有显著的提升，并且YOLOv2的速度依然很快，保持着自己作为one-stage方法的优势，YOLOv2和Faster R-CNN, SSD等模型的对比如图1所示。这里将首先介绍YOLOv2的改进策略，并给出YOLOv2的TensorFlow实现过程，然后介绍YOLO9000的训练方法。</p><p><img src="/img/4.29/1.png" alt="YOLOv2与其它模型在VOC 2007数据集上的效果对比"></p><h1 id="YOLOv2的改进策略"><a href="#YOLOv2的改进策略" class="headerlink" title="YOLOv2的改进策略"></a>YOLOv2的改进策略</h1><p>YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图2所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。下面详细介绍各个改进策略。</p><p><img src="/img/4.29/2.jpg" alt="YOLOv2相比YOLOv1的改进策略"></p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p><h2 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h2><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为<script type="math/tex">224 \times 224</script>的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用<script type="math/tex">224 \times 224</script>分类模型预训练后，将分辨率增加至<script type="math/tex">448 \times 448</script>，并使用这个高分辨率在检测数据集上finetune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用<script type="math/tex">448 \times 448</script>来finetune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p><h2 id="Convolutionlal-With-Anchor-Boxes"><a href="#Convolutionlal-With-Anchor-Boxes" class="headerlink" title="Convolutionlal With Anchor Boxes"></a>Convolutionlal With Anchor Boxes</h2><p>在YOLOv1中，输入图片最终被划分为7*7网格，每个单元格预测2个边界框。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比（scales and ratios）的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框（anchor boxes，prior boxes，SSD也采用了先验框）策略。RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体），并且各个位置设置不同尺度和比例的先验框，所以RPN预测的是边界框相对于先验框的offsets值（其实是transform值，详细见Faster R_CNN论文），采用先验框使得模型更容易学习。所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采481*418图片作为输入，而是采用416*416大小。因为YOLOv2模型下采样的总步长为32,对于416*416大小的图片，最终得到的特征图大小为13*13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。所以在YOLOv2设计中要保证最终的特征图有奇数个位置。对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值：<script type="math/tex">(x, y, w, h, c)</script>前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。</p><p>使用anchor boxes之后，YOLOv2的mAP有稍微下降（这里下降的原因，我猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框<script type="math/tex">(7 \times 7 \times 2)</script>，而YOLOv2使用anchor boxes之后可以预测上千个边界框<script type="math/tex">(13 \times 13 \times \text { num_anchors })</script>。所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%。</p><h2 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h2><p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： </p><script type="math/tex; mode=display">d(\text { box, centroid })=1-I O U(\text { box }, \text { centroid })</script><p>下图为在VOC和COCO数据集上的聚类分析结果，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取5个聚类中心作为先验框，其相对于图片的大小如右边图所示。对于两个数据集，5个先验框的width和height如下所示（来源：YOLO源码的cfg文件）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">COCO: (<span class="hljs-number">0.57273</span>, <span class="hljs-number">0.677385</span>), (<span class="hljs-number">1.87446</span>, <span class="hljs-number">2.06253</span>), (<span class="hljs-number">3.33843</span>, <span class="hljs-number">5.47434</span>), (<span class="hljs-number">7.88282</span>, <span class="hljs-number">3.52778</span>), (<span class="hljs-number">9.77052</span>, <span class="hljs-number">9.16828</span>)<br>VOC: (<span class="hljs-number">1.3221</span>, <span class="hljs-number">1.73145</span>), (<span class="hljs-number">3.19275</span>, <span class="hljs-number">4.00944</span>), (<span class="hljs-number">5.05587</span>, <span class="hljs-number">8.09892</span>), (<span class="hljs-number">9.47112</span>, <span class="hljs-number">4.84053</span>), (<span class="hljs-number">11.2364</span>, <span class="hljs-number">10.0071</span>)<br></code></pre></td></tr></table></figure><p><img src="/img/4.29/3.png" alt="数据集VOC和COCO上的边界框聚类分析结果"></p><p>但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（13*13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。</p><h2 id="New-Network：Darknet-19"><a href="#New-Network：Darknet-19" class="headerlink" title="New Network：Darknet-19"></a>New Network：Darknet-19</h2><p>YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3*3卷积，采用2*2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3*3卷积之间使用1*1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p><p><img src="/img/4.29/4.jpg" alt="Darknet-19模型结构"></p><h2 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h2><p>前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置<script type="math/tex">(x,y)</script>，需要根据预测的坐标偏移值<script type="math/tex">(t_{x},t_{y})</script>，先验框的尺度<script type="math/tex">\left(w_{a}, h_{a}\right)</script>以及中心坐标<script type="math/tex">\left(x_{a}, y_{a}\right)</script>（特征图每个位置的中心点）来计算：</p><script type="math/tex; mode=display">x=\left(t_{x} \times w_{a}\right)-x_{a}, y=\left(t_{y} \times h_{a}\right)-y_{a}</script><p>但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当<script type="math/tex">t_{x}=1</script>时边界框将向右偏移先验框的一个宽度大小，而当<script type="math/tex">t_{x}=-1</script>时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets<script type="math/tex">t_{x}, t_{y}, t_{w}, t_{h}</script>，可以按如下公式计算出边界框实际位置和大小：</p><script type="math/tex; mode=display">\begin{array}{c}b_{x}=\sigma\left(t_{x}\right)+c_{x}, b_{y}=\sigma\left(t_{y}\right)+c_{y} \\b_{w}=p_{w} e^{t_{w}}, b_{h}=p_{h} e^{t_{h}}\end{array}</script><p>其中<script type="math/tex">\left(c_{x}, x_{y}\right)</script>为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为<script type="math/tex">(1,1)</script>。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而<script type="math/tex">p_{w}</script>和<script type="math/tex">p_{h}</script>是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为<script type="math/tex">(W, H)</script>（在文中是<script type="math/tex">(13,13)</script>），这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：</p><script type="math/tex; mode=display">\begin{array}{c}b_{x}=\left(\sigma\left(t_{x}\right)+c_{x}\right) / W, b_{y}=\left(\sigma\left(t_{y}\right)+c_{y}\right) / H \\b_{w}=p_{w} e^{t_{w}} / W, b_{h}=p_{h} e^{t_{h}} / H\end{array}</script><p>如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。</p><p><img src="/img/4.29/5.png" alt="边界框位置与大小的计算示例图"></p><h2 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h2><p>YOLOv2的输入图片大小为416*416，经过5次maxpooling之后得到13*13大小的特征图，并以此特征图采用卷积做预测。13*13大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26*26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26*26*512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2*2的局部区域，然后将其转化为channel维度，对于26*26*512的特征图，经passthrough层处理之后就变成了13*13*2048的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的13*13*1024特征图连接在一起形成13*13*3072的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">out = tf.extract_image_patches(<span class="hljs-keyword">in</span>, [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], padding=<span class="hljs-string">&quot;VALID&quot;</span>)<br>// <span class="hljs-keyword">or</span> use tf.space_to_depth<br>out = tf.space_to_depth(<span class="hljs-keyword">in</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="/img/4.29/6.jpg" alt="passthrough层实例"></p><p>另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个1*1卷积核进行卷积，然后再进行passthrough处理，这样26*26*512的特征图得到13*13*256的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。</p><h2 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h2><p>由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416*416大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：<script type="math/tex">\{320,352, \ldots, 608\}</script>输入图片最小为320*320，此时对应的特征图大小为10*10（不是奇数了，确实有点尴尬），而输入图片最大为608*608,对应的特征图大小为19*19,在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。</p><p><img src="/img/4.29/7.jpg" alt="Multi-Scale Training"></p><p>采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544,mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。</p><p><img src="/img/4.29/8.jpg" alt="YOLOv2在VOC 2007数据集上的性能对比"></p><p>总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。</p><h1 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h1><p>YOLOv2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为224*224,共训练160个epochs。然后第二阶段将网络的输入调整为448*448,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个3*3*2014卷积层，同时增加了一个passthrough层，最后使用1*1卷积层输出预测结果，输出的channels数为：<script type="math/tex">\text { num_anchors } \times(5+\text { num_classes })</script>，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为T（shape为<script type="math/tex">(\text { batch_size }, 13,13,125)</script>），可以先将其reshape为<script type="math/tex">(\text { batch_size }, 13,13,5,25)</script>，其中<script type="math/tex">T[:,:,:,:, 0: 4]</script>为边界框的位置和大小，<script type="math/tex">\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$$$$T[:,:,:,:, 4]</script>为边界框的置信度，而<script type="math/tex">T[:,:,:,:, 5:]</script>为类别预测值。</p><p><img src="/img/4.29/9.jpg" alt="YOLOv2训练的三个阶段"></p><p><img src="/img/4.29/10.jpg" alt="YOLOv2结构示意图"></p><p>YOLOv2的网络结构以及训练参数我们都知道了，但是貌似少了点东西。仔细一想，原来作者并没有给出YOLOv2的训练过程的两个最重要方面，即先验框匹配（样本选择）以及训练的损失函数，难怪Ng说YOLO论文很难懂，没有这两方面的说明我们确实不知道YOLOv2到底是怎么训练起来的。不过默认按照YOLOv1的处理方式也是可以处理，我看了YOLO在TensorFlow上的实现darkflow（见yolov2/train.py），发现它就是如此处理的：和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的5个先验框所对应的边界框负责预测它，具体是哪个边界框预测它，需要在训练中确定，即由那个与ground truth的IOU最大的边界框预测它，而剩余的4个边界框不与该ground truth匹配。YOLOv2同样需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的先验框计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的边界框只计算置信度误差（此时target为0）。YOLOv2和YOLOv1的损失函数一样，为均方差函数。但是我看了YOLOv2的源码（训练样本处理与loss计算都包含在文件region_layer.c中，YOLO源码没有任何注释，反正我看了是直摇头），并且参考国外的blog以及allanzelener/YAD2K（Ng深度学习教程所参考的那个Keras实现）上的实现，发现YOLOv2的处理比原来的v1版本更加复杂。先给出loss计算公式：</p><script type="math/tex; mode=display">\begin{array}{r}\operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} \quad 1_{\operatorname{Max} I O U<\text { Thresh }} \lambda_{\text {noobj }} *\left(-b_{i j k}^{o}\right)^{2} \\+1_{t<12800} \lambda_{\text {prior }} * \sum_{r \in(x, y, w, h)}\left(\text { prior }_{k}^{r}-b_{i j k}^{r}\right)^{2} \\+1_{k}^{\text {truth }}\left(\lambda_{\text {coord }} * \sum_{r \in(x, y, w, h)}\left(\text { truth }^{r}-b_{i j k}^{r}\right)^{2}\right. \\+\lambda_{o b j} *\left(I O U_{\text {truth }}^{k}-b_{i j k}^{o}\right)^{2} \\\left.+\lambda_{\text {class }} *\left(\sum_{c=1}^{C}\left(\text { truth }^{c}-b_{i j k}^{c}\right)^{2}\right)\right)\end{array}</script><p>首先<script type="math/tex">W</script>,<script type="math/tex">H</script>分别指的是特征图<script type="math/tex">(13 \times 13)</script>的宽与高，而A指的是先验框数目（这里是5），各个<script type="math/tex">lambda</script>值是各个loss部分的权重系数。第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差。第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，在YOLOv1中target=1，而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的和误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale <em> (2 - truth.w</em>truth.h)，这样对于尺度较小的boxes其权重系数会更大一些，起到和YOLOv1计算平方根相似的效果（参考YOLO v2 损失函数源码分析）。</p><p>最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。</p><h1 id="YOLOv2在TensorFlow上实现"><a href="#YOLOv2在TensorFlow上实现" class="headerlink" title="YOLOv2在TensorFlow上实现"></a>YOLOv2在TensorFlow上实现</h1><p>这里参考YOLOv2在Keras上的复现（见yhcc/yolo2）,使用TensorFlow实现YOLOv2在COCO数据集上的test过程。首先是定义YOLOv2的主体网络结构Darknet-19：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet</span>(<span class="hljs-params">images, n_last_channels=<span class="hljs-number">425</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Darknet19 for YOLOv2&quot;&quot;&quot;</span><br>    net = conv2d(images, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv1&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv2&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_5&quot;</span>)<br>    shortcut = net<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool5&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_5&quot;</span>)<br>    <span class="hljs-comment"># ---------</span><br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_2&quot;</span>)<br>    <span class="hljs-comment"># shortcut</span><br>    shortcut = conv2d(shortcut, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv_shortcut&quot;</span>)<br>    shortcut = reorg(shortcut, <span class="hljs-number">2</span>)<br>    net = tf.concat([shortcut, net], axis=-<span class="hljs-number">1</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv8&quot;</span>)<br>    <span class="hljs-comment"># detection layer</span><br>    net = conv2d(net, n_last_channels, <span class="hljs-number">1</span>, batch_normalize=<span class="hljs-number">0</span>,<br>                 activation=<span class="hljs-literal">None</span>, use_bias=<span class="hljs-literal">True</span>, name=<span class="hljs-string">&quot;conv_dec&quot;</span>)<br>    <span class="hljs-keyword">return</span> net<br></code></pre></td></tr></table></figure><p>然后实现对Darknet-19模型输出的解码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decode</span>(<span class="hljs-params">detection_feat, feat_sizes=(<span class="hljs-params"><span class="hljs-number">13</span>, <span class="hljs-number">13</span></span>), num_classes=<span class="hljs-number">80</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">           anchors=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;decode from the detection feature&quot;&quot;&quot;</span><br>    H, W = feat_sizes<br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    detetion_results = tf.reshape(detection_feat, [-<span class="hljs-number">1</span>, H * W, num_anchors,<br>                                        num_classes + <span class="hljs-number">5</span>])<br><br>    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>])<br>    bbox_wh = tf.exp(detetion_results[:, :, :, <span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br>    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">4</span>])<br>    class_probs = tf.nn.softmax(detetion_results[:, :, :, <span class="hljs-number">5</span>:])<br><br>    anchors = tf.constant(anchors, dtype=tf.float32)<br><br>    height_ind = tf.<span class="hljs-built_in">range</span>(H, dtype=tf.float32)<br>    width_ind = tf.<span class="hljs-built_in">range</span>(W, dtype=tf.float32)<br>    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)<br>    x_offset = tf.reshape(x_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    y_offset = tf.reshape(y_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br>    <span class="hljs-comment"># decode</span><br>    bbox_x = (bbox_xy[:, :, :, <span class="hljs-number">0</span>] + x_offset) / W<br>    bbox_y = (bbox_xy[:, :, :, <span class="hljs-number">1</span>] + y_offset) / H<br>    bbox_w = bbox_wh[:, :, :, <span class="hljs-number">0</span>] * anchors[:, <span class="hljs-number">0</span>] / W * <span class="hljs-number">0.5</span><br>    bbox_h = bbox_wh[:, :, :, <span class="hljs-number">1</span>] * anchors[:, <span class="hljs-number">1</span>] / H * <span class="hljs-number">0.5</span><br><br>    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,<br>                       bbox_x + bbox_w, bbox_y + bbox_h], axis=<span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">return</span> bboxes, obj_probs, class_probs<br></code></pre></td></tr></table></figure><h1 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h1><p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p><p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p><p><img src="/img/4.29/11.jpg" alt="基于COCO和ImageNet数据集建立的WordTree"></p><p>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。</p><p><img src="/img/4.29/12.jpg" alt="ImageNet与WordTree预测的对比">在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是<script type="math/tex">Pr(physicalobject)</script>，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p><p>通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。我觉得这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。</p>]]></content>
    
    
    <categories>
      
      <category>Computer Version</category>
      
      <category>Paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>You Only Look Once: Unified,Real-Time Object Detection</title>
    <link href="/2021/04/27/You-Only-Look-Once-Unified-Real-Time-Object-Detection/"/>
    <url>/2021/04/27/You-Only-Look-Once-Unified-Real-Time-Object-Detection/</url>
    
    <content type="html"><![CDATA[<p>YOLOv1模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv1.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="what’s-YOLO"><a href="#what’s-YOLO" class="headerlink" title="what’s YOLO"></a>what’s YOLO</h1><p>根据<a href="https://pjreddie.com/darknet/yolo/">YOLO官网</a>对它的解释，YOLO：Real-Time Object Detection. You Only Look Once(YOLO)是一个最先进的实时的目标检测系统。在Pascal Titan X上面处理图像能够达到30FPS，在COCO test-dev上具有57.9%的<a href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/">mAP</a>。</p><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p><img src="/img/4.28/1.png" alt="目标检测"></p><p>2015年之前目标检测方法在Pascal VOC 2007数据集上测试的FPS和mAP结果对比,传统的目标检测方法大致分为三个步骤，先使用不同的方法(滑动窗口，区域候选)提取区域的特征图，然后再使用分类器进行识别，最后回归预测。大多数方法都较为复杂，速度较慢，训练耗时。</p><p>传统的方法可以按照检测系统分为两种：</p><ol><li>DPM，Deformatable Parts Models，采用sliding window检测</li><li>R-CNN、Fast R-CNN。采用region proposal的方法，生成一些可能包含待检测物体的potential bounding box，再通过一个classifier(SVM)判断每个bbox里是否真的包含物体，以及物体的class probability。</li></ol><p>目前深度学习相关的目标检测方法大致可以分为两派：</p><ol><li>基于区域提名的(regin proposal)的，比如R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、R-FCN。</li><li>基于端到端(end to end)的，无需候选区域，如YOLO、SSD。<br>二者发展都很迅速，区域提名准确率较好、端到端的方法速度较快。</li></ol><h2 id="YOLO-核心思想"><a href="#YOLO-核心思想" class="headerlink" title="YOLO 核心思想"></a>YOLO 核心思想</h2><ul><li>将整张图片作为网络的输入，直接在输出层对bounding box的位置和所属类别进行回归。与Faster R-CNN网络相比，虽然后者也是使用整张图片作为输入，但是它采用了RCNN那种区域预测+分类的思想，把提取proposal的步骤放在了CNN中实现，而YOLO则采用直接回归的思路，将目标定位和目标类别预测整合于在单个神经网络模型中。</li></ul><p><img src="/img/4.28/2.png" alt="YOLO检测系统"></p><ul><li>直接在输出层回归bbox的位置和所属类别。</li></ul><p>YOLO检测系统简单直接，可以看做只有三步：</p><ol><li>YOLO检测系统先将输入图像调整到448×448；</li><li>在图像上运行卷积网络；</li><li>通过模型的置信度对结果进行阈值。</li></ol><h3 id="YOLO实现细节"><a href="#YOLO实现细节" class="headerlink" title="YOLO实现细节"></a>YOLO实现细节</h3><ol><li>将一幅图像分成<script type="math/tex">S \times S</script>个网格(Grid Cell)，如果某个object的中心落在某个网格中(通过ground-truth框确定)，则这个网格就负责预测这个object。</li><li>每个网格要预测B个bounding box，每个box除了要回归自身的位置之外，还要附带预测一个confidence值。这个值代表了所预测的bounding box中是否含有object和若有object，这个object预测得有多准的两重信息，计算方式：</li></ol><script type="math/tex; mode=display">\operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><p>如果有object的中心落在一个网格里面，*的前第一项取1，否则取0。第二项是预测的边界框和ground-truth之间的<a href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/">IoU</a>值。</p><ol><li>每个网格单元针对20种类别预测bboxes属于单个类别的条件概率<script type="math/tex">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right)</script>,属于同一个网格的B个bboxes共享一个条件概率。在测试时，将条件概率分别和单个的bbox的confidence预测相乘：</li></ol><script type="math/tex; mode=display">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><ol><li>在Pascal VOC中，YOLO检测系统的图像输入为448×448，S=7，B=2，一共有20个class(C=20)，输出就是7×7×30的一个tensor。这个是怎么算出来的呢？看下面详解。</li></ol><h2 id="YOLO-网络设计"><a href="#YOLO-网络设计" class="headerlink" title="YOLO 网络设计"></a>YOLO 网络设计</h2><p><img src="/img/4.28/3.png" alt="YOLO网络结构"></p><p>YOLO使用了24个级联卷积层和最后2个全连接层，交替的1×1卷积层降低了前面层的特征空间。在ImageNet分类任务上使用分辨率的一半(224×224输入图像)对卷积层进行预训练，然后将分辨率加倍进行目标检测。</p><p><img src="/img/4.28/4.png" alt="YOLO网络概览图"></p><p>YOLO网络借鉴了GoogleNet的思想，但与之不同的是，为了更好的性能，它增加额外的4层卷积层(conv)。YOLO一共使用了24个级联的卷积层和2个全连接层(fc)，其中conv层中包含了1×1和3×3两种kernel，最后一个fc全连接层后经过reshape之后就是YOLO网络的输出，是长度为<script type="math/tex">S \times S \times (B \times 5 + C ) = 7 \times 7 \times 30</script>的tensor，最后经过识别过程得到最终的检测结果。</p><p><img src="/img/4.28/5.png" alt="1x1x30 tensor解释"></p><p>上文说到每个bounding box要预测(x,y,w,h,confidence)五个值，一张图片共分为<script type="math/tex">S \times S</script>个网格，每个网格要预测出B个bounding box和一个网格负责的object的类别信息，记为<script type="math/tex">C</script>。</p><p>则输出为<script type="math/tex">S \times S \times ( 5 \times B + C )</script>的tensor张量，<script type="math/tex">(x,y)</script>表示bounding box相对于网格单元的边界的offset，归一化到<script type="math/tex">(0,1)</script>范围之内，而<script type="math/tex">w</script>,<script type="math/tex">h</script>表示相对于整个图片的预测宽和高，也被归一化到<script type="math/tex">(0,1)</script>范围内，<script type="math/tex">c</script>代表的是object在某个bounding box的confidence。</p><p>使用下图更形象的说明，7×7×30的Tensor中的一个1×1×30的前10维的所代表的含义。</p><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>下面解释如何将预测坐标的x,y用相对于对应网格的offset归一化到0-1和w,h是如何利用图像的宽高归一化到0-1之间。</p><p>每个单元格预测的B个<script type="math/tex">(x,y,w,h,confidence)</script>向量，假设图片为<script type="math/tex">S \times S</script>个网格，S=7，图片宽为<script type="math/tex">w_{i}</script>高为<script type="math/tex">h_{i}</script>：</p><ol><li><script type="math/tex">(x,y)</script>是bbox的中心相对于单元格的offset对于下图中蓝色单元格，坐标为<script type="math/tex">\left(x_{col}=1, y_{row} \text =4\right)</script>，假设它的预测输出是红色框bbox，设bbox的中心坐标为那么最终预测出来的<script type="math/tex">(x,y)</script>是经过归一化处理的，表示的是相对于单元格的offset，公式：</li></ol><script type="math/tex; mode=display">x=\frac{x_{c}}{w_{i}} * S-x_{\text {col }}, \quad y=\frac{y_{c}}{h_{i}} * S-y_{\text {row }}</script><p><img src="/img/4.28/6.png" alt="归一化"></p><ol><li><script type="math/tex">(w,h)</script>是bbox相对于整个图片的比例预测的bbox的宽高为<script type="math/tex">w_{b}</script>, <script type="math/tex">h_{b}</script>，<script type="math/tex">(w,h)</script>表示的是bbox相对于整张图片的占比，公式：</li></ol><script type="math/tex; mode=display">w=\frac{w_{b}}{w_{i}}, h=\frac{h_{b}}{h_{i}}</script><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p><img src="/img/4.28/7.png" alt="1x1x30 前10维解释"></p><p>每个grid cell对应的两个bounding box的5个值在tensor中体现出来，组成前十维。每个单元格负责预测一个object，但是每个单元格预测出2个bounding box，如图。</p><p><img src="/img/4.28/8.png" alt="1x1x30 Tensor的30维解释"></p><p>每个grid cell有30维，其中8维是两个预测回归bboxes的坐标信息，2维是bboxes的confidences，还有20维是与类别相关的信息。</p><p><img src="/img/4.28/9.png" alt="20维中每个值表示此object在此bbox中的时候属于class_i的概率分数"></p><p><img src="/img/4.28/10.png" alt="类别预测分数的计算"></p><p><img src="/img/4.28/11.png" alt="类别预测分数的计算"></p><p>每个单元格预测一个属于类别<script type="math/tex">class_{i}</script>的条件概率$\operatorname{Pr}\left(\right.$ Class $_{i} \mid$ Object $)$。要注意的是，属于一个网格的2个bboxs共享一套条件概率值，因为这两个box都是为了一个grid cell服务，最终预测出一个object类别。在测试时，将条件概率和单个bbox的confidence预测相乘：</p><script type="math/tex; mode=display">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><p>如上式所述，每个网格预测的类别概率乘以每个bbox的预测confidence，得到每个bbox的class-specific confidence score分数。对每个格子的每一个bounding box进行此运算，最后会得到7×7×2=98个scores，设置一个阈值，滤掉得分低的bboxes，对保留的bboxes进行NMS(Non Maximum Suppression)处理，最终得到目标检测结果。</p><p><img src="/img/4.28/12.png" alt="图像中第一个网格中两个bboxes经过计算后的结果"></p><p><img src="/img/4.28/13.png" alt="依次计算最终得到98个附带有score的tensors"></p><p><img src="/img/4.28/14.png" alt="依次计算最终得到98个附带有score的tensors"></p><h2 id="预测框的定位"><a href="#预测框的定位" class="headerlink" title="预测框的定位"></a>预测框的定位</h2><p><img src="/img/4.28/15.png" alt="得到7×7×30的张量之后，经过检测过程处理的到定位框"></p><p><img src="/img/4.28/16.png" alt="假设得到的每个20x1的tensor的第一维为预测为dog的score"></p><p><img src="/img/4.28/17.png" alt="通过设定阈值过滤掉score低的冗余bboxes"></p><p><img src="/img/4.28/18.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/19.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/20.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/21.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/22.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/23.png" alt="根据得到的98个bboxes中对dog的预测分数经过阈值后进行从大到小排序"></p><p>最后得到第一个为最大的score值，找出针对dog这个种类预测出的对应框，记为bbox_max。然后将它与其他分数较低的但不是0的框作对比，这种框记为bbox_cur。将bbox_max和bbox_cur分别做IoU计算，如果 IoU(bbox_max, bbox_cur) &gt; 0.5，那么将bbox_cur对应的score设为0。例如：</p><p><img src="/img/4.28/24.png" alt="IoU &gt; 0.5的分数较小的框的score归零"></p><p>然后接着遍历下一个score，如果它不是最大的且不为0，就和最大的score对应的框座IoU运算，若结果大于0.5则，同上。否则它的score不变，继续处理下一个bbox_cur……直到最后一个score，如图：</p><p><img src="/img/4.28/25.png" alt="IoU &lt; 0.5， 继续下一个score与max score对应框做IOU计算"></p><p>计算完一轮之后，假如得到score序列：0.5、0、0.2、0.1 … 0、0、0、0<br>那么进行下一轮循环，从0.2开始，将0.2对应的框作为bbox_max，继续循环计算后面的bbox_cur与新的bbox_max的IoU值，大于0.5的设为0，小于0.5的score不变。再这样一直计算比较到最后一个score。</p><p>得到新的的score序列为：0.5、0、0.2、0 … 0、0、0、0<br>即最后只得到两个score不为0的框，如图：</p><p><img src="/img/4.28/26.png" alt="多次循环完毕的结果"></p><p><img src="/img/4.28/27.png" alt="98个框对下一类别cat的score的NMS处理"></p><p><img src="/img/4.28/28.png" alt="对98个框的针对其他类别预测的score依次进行NMS处理"></p><h2 id="再筛选bounding-box"><a href="#再筛选bounding-box" class="headerlink" title="再筛选bounding box"></a>再筛选bounding box</h2><p>经过这些NMS算法的处理，会出现很多框针对某个class的预测的score为0的情况。<br>最后，针对每个bbox的20×1的张量，对20种class的预测score进行判断。例如，</p><ol><li>先取出针对bbox3的所有20个scores，按照类别的默认顺序找出score最大的那个score的index索引号(根据此index可以找出所属的类别)记为class；</li><li>然后找出bbox3的最大score分数，记为score。</li><li>判断score是否大于0，如果是，就在图像中画出标有class的框。否则，丢弃此bbox。<br>如图：</li></ol><p><img src="/img/4.28/29.png" alt="筛选bbox的过程示例"></p><p><img src="/img/4.28/30.png" alt="判断每个bbox是否能在图像中最后保留"></p><p><img src="/img/4.28/31.png" alt="遍历完毕，得到带有预测框的图像"></p><h2 id="YOLO-Key-Points"><a href="#YOLO-Key-Points" class="headerlink" title="YOLO Key Points"></a>YOLO Key Points</h2><p><img src="/img/4.28/32.png" alt="YOLO Key Points"></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="损失函数解析"><a href="#损失函数解析" class="headerlink" title="损失函数解析"></a>损失函数解析</h2><p>论文中损失函数的形式，一共分为四个部分:</p><p><img src="/img/4.28/33.png" alt="Loss Function"></p><p>在此损失函数中，只有某个网格中有object的中心落入的时候才对classification loss进行惩罚。只有当某个网格i中的bbox对某的ground-truth box负责的时候，才会对box的coordinate error进行惩罚。哪个bbox对ground-truth box负责就看ground-truth box和bbox的IoU最大。</p><p>Loss相关解释：</p><ol><li>8维的localization error和20维的classification error同样重要显然是不合理的，应该更重视8维的坐标预测，所以作者给这个损失前加了个权重，记为：<script type="math/tex">\lambda_{\text {coord }}</script>,在Pascal VOC训练中取5。</li><li>对于不存在object的bbox的confidence loss，赋予了更小的损失权重，记为<script type="math/tex">\lambda_{\text {noobj }}</script>，在Pascal VOC中取0.5。若没有任何物体中心落入边界框中，则<script type="math/tex">\widehat{C}_{i}</script>为0，此时我们希望预测含有物体的置信度<script type="math/tex">C_{i}</script>越小越好。然而，大部分bbox中都没有object，积少成多，造成loss的第2部分与第3部分的不平衡，因此，在loss的三部分增加权重<script type="math/tex">\lambda_{\text {noobj}}=0.5</script>。</li><li>对于存在object的bbox的confidence loss和类别的loss，权重取常数1。对于每个格子而言，作者设计只能包含同种物体。若格子中包含物体，我们希望希望预测正确的类别的概率越接近于1越好，而错误类别的概率越接近于0越好。loss第4部分中，若<script type="math/tex">\hat{p}_{i}(c)</script>为0中<script type="math/tex">c</script>为正确类别，则值为1，若非正确类别，则值为0。</li><li>对于第二行中为什么对<script type="math/tex">w_{i}</script>和<script type="math/tex">h_{i}</script>取平方根处理，原因是：对于一个图像中存在的不同大小的object，小物体的预测框偏离一点和大的相比肯定更严重，而sum-square error loss中对大小object的偏移loss是一同处理的。为了缓和，作者使用开方的方法，将bbox的width和height去平方根代替width和height。我们可以这样理解，假如预测出的小物体<script type="math/tex">(width=10, height=10)</script>的宽比实际大了10pixel，大物体<script type="math/tex">(width=100,height=100)</script>的宽度比实际预测大了10pixel。显然对于小物体，预测大了一倍，我们接受不了，而对大物体来说就影响不大。所以我们就要加大惩罚措施，让小物体预测大了10pixel的后果对于损失函数的值贡献更大，我们通过取平方根<script type="math/tex">\sqrt{20}-\sqrt{10} \approx 3.4>\sqrt{110}-\sqrt{100}\approx 0.5</script>，这样也算达到了目的吧。</li></ol><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="与其他检测方法对比"><a href="#与其他检测方法对比" class="headerlink" title="与其他检测方法对比"></a>与其他检测方法对比</h2><p>详见论文。</p><h3 id="在Pascal-VOC-2007上的测试结果"><a href="#在Pascal-VOC-2007上的测试结果" class="headerlink" title="在Pascal VOC 2007上的测试结果"></a>在Pascal VOC 2007上的测试结果</h3><p><img src="/img/4.28/34.png" alt="在Pascal VOC 2007 上与其他检测方法的对比"></p><p>Fast YOLO在Pascal数据集上速度最快，达到155FPS，而UOLO的mAP是最高的。</p><h3 id="VOC-2007错误分析"><a href="#VOC-2007错误分析" class="headerlink" title="VOC 2007错误分析"></a>VOC 2007错误分析</h3><p>比较了YOLO和Faster R-CNN的错误情况，结果如图所示。</p><p>预测结果包括以下几类：<br>正确：类别正确，IOU&gt;0.5<br>定位：类别正确，0.1<IOU<0.5类似：类别相似，IOU>0.1<br>其它：类别错误，IOU&gt;0.1<br>背景：任何物体，IOU&lt;0.1</p><p><img src="/img/4.28/35.png" alt="YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO"></p><h2 id="组合Fast-R-CNN-和-YOLO"><a href="#组合Fast-R-CNN-和-YOLO" class="headerlink" title="组合Fast R-CNN 和 YOLO"></a>组合Fast R-CNN 和 YOLO</h2><p><img src="/img/4.28/36.png" alt="模型组合在VOC 2007上的实验结果对比"></p><h2 id="在Pascal-VOC-2012上测试结果"><a href="#在Pascal-VOC-2012上测试结果" class="headerlink" title="在Pascal VOC 2012上测试结果"></a>在Pascal VOC 2012上测试结果</h2><p><img src="/img/4.28/37.png" alt="mAP排序"></p><h2 id="通用性"><a href="#通用性" class="headerlink" title="通用性"></a>通用性</h2><p><img src="/img/4.28/38.png" alt="通用性(Picasso 数据集和 People-Art数据集)YOLO都具有很好的检测结果"></p><h1 id="merits-and-drawbacks"><a href="#merits-and-drawbacks" class="headerlink" title="merits and drawbacks"></a>merits and drawbacks</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>假阳性(FP)错误率低。YOLO网络将整个图像的全部信息作为上下文，在训练的过程中使用到了全图信息，能够更好的区分目标个背景区域。</li><li>端到端(end-to-end)的方法，速度快，如官网所说，具有实时性，45fps，在YOLO-tiny上可以达到155fps。</li><li>通用性强，可以学到物体的generalizable-representation。对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于DPM和RCNN系列检测方法。</li></ol><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ol><li>性能表现比当前最先进的目标检测技术低</li><li>定位准确性差，对于小目标或者密集型群体object的定位效果不好。虽然每个格子可以预测出B个bounding box，但是最终只选择一个score最高的作为输出，即每个格子最多只得出一个box，如果多个物体的中心落在同一个格子，那么最终可能只预测出一个object。</li><li>YOLO的loss函数中，大物体的IoU误差和小物体的IoU的误差对训练中的loss贡献值接近，造成定位不准确。作者采用了一个去平方根的方式，但还不是最好的解决方法。</li><li>输入尺寸固定。由于输出层为全连接层，因此在检测时，YOLO训练模型只支持与训练图像相同的输入分辨率。其他分辨率需要缩放成此固定分辨率大小。</li><li>采用了多个下采样层，网络学到的物体特征并不精细，因此对检测效果会有影响。</li></ol><h1 id="代码-yolo-small即dark-net19"><a href="#代码-yolo-small即dark-net19" class="headerlink" title="代码(yolo-small即dark-net19)"></a>代码(yolo-small即dark-net19)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br> <br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> cv2<br> <br> <br><span class="hljs-comment"># leaky_relu激活函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">0.1</span></span>):</span><br>    <span class="hljs-keyword">return</span> tf.maximum(alpha * x, x)<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Yolo</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, weights_file, input_image, verbose=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-comment"># 后面程序打印描述功能的标志位</span><br>        self.verbose = verbose<br> <br>        <span class="hljs-comment"># 检测超参数</span><br>        self.S = <span class="hljs-number">7</span>  <span class="hljs-comment"># cell数量</span><br>        self.B = <span class="hljs-number">2</span>  <span class="hljs-comment"># 每个网格的边界框数</span><br>        self.classes = [<span class="hljs-string">&quot;aeroplane&quot;</span>, <span class="hljs-string">&quot;bicycle&quot;</span>, <span class="hljs-string">&quot;bird&quot;</span>, <span class="hljs-string">&quot;boat&quot;</span>, <span class="hljs-string">&quot;bottle&quot;</span>,<br>                        <span class="hljs-string">&quot;bus&quot;</span>, <span class="hljs-string">&quot;car&quot;</span>, <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;chair&quot;</span>, <span class="hljs-string">&quot;cow&quot;</span>, <span class="hljs-string">&quot;diningtable&quot;</span>,<br>                        <span class="hljs-string">&quot;dog&quot;</span>, <span class="hljs-string">&quot;horse&quot;</span>, <span class="hljs-string">&quot;motorbike&quot;</span>, <span class="hljs-string">&quot;person&quot;</span>, <span class="hljs-string">&quot;pottedplant&quot;</span>,<br>                        <span class="hljs-string">&quot;sheep&quot;</span>, <span class="hljs-string">&quot;sofa&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;tvmonitor&quot;</span>]<br>        self.C = <span class="hljs-built_in">len</span>(self.classes)  <span class="hljs-comment"># 类别数</span><br> <br>        self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)] * self.S * self.B),<br>                                                [self.B, self.S, self.S]), [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>])<br>        self.y_offset = np.transpose(self.x_offset, [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>])  <span class="hljs-comment"># 改变数组的shape</span><br> <br>        self.threshold = <span class="hljs-number">0.2</span>  <span class="hljs-comment"># 类别置信度分数阈值</span><br>        self.iou_threshold = <span class="hljs-number">0.4</span>  <span class="hljs-comment"># IOU阈值，小于0.4的会过滤掉</span><br> <br>        self.max_output_size = <span class="hljs-number">10</span>  <span class="hljs-comment"># NMS选择的边界框的最大数量</span><br> <br>        self.sess = tf.Session()<br>        self._build_net()  <span class="hljs-comment"># 【1】搭建网络模型(预测):模型的主体网络部分，这个网络将输出[batch,7*7*30]的张量</span><br>        self._build_detector()  <span class="hljs-comment"># 【2】解析网络的预测结果：先判断预测框类别，再NMS</span><br>        self._load_weights(weights_file)  <span class="hljs-comment"># 【3】导入权重文件</span><br>        self.detect_from_file(image_file=input_image)  <span class="hljs-comment"># 【4】从预测输入图片，并可视化检测边界框、将obj的分类结果和坐标保存成txt。</span><br> <br>    <span class="hljs-comment"># 【1】搭建网络模型(预测):模型的主体网络部分，这个网络将输出[batch,7*7*30]的张量</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_build_net</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 打印状态信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start to build the network ...&quot;</span>)<br> <br>        <span class="hljs-comment"># 输入、输出用占位符，因为尺寸一般不会改变</span><br>        self.images = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment"># None表示不确定，为了自适应batchsize</span><br> <br>        <span class="hljs-comment"># 搭建网络模型</span><br>        net = self._conv_layer(self.images, <span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">2</span>, <span class="hljs-number">192</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">3</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">4</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">5</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">6</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">7</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">8</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">9</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">10</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">11</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">12</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">13</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">14</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">15</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">16</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">16</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">17</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">18</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">19</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">20</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">21</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">22</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">23</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">24</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._flatten(net)<br>        net = self._fc_layer(net, <span class="hljs-number">25</span>, <span class="hljs-number">512</span>, activation=leaky_relu)<br>        net = self._fc_layer(net, <span class="hljs-number">26</span>, <span class="hljs-number">4096</span>, activation=leaky_relu)<br>        net = self._fc_layer(net, <span class="hljs-number">27</span>, self.S * self.S * (self.B * <span class="hljs-number">5</span> + self.C))<br> <br>        <span class="hljs-comment"># 网络输出，[batch,7*7*30]的张量</span><br>        self.predicts = net<br> <br>    <span class="hljs-comment"># 【2】解析网络的预测结果：先判断预测框类别，再NMS</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_build_detector</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 原始图片的宽和高</span><br>        self.width = tf.placeholder(tf.float32, name=<span class="hljs-string">&#x27;img_w&#x27;</span>)<br>        self.height = tf.placeholder(tf.float32, name=<span class="hljs-string">&#x27;img_h&#x27;</span>)<br> <br>        <span class="hljs-comment"># 网络回归[batch,7*7*30]：</span><br>        idx1 = self.S * self.S * self.C<br>        idx2 = idx1 + self.S * self.S * self.B<br>        <span class="hljs-comment"># 1.类别概率[:,:7*7*20]  20维</span><br>        class_probs = tf.reshape(self.predicts[<span class="hljs-number">0</span>, :idx1], [self.S, self.S, self.C])<br>        <span class="hljs-comment"># 2.置信度[:,7*7*20:7*7*(20+2)]  2维</span><br>        confs = tf.reshape(self.predicts[<span class="hljs-number">0</span>, idx1:idx2], [self.S, self.S, self.B])<br>        <span class="hljs-comment"># 3.边界框[:,7*7*(20+2):]  8维 -&gt; (x,y,w,h)</span><br>        boxes = tf.reshape(self.predicts[<span class="hljs-number">0</span>, idx2:], [self.S, self.S, self.B, <span class="hljs-number">4</span>])<br> <br>        <span class="hljs-comment"># 将x，y转换为相对于图像左上角的坐标</span><br>        <span class="hljs-comment"># w，h的预测是平方根乘以图像的宽度和高度</span><br>        boxes = tf.stack([(boxes[:, :, :, <span class="hljs-number">0</span>] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,<br>                          (boxes[:, :, :, <span class="hljs-number">1</span>] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height,<br>                          tf.square(boxes[:, :, :, <span class="hljs-number">2</span>]) * self.width,<br>                          tf.square(boxes[:, :, :, <span class="hljs-number">3</span>]) * self.height], axis=<span class="hljs-number">3</span>)<br> <br>        <span class="hljs-comment"># 类别置信度分数：[S,S,B,1]*[S,S,1,C]=[S,S,B,类别置信度C]</span><br>        scores = tf.expand_dims(confs, -<span class="hljs-number">1</span>) * tf.expand_dims(class_probs, <span class="hljs-number">2</span>)<br> <br>        scores = tf.reshape(scores, [-<span class="hljs-number">1</span>, self.C])  <span class="hljs-comment"># [S*S*B, C]</span><br>        boxes = tf.reshape(boxes, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])  <span class="hljs-comment"># [S*S*B, 4]</span><br> <br>        <span class="hljs-comment"># 只选择类别置信度最大的值作为box的类别、分数</span><br>        box_classes = tf.argmax(scores, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 边界框box的类别</span><br>        box_class_scores = tf.reduce_max(scores, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 边界框box的分数</span><br> <br>        <span class="hljs-comment"># 利用类别置信度阈值self.threshold，过滤掉类别置信度低的</span><br>        filter_mask = box_class_scores &gt;= self.threshold<br>        scores = tf.boolean_mask(box_class_scores, filter_mask)<br>        boxes = tf.boolean_mask(boxes, filter_mask)<br>        box_classes = tf.boolean_mask(box_classes, filter_mask)<br> <br>        <span class="hljs-comment"># NMS (不区分不同的类别)</span><br>        <span class="hljs-comment"># 中心坐标+宽高box (x, y, w, h) -&gt; xmin=x-w/2 -&gt; 左上+右下box (xmin, ymin, xmax, ymax)，因为NMS函数是这种计算方式</span><br>        _boxes = tf.stack([boxes[:, <span class="hljs-number">0</span>] - <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">1</span>] - <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">3</span>],<br>                           boxes[:, <span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>)<br>        nms_indices = tf.image.non_max_suppression(_boxes, scores,<br>                                                   self.max_output_size, self.iou_threshold)<br>        self.scores = tf.gather(scores, nms_indices)<br>        self.boxes = tf.gather(boxes, nms_indices)<br>        self.box_classes = tf.gather(box_classes, nms_indices)<br> <br>    <span class="hljs-comment"># 【3】导入权重文件</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_load_weights</span>(<span class="hljs-params">self, weights_file</span>):</span><br>        <span class="hljs-comment"># 打印状态信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start to load weights from file:%s&quot;</span> % (weights_file))<br> <br>        <span class="hljs-comment"># 导入权重</span><br>        saver = tf.train.Saver()  <span class="hljs-comment"># 初始化</span><br>        saver.restore(self.sess, weights_file)  <span class="hljs-comment"># saver.restore导入/saver.save保存</span><br> <br>    <span class="hljs-comment"># 【4】从预测输入图片，并可视化检测边界框、将obj的分类结果和坐标保存成txt。</span><br>    <span class="hljs-comment"># image_file是输入图片文件路径；</span><br>    <span class="hljs-comment"># deteted_boxes_file=&quot;boxes.txt&quot;是最后坐标txt；detected_image_file=&quot;detected_image.jpg&quot;是检测结果可视化图片</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">detect_from_file</span>(<span class="hljs-params">self, image_file, imshow=<span class="hljs-literal">True</span>, deteted_boxes_file=<span class="hljs-string">&quot;boxes.txt&quot;</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                         detected_image_file=<span class="hljs-string">&quot;detected_image.jpg&quot;</span></span>):</span><br>        <span class="hljs-comment"># read image</span><br>        image = cv2.imread(image_file)<br>        img_h, img_w, _ = image.shape<br>        scores, boxes, box_classes = self._detect_from_image(image)<br>        predict_boxes = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(scores)):<br>            <span class="hljs-comment"># 预测框数据为：[概率,x,y,w,h,类别置信度]</span><br>            predict_boxes.append((self.classes[box_classes[i]], boxes[i, <span class="hljs-number">0</span>],<br>                                  boxes[i, <span class="hljs-number">1</span>], boxes[i, <span class="hljs-number">2</span>], boxes[i, <span class="hljs-number">3</span>], scores[i]))<br>        self.show_results(image, predict_boxes, imshow, deteted_boxes_file, detected_image_file)<br> <br>    <span class="hljs-comment">################# 对应【1】:定义conv/maxpool/flatten/fc层#############################################################</span><br>    <span class="hljs-comment"># 卷积层：x输入；id：层数索引；num_filters：卷积核个数；filter_size：卷积核尺寸；stride：步长</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_conv_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, num_filters, filter_size, stride</span>):</span><br> <br>        <span class="hljs-comment"># 通道数</span><br>        in_channels = x.get_shape().as_list()[-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># 均值为0标准差为0.1的正态分布，初始化权重w；shape=行*列*通道数*卷积核个数</span><br>        weight = tf.Variable(<br>            tf.truncated_normal([filter_size, filter_size, in_channels, num_filters], mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">0.1</span>))<br>        bias = tf.Variable(tf.zeros([num_filters, ]))  <span class="hljs-comment"># 列向量</span><br> <br>        <span class="hljs-comment"># padding, 注意: 不用padding=&quot;SAME&quot;,否则可能会导致坐标计算错误</span><br>        pad_size = filter_size // <span class="hljs-number">2</span>  <span class="hljs-comment"># 除法运算，保留商的整数部分</span><br>        pad_mat = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [pad_size, pad_size], [pad_size, pad_size], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>        x_pad = tf.pad(x, pad_mat)<br>        conv = tf.nn.conv2d(x_pad, weight, strides=[<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&quot;VALID&quot;</span>)<br>        output = leaky_relu(tf.nn.bias_add(conv, bias))<br> <br>        <span class="hljs-comment"># 打印该层信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=conv,num_filter=%d,filter_size=%d,stride=%d,output_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, num_filters, filter_size, stride, <span class="hljs-built_in">str</span>(output.get_shape())))<br> <br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment"># 池化层：x输入；id：层数索引；pool_size：池化尺寸；stride：步长</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_maxpool_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, pool_size, stride</span>):</span><br>        output = tf.layers.max_pooling2d(inputs=x,<br>                                         pool_size=pool_size,<br>                                         strides=stride,<br>                                         padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)<br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=MaxPool,pool_size=%d,stride=%d,out_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, pool_size, stride, <span class="hljs-built_in">str</span>(output.get_shape())))<br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment"># 扁平层：因为接下来会连接全连接层，例如[n_samples, 7, 7, 32] -&gt; [n_samples, 7*7*32]</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_flatten</span>(<span class="hljs-params">self, x</span>):</span><br>        tran_x = tf.transpose(x, [<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  <span class="hljs-comment"># [batch,行,列,通道数channels] -&gt; [batch,通道数channels,列,行]</span><br>        nums = np.product(x.get_shape().as_list()[<span class="hljs-number">1</span>:])  <span class="hljs-comment"># 计算的是总共的神经元数量，第一个表示batch数量所以去掉</span><br>        <span class="hljs-keyword">return</span> tf.reshape(tran_x, [-<span class="hljs-number">1</span>, nums])  <span class="hljs-comment"># [batch,通道数channels,列,行] -&gt; [batch,通道数channels*列*行],-1代表自适应batch数量</span><br> <br>    <span class="hljs-comment"># 全连接层：x输入；id：层数索引；num_out：输出尺寸；activation：激活函数</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_fc_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, num_out, activation=<span class="hljs-literal">None</span></span>):</span><br>        num_in = x.get_shape().as_list()[-<span class="hljs-number">1</span>]  <span class="hljs-comment"># 通道数/维度</span><br>        <span class="hljs-comment"># 均值为0标准差为0.1的正态分布，初始化权重w；shape=行*列*通道数*卷积核个数</span><br>        weight = tf.Variable(tf.truncated_normal(shape=[num_in, num_out], mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">0.1</span>))<br>        bias = tf.Variable(tf.zeros(shape=[num_out, ]))  <span class="hljs-comment"># 列向量</span><br>        output = tf.nn.xw_plus_b(x, weight, bias)<br> <br>        <span class="hljs-comment"># 正常全连接层是leak_relu激活函数；但是最后一层是liner函数</span><br>        <span class="hljs-keyword">if</span> activation:<br>            output = activation(output)<br> <br>        <span class="hljs-comment"># 打印该层信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=Fc,num_out=%d,output_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, num_out, <span class="hljs-built_in">str</span>(output.get_shape())))<br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment">######################## 对应【4】:可视化检测边界框、将obj的分类结果和坐标保存成txt#########################################</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_detect_from_image</span>(<span class="hljs-params">self, image</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Do detection given a cv image&quot;&quot;&quot;</span><br>        img_h, img_w, _ = image.shape<br>        img_resized = cv2.resize(image, (<span class="hljs-number">448</span>, <span class="hljs-number">448</span>))<br>        img_RGB = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)<br>        img_resized_np = np.asarray(img_RGB)<br>        _images = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>, <span class="hljs-number">3</span>), dtype=np.float32)<br>        _images[<span class="hljs-number">0</span>] = (img_resized_np / <span class="hljs-number">255.0</span>) * <span class="hljs-number">2.0</span> - <span class="hljs-number">1.0</span><br>        scores, boxes, box_classes = self.sess.run([self.scores, self.boxes, self.box_classes],<br>                                                   feed_dict=&#123;self.images: _images, self.width: img_w,<br>                                                              self.height: img_h&#125;)<br>        <span class="hljs-keyword">return</span> scores, boxes, box_classes<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_results</span>(<span class="hljs-params">self, image, results, imshow=<span class="hljs-literal">True</span>, deteted_boxes_file=<span class="hljs-literal">None</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                     detected_image_file=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Show the detection boxes&quot;&quot;&quot;</span><br>        img_cp = image.copy()<br>        <span class="hljs-keyword">if</span> deteted_boxes_file:<br>            f = <span class="hljs-built_in">open</span>(deteted_boxes_file, <span class="hljs-string">&quot;w&quot;</span>)<br>        <span class="hljs-comment"># draw boxes</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(results)):<br>            x = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">1</span>])<br>            y = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">2</span>])<br>            w = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">3</span>]) // <span class="hljs-number">2</span><br>            h = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">4</span>]) // <span class="hljs-number">2</span><br>            <span class="hljs-keyword">if</span> self.verbose:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class: %s, [x, y, w, h]=[%d, %d, %d, %d], confidence=%f&quot;</span><br>                      % (results[i][<span class="hljs-number">0</span>], x, y, w, h, results[i][-<span class="hljs-number">1</span>]))<br> <br>                <span class="hljs-comment"># 中心坐标 + 宽高box(x, y, w, h) -&gt; xmin = x - w / 2 -&gt; 左上 + 右下box(xmin, ymin, xmax, ymax)</span><br>                cv2.rectangle(img_cp, (x - w, y - h), (x + w, y + h), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)<br> <br>                <span class="hljs-comment"># 在边界框上显示类别、分数(类别置信度)</span><br>                cv2.rectangle(img_cp, (x - w, y - h - <span class="hljs-number">20</span>), (x + w, y - h), (<span class="hljs-number">125</span>, <span class="hljs-number">125</span>, <span class="hljs-number">125</span>), -<span class="hljs-number">1</span>)  <span class="hljs-comment"># puttext函数的背景</span><br>                cv2.putText(img_cp, results[i][<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27; : %.2f&#x27;</span> % results[i][<span class="hljs-number">5</span>], (x - w + <span class="hljs-number">5</span>, y - h - <span class="hljs-number">7</span>),<br>                            cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)<br> <br>            <span class="hljs-keyword">if</span> deteted_boxes_file:<br>                <span class="hljs-comment"># 保存obj检测结果为txt文件</span><br>                f.write(results[i][<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(x) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(y) + <span class="hljs-string">&#x27;,&#x27;</span> +<br>                        <span class="hljs-built_in">str</span>(w) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(h) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(results[i][<span class="hljs-number">5</span>]) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>        <span class="hljs-keyword">if</span> imshow:<br>            cv2.imshow(<span class="hljs-string">&#x27;YOLO_small detection&#x27;</span>, img_cp)<br>            cv2.waitKey(<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> detected_image_file:<br>            cv2.imwrite(detected_image_file, img_cp)<br>        <span class="hljs-keyword">if</span> deteted_boxes_file:<br>            f.close()<br> <br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    yolo_net = Yolo(weights_file=<span class="hljs-string">&#x27;D:/Python/YOLOv1-Tensorflow-master/YOLO_small.ckpt&#x27;</span>,<br>                    input_image=<span class="hljs-string">&#x27;D:/Python/YOLOv1-Tensorflow-master/car.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Computer Version</category>
      
      <category>Paper</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-7</title>
    <link href="/2021/04/22/DL-with-python-7/"/>
    <url>/2021/04/22/DL-with-python-7/</url>
    
    <content type="html"><![CDATA[<p>5.2-using-convnets-with-small-datasets</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="5-2-Using-convnets-with-small-datasets"><a href="#5-2-Using-convnets-with-small-datasets" class="headerlink" title="5.2 - Using convnets with small datasets"></a>5.2 - Using convnets with small datasets</h1><p>This notebook contains the code sample found in Chapter 5, Section 2 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><h2 id="Training-a-convnet-from-scratch-on-a-small-dataset"><a href="#Training-a-convnet-from-scratch-on-a-small-dataset" class="headerlink" title="Training a convnet from scratch on a small dataset"></a>Training a convnet from scratch on a small dataset</h2><p>Having to train an image classification model using only very little data is a common situation, which you likely encounter yourself in<br>practice if you ever do computer vision in a professional context.</p><p>Having “few” samples can mean anywhere from a few hundreds to a few tens of thousands of images. As a practical example, we will focus on<br>classifying images as “dogs” or “cats”, in a dataset containing 4000 pictures of cats and dogs (2000 cats, 2000 dogs). We will use 2000<br>pictures for training, 1000 for validation, and finally 1000 for testing.</p><p>In this section, we will review one basic strategy to tackle this problem: training a new model from scratch on what little data we have. We<br>will start by naively training a small convnet on our 2000 training samples, without any regularization, to set a baseline for what can be<br>achieved. This will get us to a classification accuracy of 71%. At that point, our main issue will be overfitting. Then we will introduce<br><em>data augmentation</em>, a powerful technique for mitigating overfitting in computer vision. By leveraging data augmentation, we will improve<br>our network to reach an accuracy of 82%.</p><p>In the next section, we will review two more essential techniques for applying deep learning to small datasets: <em>doing feature extraction<br>with a pre-trained network</em> (this will get us to an accuracy of 90% to 93%), and <em>fine-tuning a pre-trained network</em> (this will get us to<br>our final accuracy of 95%). Together, these three strategies — training a small model from scratch, doing feature extracting using a<br>pre-trained model, and fine-tuning a pre-trained model — will constitute your future toolbox for tackling the problem of doing computer<br>vision with small datasets.</p><h2 id="The-relevance-of-deep-learning-for-small-data-problems"><a href="#The-relevance-of-deep-learning-for-small-data-problems" class="headerlink" title="The relevance of deep learning for small-data problems"></a>The relevance of deep learning for small-data problems</h2><p>You will sometimes hear that deep learning only works when lots of data is available. This is in part a valid point: one fundamental<br>characteristic of deep learning is that it is able to find interesting features in the training data on its own, without any need for manual<br>feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where<br>the input samples are very high-dimensional, like images.</p><p>However, what constitutes “lots” of samples is relative — relative to the size and depth of the network you are trying to train, for<br>starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundreds can<br>potentially suffice if the model is small and well-regularized and if the task is simple.<br>Because convnets learn local, translation-invariant features, they are very<br>data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results<br>despite a relative lack of data, without the need for any custom feature engineering. You will see this in action in this section.</p><p>But what’s more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model<br>trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes. Specifically, in the case of<br>computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used<br>to bootstrap powerful vision models out of very little data. That’s what we will do in the next section.</p><p>For now, let’s get started by getting our hands on the data.</p><h2 id="Downloading-the-data"><a href="#Downloading-the-data" class="headerlink" title="Downloading the data"></a>Downloading the data</h2><p>The cats vs. dogs dataset that we will use isn’t packaged with Keras. It was made available by Kaggle.com as part of a computer vision<br>competition in late 2013, back when convnets weren’t quite mainstream. You can download the original dataset at:<br><code>https://www.kaggle.com/c/dogs-vs-cats/data</code> (you will need to create a Kaggle account if you don’t already have one — don’t worry, the<br>process is painless).</p><p>The pictures are medium-resolution color JPEGs. They look like this:</p><p><img src="https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg" alt="cats_vs_dogs_samples"></p><p>Unsurprisingly, the cats vs. dogs Kaggle competition in 2013 was won by entrants who used convnets. The best entries could achieve up to<br>95% accuracy. In our own example, we will get fairly close to this accuracy (in the next section), even though we will be training our<br>models on less than 10% of the data that was available to the competitors.<br>This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). After downloading<br>and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, a validation<br>set with 500 samples of each class, and finally a test set with 500 samples of each class.</p><p>Here are a few lines of code to do this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os, shutil<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># The path to the directory where the original</span><br><span class="hljs-comment"># dataset was uncompressed</span><br>original_dataset_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/kaggle_original_data&#x27;</span><br><br><span class="hljs-comment"># The directory where we will</span><br><span class="hljs-comment"># store our smaller dataset</span><br>base_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/cats_and_dogs_small&#x27;</span><br>os.mkdir(base_dir)<br><br><span class="hljs-comment"># Directories for our training,</span><br><span class="hljs-comment"># validation and test splits</span><br>train_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;train&#x27;</span>)<br>os.mkdir(train_dir)<br>validation_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;validation&#x27;</span>)<br>os.mkdir(validation_dir)<br>test_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;test&#x27;</span>)<br>os.mkdir(test_dir)<br><br><span class="hljs-comment"># Directory with our training cat pictures</span><br>train_cats_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(train_cats_dir)<br><br><span class="hljs-comment"># Directory with our training dog pictures</span><br>train_dogs_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(train_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(validation_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(validation_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>test_cats_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(test_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>test_dogs_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(test_dogs_dir)<br><br><span class="hljs-comment"># Copy first 1000 cat images to train_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br><br><span class="hljs-comment"># Copy next 500 cat images to validation_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 cat images to test_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy first 1000 dog images to train_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to validation_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to test_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br></code></pre></td></tr></table></figure><p>As a sanity check, let’s count how many pictures we have in each training split (train/validation/test):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total training cat images: 1000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total training dog images: 1000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total validation cat images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total validation dog images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total test cat images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total test dog images: 500</code></pre><p>So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of<br>samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate<br>measure of success.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>We’ve already built a small convnet for MNIST in the previous example, so you should be familiar with them. We will reuse the same<br>general structure: our convnet will be a stack of alternated <code>Conv2D</code> (with <code>relu</code> activation) and <code>MaxPooling2D</code> layers.</p><p>However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger: it will have one<br>more <code>Conv2D</code> + <code>MaxPooling2D</code> stage. This serves both to augment the capacity of the network, and to further reduce the size of the<br>feature maps, so that they aren’t overly large when we reach the <code>Flatten</code> layer. Here, since we start from inputs of size 150x150 (a<br>somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the <code>Flatten</code> layer.</p><p>Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is<br>decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.</p><p>Since we are attacking a binary classification problem, we are ending the network with a single unit (a <code>Dense</code> layer of size 1) and a<br><code>sigmoid</code> activation. This unit will encode the probability that the network is looking at one class or the other.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><br>model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Let’s take a look at how the dimensions of the feature maps change with every successive layer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         _________________________________________________________________flatten_1 (Flatten)          (None, 6272)              0         _________________________________________________________________dense_1 (Dense)              (None, 512)               3211776   _________________________________________________________________dense_2 (Dense)              (None, 1)                 513       =================================================================Total params: 3,453,121Trainable params: 3,453,121Non-trainable params: 0_________________________________________________________________</code></pre><p>For our compilation step, we’ll go with the <code>RMSprop</code> optimizer as usual. Since we ended our network with a single sigmoid unit, we will<br>use binary crossentropy as our loss (as a reminder, check out the table in Chapter 4, section 5 for a cheatsheet on what loss function to<br>use in various situations).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p>As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our<br>network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:</p><ul><li>Read the picture files.</li><li>Decode the JPEG content to RBG grids of pixels.</li><li>Convert these into floating point tensors.</li><li>Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).</li></ul><p>It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image<br>processing helper tools, located at <code>keras.preprocessing.image</code>. In particular, it contains the class <code>ImageDataGenerator</code> which allows to<br>quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we<br>will use here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator<br><br><span class="hljs-comment"># All images will be rescaled by 1./255</span><br>train_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br></code></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.</code></pre><p>Let’s take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape <code>(20, 150, 150, 3)</code>) and binary<br>labels (shape <code>(20,)</code>). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches<br>indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to <code>break</code> the iteration loop<br>at some point.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> data_batch, labels_batch <span class="hljs-keyword">in</span> train_generator:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data batch shape:&#x27;</span>, data_batch.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;labels batch shape:&#x27;</span>, labels_batch.shape)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><pre><code>data batch shape: (20, 150, 150, 3)labels batch shape: (20,)</code></pre><p>Let’s fit our model to the data using the generator. We do it using the <code>fit_generator</code> method, the equivalent of <code>fit</code> for data generators<br>like ours. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does.<br>Because the data is being generated endlessly, the generator needs to know example how many samples to draw from the generator before<br>declaring an epoch over. This is the role of the <code>steps_per_epoch</code> argument: after having drawn <code>steps_per_epoch</code> batches from the<br>generator, i.e. after having run for <code>steps_per_epoch</code> gradient descent steps, the fitting process will go to the next epoch. In our case,<br>batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.</p><p>When using <code>fit_generator</code>, one may pass a <code>validation_data</code> argument, much like with the <code>fit</code> method. Importantly, this argument is<br>allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as <code>validation_data</code>, then<br>this generator is expected to yield batches of validation data endlessly, and thus you should also specify the <code>validation_steps</code> argument,<br>which tells the process how many batches to draw from the validation generator for evaluation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">30</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/30100/100 [==============================] - 9s - loss: 0.6898 - acc: 0.5285 - val_loss: 0.6724 - val_acc: 0.5950Epoch 2/30100/100 [==============================] - 8s - loss: 0.6543 - acc: 0.6340 - val_loss: 0.6565 - val_acc: 0.5950Epoch 3/30100/100 [==============================] - 8s - loss: 0.6143 - acc: 0.6690 - val_loss: 0.6116 - val_acc: 0.6650Epoch 4/30100/100 [==============================] - 8s - loss: 0.5626 - acc: 0.7125 - val_loss: 0.5774 - val_acc: 0.6970Epoch 5/30100/100 [==============================] - 8s - loss: 0.5266 - acc: 0.7335 - val_loss: 0.5726 - val_acc: 0.6960Epoch 6/30100/100 [==============================] - 8s - loss: 0.5007 - acc: 0.7550 - val_loss: 0.6075 - val_acc: 0.6580Epoch 7/30100/100 [==============================] - 8s - loss: 0.4723 - acc: 0.7840 - val_loss: 0.5516 - val_acc: 0.7060Epoch 8/30100/100 [==============================] - 8s - loss: 0.4521 - acc: 0.7875 - val_loss: 0.5724 - val_acc: 0.6980Epoch 9/30100/100 [==============================] - 8s - loss: 0.4163 - acc: 0.8095 - val_loss: 0.5653 - val_acc: 0.7140Epoch 10/30100/100 [==============================] - 8s - loss: 0.3988 - acc: 0.8185 - val_loss: 0.5508 - val_acc: 0.7180Epoch 11/30100/100 [==============================] - 8s - loss: 0.3694 - acc: 0.8385 - val_loss: 0.5712 - val_acc: 0.7300Epoch 12/30100/100 [==============================] - 8s - loss: 0.3385 - acc: 0.8465 - val_loss: 0.6097 - val_acc: 0.7110Epoch 13/30100/100 [==============================] - 8s - loss: 0.3229 - acc: 0.8565 - val_loss: 0.5827 - val_acc: 0.7150Epoch 14/30100/100 [==============================] - 8s - loss: 0.2962 - acc: 0.8720 - val_loss: 0.5928 - val_acc: 0.7190Epoch 15/30100/100 [==============================] - 8s - loss: 0.2684 - acc: 0.9005 - val_loss: 0.5921 - val_acc: 0.7190Epoch 16/30100/100 [==============================] - 8s - loss: 0.2509 - acc: 0.8980 - val_loss: 0.6148 - val_acc: 0.7250Epoch 17/30100/100 [==============================] - 8s - loss: 0.2221 - acc: 0.9110 - val_loss: 0.6487 - val_acc: 0.7010Epoch 18/30100/100 [==============================] - 8s - loss: 0.2021 - acc: 0.9250 - val_loss: 0.6185 - val_acc: 0.7300Epoch 19/30100/100 [==============================] - 8s - loss: 0.1824 - acc: 0.9310 - val_loss: 0.7713 - val_acc: 0.7020Epoch 20/30100/100 [==============================] - 8s - loss: 0.1579 - acc: 0.9425 - val_loss: 0.6657 - val_acc: 0.7260Epoch 21/30100/100 [==============================] - 8s - loss: 0.1355 - acc: 0.9550 - val_loss: 0.8077 - val_acc: 0.7040Epoch 22/30100/100 [==============================] - 8s - loss: 0.1247 - acc: 0.9545 - val_loss: 0.7726 - val_acc: 0.7080Epoch 23/30100/100 [==============================] - 8s - loss: 0.1111 - acc: 0.9585 - val_loss: 0.7387 - val_acc: 0.7220Epoch 24/30100/100 [==============================] - 8s - loss: 0.0932 - acc: 0.9710 - val_loss: 0.8196 - val_acc: 0.7050Epoch 25/30100/100 [==============================] - 8s - loss: 0.0707 - acc: 0.9790 - val_loss: 0.9012 - val_acc: 0.7190Epoch 26/30100/100 [==============================] - 8s - loss: 0.0625 - acc: 0.9855 - val_loss: 1.0437 - val_acc: 0.6970Epoch 27/30100/100 [==============================] - 8s - loss: 0.0611 - acc: 0.9820 - val_loss: 0.9831 - val_acc: 0.7060Epoch 28/30100/100 [==============================] - 8s - loss: 0.0488 - acc: 0.9865 - val_loss: 0.9721 - val_acc: 0.7310Epoch 29/30100/100 [==============================] - 8s - loss: 0.0375 - acc: 0.9915 - val_loss: 0.9987 - val_acc: 0.7100Epoch 30/30100/100 [==============================] - 8s - loss: 0.0387 - acc: 0.9895 - val_loss: 1.0139 - val_acc: 0.7240</code></pre><p>It is good practice to always save your models after training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_1.h5&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Let’s plot the loss and accuracy of the model over the training and validation data during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_30_0p.png" alt="output"></p><p><img src="/img/output_30_1.png" alt="output"></p><p>These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our<br>validation accuracy stalls at 70-72%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss<br>keeps decreasing linearly until it reaches nearly 0.</p><p>Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. You already know about a<br>number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to<br>introduce a new one, specific to computer vision, and used almost universally when processing images with deep learning models: <em>data<br>augmentation</em>.</p><h2 id="Using-data-augmentation"><a href="#Using-data-augmentation" class="headerlink" title="Using data augmentation"></a>Using data augmentation</h2><p>Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data.<br>Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data<br>augmentation takes the approach of generating more training data from existing training samples, by “augmenting” the samples via a number<br>of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same<br>picture twice. This helps the model get exposed to more aspects of the data and generalize better.</p><p>In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our <code>ImageDataGenerator</code><br>instance. Let’s get started with an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">datagen = ImageDataGenerator(<br>      rotation_range=<span class="hljs-number">40</span>,<br>      width_shift_range=<span class="hljs-number">0.2</span>,<br>      height_shift_range=<span class="hljs-number">0.2</span>,<br>      shear_range=<span class="hljs-number">0.2</span>,<br>      zoom_range=<span class="hljs-number">0.2</span>,<br>      horizontal_flip=<span class="hljs-literal">True</span>,<br>      fill_mode=<span class="hljs-string">&#x27;nearest&#x27;</span>)<br></code></pre></td></tr></table></figure><p>These are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over what we just wrote:</p><ul><li><code>rotation_range</code> is a value in degrees (0-180), a range within which to randomly rotate pictures.</li><li><code>width_shift</code> and <code>height_shift</code> are ranges (as a fraction of total width or height) within which to randomly translate pictures<br>vertically or horizontally.</li><li><code>shear_range</code> is for randomly applying shearing transformations.</li><li><code>zoom_range</code> is for randomly zooming inside pictures.</li><li><code>horizontal_flip</code> is for randomly flipping half of the images horizontally — relevant when there are no assumptions of horizontal<br>asymmetry (e.g. real-world pictures).</li><li><code>fill_mode</code> is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.</li></ul><p>Let’s take a look at our augmented images:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This is module with image preprocessing utilities</span><br><span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> image<br><br>fnames = [os.path.join(train_cats_dir, fname) <span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> os.listdir(train_cats_dir)]<br><br><span class="hljs-comment"># We pick one image to &quot;augment&quot;</span><br>img_path = fnames[<span class="hljs-number">3</span>]<br><br><span class="hljs-comment"># Read the image and resize it</span><br>img = image.load_img(img_path, target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>))<br><br><span class="hljs-comment"># Convert it to a Numpy array with shape (150, 150, 3)</span><br>x = image.img_to_array(img)<br><br><span class="hljs-comment"># Reshape it to (1, 150, 150, 3)</span><br>x = x.reshape((<span class="hljs-number">1</span>,) + x.shape)<br><br><span class="hljs-comment"># The .flow() command below generates batches of randomly transformed images.</span><br><span class="hljs-comment"># It will loop indefinitely, so we need to `break` the loop at some point!</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> datagen.flow(x, batch_size=<span class="hljs-number">1</span>):<br>    plt.figure(i)<br>    imgplot = plt.imshow(image.array_to_img(batch[<span class="hljs-number">0</span>]))<br>    i += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">4</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">break</span><br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_35_0.png" alt="output"></p><p><img src="/img/output_35_1.png" alt="output"></p><p><img src="/img/output_35_2.png" alt="output"></p><p><img src="/img/output_35_3.png" alt="output"></p><p>If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs<br>that it sees are still heavily intercorrelated, since they come from a small number of original images — we cannot produce new information,<br>we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight<br>overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Let’s train our network using data augmentation and dropout:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">train_datagen = ImageDataGenerator(<br>    rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>,<br>    rotation_range=<span class="hljs-number">40</span>,<br>    width_shift_range=<span class="hljs-number">0.2</span>,<br>    height_shift_range=<span class="hljs-number">0.2</span>,<br>    shear_range=<span class="hljs-number">0.2</span>,<br>    zoom_range=<span class="hljs-number">0.2</span>,<br>    horizontal_flip=<span class="hljs-literal">True</span>,)<br><br><span class="hljs-comment"># Note that the validation data should not be augmented!</span><br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">100</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.Epoch 1/100100/100 [==============================] - 24s - loss: 0.6857 - acc: 0.5447 - val_loss: 0.6620 - val_acc: 0.5888Epoch 2/100100/100 [==============================] - 23s - loss: 0.6710 - acc: 0.5675 - val_loss: 0.6606 - val_acc: 0.5825Epoch 3/100100/100 [==============================] - 22s - loss: 0.6609 - acc: 0.5913 - val_loss: 0.6663 - val_acc: 0.5711.594 - ETA: 7s - loss: 0.6655 - ETA: 5s - los - ETA: 1s - loss: 0.6620 - acc: Epoch 4/100100/100 [==============================] - 22s - loss: 0.6446 - acc: 0.6178 - val_loss: 0.6200 - val_acc: 0.6379Epoch 5/100100/100 [==============================] - 22s - loss: 0.6267 - acc: 0.6325 - val_loss: 0.6280 - val_acc: 0.5996Epoch 6/100100/100 [==============================] - 22s - loss: 0.6080 - acc: 0.6631 - val_loss: 0.6841 - val_acc: 0.5490Epoch 7/100100/100 [==============================] - 22s - loss: 0.5992 - acc: 0.6700 - val_loss: 0.5717 - val_acc: 0.6946Epoch 8/100100/100 [==============================] - 22s - loss: 0.5908 - acc: 0.6819 - val_loss: 0.5858 - val_acc: 0.6764Epoch 9/100100/100 [==============================] - 22s - loss: 0.5869 - acc: 0.6856 - val_loss: 0.5658 - val_acc: 0.6785Epoch 10/100100/100 [==============================] - 23s - loss: 0.5692 - acc: 0.6934 - val_loss: 0.5409 - val_acc: 0.7170Epoch 11/100100/100 [==============================] - 22s - loss: 0.5708 - acc: 0.6897 - val_loss: 0.5325 - val_acc: 0.7274Epoch 12/100100/100 [==============================] - 23s - loss: 0.5583 - acc: 0.7047 - val_loss: 0.5683 - val_acc: 0.7126Epoch 13/100100/100 [==============================] - 22s - loss: 0.5602 - acc: 0.7069 - val_loss: 0.6010 - val_acc: 0.6593Epoch 14/100100/100 [==============================] - 22s - loss: 0.5510 - acc: 0.7231 - val_loss: 0.5387 - val_acc: 0.7229Epoch 15/100100/100 [==============================] - 23s - loss: 0.5527 - acc: 0.7175 - val_loss: 0.5204 - val_acc: 0.7322Epoch 16/100100/100 [==============================] - 23s - loss: 0.5426 - acc: 0.7181 - val_loss: 0.5083 - val_acc: 0.7410Epoch 17/100100/100 [==============================] - 23s - loss: 0.5399 - acc: 0.7344 - val_loss: 0.5103 - val_acc: 0.7468Epoch 18/100100/100 [==============================] - 23s - loss: 0.5375 - acc: 0.7312 - val_loss: 0.5133 - val_acc: 0.7430Epoch 19/100100/100 [==============================] - 22s - loss: 0.5308 - acc: 0.7338 - val_loss: 0.4936 - val_acc: 0.7610Epoch 20/100100/100 [==============================] - 22s - loss: 0.5225 - acc: 0.7387 - val_loss: 0.4952 - val_acc: 0.7563Epoch 21/100100/100 [==============================] - 22s - loss: 0.5180 - acc: 0.7491 - val_loss: 0.4999 - val_acc: 0.7481Epoch 22/100100/100 [==============================] - 23s - loss: 0.5118 - acc: 0.7538 - val_loss: 0.4770 - val_acc: 0.7764Epoch 23/100100/100 [==============================] - 22s - loss: 0.5245 - acc: 0.7378 - val_loss: 0.4929 - val_acc: 0.7671Epoch 24/100100/100 [==============================] - 22s - loss: 0.5136 - acc: 0.7503 - val_loss: 0.4709 - val_acc: 0.7732Epoch 25/100100/100 [==============================] - 22s - loss: 0.4980 - acc: 0.7512 - val_loss: 0.4775 - val_acc: 0.7684Epoch 26/100100/100 [==============================] - 22s - loss: 0.4875 - acc: 0.7622 - val_loss: 0.4745 - val_acc: 0.7790Epoch 27/100100/100 [==============================] - 22s - loss: 0.5044 - acc: 0.7578 - val_loss: 0.5000 - val_acc: 0.7403Epoch 28/100100/100 [==============================] - 22s - loss: 0.4948 - acc: 0.7603 - val_loss: 0.4619 - val_acc: 0.7754Epoch 29/100100/100 [==============================] - 22s - loss: 0.4898 - acc: 0.7578 - val_loss: 0.4730 - val_acc: 0.7726Epoch 30/100100/100 [==============================] - 22s - loss: 0.4808 - acc: 0.7691 - val_loss: 0.4599 - val_acc: 0.7716Epoch 31/100100/100 [==============================] - 22s - loss: 0.4792 - acc: 0.7678 - val_loss: 0.4671 - val_acc: 0.7790Epoch 32/100100/100 [==============================] - 22s - loss: 0.4723 - acc: 0.7716 - val_loss: 0.4451 - val_acc: 0.7849Epoch 33/100100/100 [==============================] - 22s - loss: 0.4750 - acc: 0.7694 - val_loss: 0.4827 - val_acc: 0.7665Epoch 34/100100/100 [==============================] - 22s - loss: 0.4816 - acc: 0.7647 - val_loss: 0.4953 - val_acc: 0.7513Epoch 35/100100/100 [==============================] - 22s - loss: 0.4598 - acc: 0.7813 - val_loss: 0.4426 - val_acc: 0.7843Epoch 36/100100/100 [==============================] - 23s - loss: 0.4643 - acc: 0.7781 - val_loss: 0.4692 - val_acc: 0.7680Epoch 37/100100/100 [==============================] - 22s - loss: 0.4675 - acc: 0.7778 - val_loss: 0.4849 - val_acc: 0.7633Epoch 38/100100/100 [==============================] - 22s - loss: 0.4658 - acc: 0.7737 - val_loss: 0.4632 - val_acc: 0.7760Epoch 39/100100/100 [==============================] - 22s - loss: 0.4581 - acc: 0.7866 - val_loss: 0.4489 - val_acc: 0.7880Epoch 40/100100/100 [==============================] - 23s - loss: 0.4485 - acc: 0.7856 - val_loss: 0.4479 - val_acc: 0.7931Epoch 41/100100/100 [==============================] - 22s - loss: 0.4637 - acc: 0.7759 - val_loss: 0.4453 - val_acc: 0.7990Epoch 42/100100/100 [==============================] - 22s - loss: 0.4528 - acc: 0.7841 - val_loss: 0.4758 - val_acc: 0.7868Epoch 43/100100/100 [==============================] - 22s - loss: 0.4481 - acc: 0.7856 - val_loss: 0.4472 - val_acc: 0.7893Epoch 44/100100/100 [==============================] - 22s - loss: 0.4540 - acc: 0.7953 - val_loss: 0.4366 - val_acc: 0.7867A: 6s - loss: 0.4523 - acc: - ETA: Epoch 45/100100/100 [==============================] - 22s - loss: 0.4411 - acc: 0.7919 - val_loss: 0.4708 - val_acc: 0.7697Epoch 46/100100/100 [==============================] - 22s - loss: 0.4493 - acc: 0.7869 - val_loss: 0.4366 - val_acc: 0.7829Epoch 47/100100/100 [==============================] - 22s - loss: 0.4436 - acc: 0.7916 - val_loss: 0.4307 - val_acc: 0.8090Epoch 48/100100/100 [==============================] - 22s - loss: 0.4391 - acc: 0.7928 - val_loss: 0.4203 - val_acc: 0.8065Epoch 49/100100/100 [==============================] - 23s - loss: 0.4284 - acc: 0.8053 - val_loss: 0.4422 - val_acc: 0.8041Epoch 50/100100/100 [==============================] - 22s - loss: 0.4492 - acc: 0.7906 - val_loss: 0.5422 - val_acc: 0.7437Epoch 51/100100/100 [==============================] - 22s - loss: 0.4292 - acc: 0.7953 - val_loss: 0.4446 - val_acc: 0.7932Epoch 52/100100/100 [==============================] - 22s - loss: 0.4275 - acc: 0.8037 - val_loss: 0.4287 - val_acc: 0.7989Epoch 53/100100/100 [==============================] - 22s - loss: 0.4297 - acc: 0.7975 - val_loss: 0.4091 - val_acc: 0.8046Epoch 54/100100/100 [==============================] - 23s - loss: 0.4198 - acc: 0.7978 - val_loss: 0.4413 - val_acc: 0.7964Epoch 55/100100/100 [==============================] - 23s - loss: 0.4195 - acc: 0.8019 - val_loss: 0.4265 - val_acc: 0.8001Epoch 56/100100/100 [==============================] - 22s - loss: 0.4081 - acc: 0.8056 - val_loss: 0.4374 - val_acc: 0.7957Epoch 57/100100/100 [==============================] - 22s - loss: 0.4214 - acc: 0.8006 - val_loss: 0.4228 - val_acc: 0.8020Epoch 58/100100/100 [==============================] - 22s - loss: 0.4050 - acc: 0.8097 - val_loss: 0.4332 - val_acc: 0.7900Epoch 59/100100/100 [==============================] - 22s - loss: 0.4162 - acc: 0.8134 - val_loss: 0.4088 - val_acc: 0.8099Epoch 60/100100/100 [==============================] - 22s - loss: 0.4042 - acc: 0.8141 - val_loss: 0.4436 - val_acc: 0.7957Epoch 61/100100/100 [==============================] - 23s - loss: 0.4016 - acc: 0.8212 - val_loss: 0.4082 - val_acc: 0.8189Epoch 62/100100/100 [==============================] - 22s - loss: 0.4167 - acc: 0.8097 - val_loss: 0.3935 - val_acc: 0.8236Epoch 63/100100/100 [==============================] - 23s - loss: 0.4052 - acc: 0.8138 - val_loss: 0.4509 - val_acc: 0.7824Epoch 64/100100/100 [==============================] - 22s - loss: 0.4011 - acc: 0.8209 - val_loss: 0.3874 - val_acc: 0.8299Epoch 65/100100/100 [==============================] - 22s - loss: 0.3966 - acc: 0.8131 - val_loss: 0.4328 - val_acc: 0.7970Epoch 66/100100/100 [==============================] - 23s - loss: 0.3889 - acc: 0.8163 - val_loss: 0.4766 - val_acc: 0.7719Epoch 67/100100/100 [==============================] - 22s - loss: 0.3960 - acc: 0.8163 - val_loss: 0.3859 - val_acc: 0.8325Epoch 68/100100/100 [==============================] - 22s - loss: 0.3893 - acc: 0.8231 - val_loss: 0.4172 - val_acc: 0.8128Epoch 69/100100/100 [==============================] - 23s - loss: 0.3828 - acc: 0.8219 - val_loss: 0.4023 - val_acc: 0.8215 loss: 0.3881 - acc:Epoch 70/100100/100 [==============================] - 22s - loss: 0.3909 - acc: 0.8275 - val_loss: 0.4275 - val_acc: 0.8008Epoch 71/100100/100 [==============================] - 22s - loss: 0.3826 - acc: 0.8244 - val_loss: 0.3815 - val_acc: 0.8177Epoch 72/100100/100 [==============================] - 22s - loss: 0.3837 - acc: 0.8272 - val_loss: 0.4040 - val_acc: 0.8287Epoch 73/100100/100 [==============================] - 23s - loss: 0.3812 - acc: 0.8222 - val_loss: 0.4039 - val_acc: 0.8058Epoch 74/100100/100 [==============================] - 22s - loss: 0.3829 - acc: 0.8281 - val_loss: 0.4204 - val_acc: 0.8015Epoch 75/100100/100 [==============================] - 22s - loss: 0.3708 - acc: 0.8350 - val_loss: 0.4083 - val_acc: 0.8204Epoch 76/100100/100 [==============================] - 22s - loss: 0.3831 - acc: 0.8216 - val_loss: 0.3899 - val_acc: 0.8215Epoch 77/100100/100 [==============================] - 22s - loss: 0.3695 - acc: 0.8375 - val_loss: 0.3963 - val_acc: 0.8293Epoch 78/100100/100 [==============================] - 22s - loss: 0.3809 - acc: 0.8234 - val_loss: 0.4046 - val_acc: 0.8236Epoch 79/100100/100 [==============================] - 22s - loss: 0.3637 - acc: 0.8362 - val_loss: 0.3990 - val_acc: 0.8325Epoch 80/100100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8400 - val_loss: 0.3925 - val_acc: 0.8350Epoch 81/100100/100 [==============================] - 22s - loss: 0.3762 - acc: 0.8303 - val_loss: 0.3813 - val_acc: 0.8331Epoch 82/100100/100 [==============================] - 23s - loss: 0.3672 - acc: 0.8347 - val_loss: 0.4539 - val_acc: 0.7931Epoch 83/100100/100 [==============================] - 22s - loss: 0.3636 - acc: 0.8353 - val_loss: 0.3988 - val_acc: 0.8261Epoch 84/100100/100 [==============================] - 22s - loss: 0.3503 - acc: 0.8453 - val_loss: 0.3987 - val_acc: 0.8325Epoch 85/100100/100 [==============================] - 22s - loss: 0.3586 - acc: 0.8437 - val_loss: 0.3842 - val_acc: 0.8306Epoch 86/100100/100 [==============================] - 22s - loss: 0.3624 - acc: 0.8353 - val_loss: 0.4100 - val_acc: 0.8196.834Epoch 87/100100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8422 - val_loss: 0.3814 - val_acc: 0.8331Epoch 88/100100/100 [==============================] - 22s - loss: 0.3487 - acc: 0.8494 - val_loss: 0.4266 - val_acc: 0.8109Epoch 89/100100/100 [==============================] - 22s - loss: 0.3598 - acc: 0.8400 - val_loss: 0.4076 - val_acc: 0.8325Epoch 90/100100/100 [==============================] - 22s - loss: 0.3510 - acc: 0.8450 - val_loss: 0.3762 - val_acc: 0.8388Epoch 91/100100/100 [==============================] - 22s - loss: 0.3458 - acc: 0.8450 - val_loss: 0.4684 - val_acc: 0.8015Epoch 92/100100/100 [==============================] - 22s - loss: 0.3454 - acc: 0.8441 - val_loss: 0.4017 - val_acc: 0.8204Epoch 93/100100/100 [==============================] - 22s - loss: 0.3402 - acc: 0.8487 - val_loss: 0.3928 - val_acc: 0.8204Epoch 94/100100/100 [==============================] - 22s - loss: 0.3569 - acc: 0.8394 - val_loss: 0.4005 - val_acc: 0.8338Epoch 95/100100/100 [==============================] - 22s - loss: 0.3425 - acc: 0.8494 - val_loss: 0.3641 - val_acc: 0.8439Epoch 96/100100/100 [==============================] - 22s - loss: 0.3335 - acc: 0.8531 - val_loss: 0.3811 - val_acc: 0.8363Epoch 97/100100/100 [==============================] - 22s - loss: 0.3204 - acc: 0.8581 - val_loss: 0.3786 - val_acc: 0.8331Epoch 98/100100/100 [==============================] - 22s - loss: 0.3250 - acc: 0.8606 - val_loss: 0.4205 - val_acc: 0.8236Epoch 99/100100/100 [==============================] - 22s - loss: 0.3255 - acc: 0.8581 - val_loss: 0.3518 - val_acc: 0.8460Epoch 100/100100/100 [==============================] - 22s - loss: 0.3280 - acc: 0.8491 - val_loss: 0.3776 - val_acc: 0.8439</code></pre><p>Let’s save our model — we will be using it in the section on convnet visualization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_2.h5&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Let’s plot our results again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_43_0.png" alt="output"></p><p><img src="/img/output_43_1.png" alt="output"></p><p>Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation<br>curves. We are now able to reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.</p><p>By leveraging regularization techniques even further and by tuning the network’s parameters (such as the number of filters per convolution<br>layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86-87%. However, it would prove<br>very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a<br>next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two<br>sections.</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-6</title>
    <link href="/2021/04/22/DL-with-python-6/"/>
    <url>/2021/04/22/DL-with-python-6/</url>
    
    <content type="html"><![CDATA[<p>5.1-introduction-to-convnets</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="5-1-Introduction-to-convnets"><a href="#5-1-Introduction-to-convnets" class="headerlink" title="5.1 - Introduction to convnets"></a>5.1 - Introduction to convnets</h1><p>This notebook contains the code sample found in Chapter 5, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>First, let’s take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you’ve already been<br>through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its<br>accuracy will still blow out of the water that of the densely-connected model from Chapter 2.</p><p>The 6 lines of code below show you what a basic convnet looks like. It’s a stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers. We’ll see in a<br>minute what they do concretely.<br>Importantly, a convnet takes as input tensors of shape <code>(image_height, image_width, image_channels)</code> (not including the batch dimension).<br>In our case, we will configure our convnet to process inputs of size <code>(28, 28, 1)</code>, which is the format of MNIST images. We do this via<br>passing the argument <code>input_shape=(28, 28, 1)</code> to our first layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><br>model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Let’s display the architecture of our convnet so far:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     =================================================================Total params: 55,744Trainable params: 55,744Non-trainable params: 0_________________________________________________________________</code></pre><p>You can see above that the output of every <code>Conv2D</code> and <code>MaxPooling2D</code> layer is a 3D tensor of shape <code>(height, width, channels)</code>. The width<br>and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to<br>the <code>Conv2D</code> layers (e.g. 32 or 64).</p><p>The next step would be to feed our last output tensor (of shape <code>(3, 3, 64)</code>) into a densely-connected classifier network like those you are<br>already familiar with: a stack of <code>Dense</code> layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor.<br>So first, we will have to flatten our 3D outputs to 1D, and then add a few <code>Dense</code> layers on top:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(layers.Flatten())<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here’s what our network<br>looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     _________________________________________________________________flatten_1 (Flatten)          (None, 576)               0         _________________________________________________________________dense_1 (Dense)              (None, 64)                36928     _________________________________________________________________dense_2 (Dense)              (None, 10)                650       =================================================================Total params: 93,322Trainable params: 93,322Non-trainable params: 0_________________________________________________________________</code></pre><p>As you can see, our <code>(3, 3, 64)</code> outputs were flattened into vectors of shape <code>(576,)</code>, before going through two <code>Dense</code> layers.</p><p>Now, let’s train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter<br>2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br><br>train_images = train_images.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>test_images = test_images.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(train_images, train_labels, epochs=<span class="hljs-number">5</span>, batch_size=<span class="hljs-number">64</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/560000/60000 [==============================] - 8s - loss: 0.1766 - acc: 0.9440     Epoch 2/560000/60000 [==============================] - 7s - loss: 0.0462 - acc: 0.9855     Epoch 3/560000/60000 [==============================] - 7s - loss: 0.0322 - acc: 0.9902     Epoch 4/560000/60000 [==============================] - 7s - loss: 0.0241 - acc: 0.9926     Epoch 5/560000/60000 [==============================] - 7s - loss: 0.0187 - acc: 0.9943     &lt;keras.callbacks.History at 0x7fbd9c4cd828&gt;</code></pre><p>Let’s evaluate the model on the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loss, test_acc = model.evaluate(test_images, test_labels)<br></code></pre></td></tr></table></figure><pre><code> 9536/10000 [===========================&gt;..] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_acc<br></code></pre></td></tr></table></figure><pre><code>0.99129999999999996</code></pre><p>While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we<br>decreased our error rate by 68% (relative). Not bad! </p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-5</title>
    <link href="/2021/04/22/DL-with-python-5/"/>
    <url>/2021/04/22/DL-with-python-5/</url>
    
    <content type="html"><![CDATA[<p>4.4-overfitting-and-underfitting</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Overfitting-and-underfitting"><a href="#Overfitting-and-underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In all the examples we saw in the previous chapter — movie review sentiment prediction, topic classification, and house price regression —<br>we could notice that the performance of our model on the held-out validation data would always peak after a few epochs and would then start<br>degrading, i.e. our model would quickly start to <em>overfit</em> to the training data. Overfitting happens in every single machine learning<br>problem. Learning how to deal with overfitting is essential to mastering machine learning.</p><p>The fundamental issue in machine learning is the tension between optimization and generalization. “Optimization” refers to the process of<br>adjusting a model to get the best performance possible on the training data (the “learning” in “machine learning”), while “generalization”<br>refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of<br>course, but you do not control generalization; you can only adjust the model based on its training data.</p><p>At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on<br>test data. While this is happening, your model is said to be <em>under-fit</em>: there is still progress to be made; the network hasn’t yet<br>modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops<br>improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns<br>that are specific to the training data but that are misleading or irrelevant when it comes to new data.</p><p>To prevent a model from learning misleading or irrelevant patterns found in the training data, <em>the best solution is of course to get<br>more training data</em>. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution<br>is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to<br>store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most<br>prominent patterns, which have a better chance of generalizing well.</p><p>The processing of fighting overfitting in this way is called <em>regularization</em>. Let’s review some of the most common regularization<br>techniques, and let’s apply them in practice to improve our movie classification model from  the previous chapter.</p><p>Note: in this notebook we will be using the IMDB test set as our validation set. It doesn’t matter in this context.</p><p>Let’s prepare the data using the code from Chapter 3, Section 5:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="Fighting-overfitting"><a href="#Fighting-overfitting" class="headerlink" title="Fighting overfitting"></a>Fighting overfitting</h1><h2 id="Reducing-the-network’s-size"><a href="#Reducing-the-network’s-size" class="headerlink" title="Reducing the network’s size"></a>Reducing the network’s size</h2><p>The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is<br>determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is<br>often referred to as the model’s “capacity”. Intuitively, a model with more parameters will have more “memorization capacity” and therefore<br>will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any<br>generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the<br>MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying<br>new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge<br>is generalization, not fitting.</p><p>On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in<br>order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets<br>— precisely the type of representations that we are interested in. At the same time, keep in mind that you should be using models that have<br>enough parameters that they won’t be underfitting: your model shouldn’t be starved for memorization resources. There is a compromise to be<br>found between “too much capacity” and “not enough capacity”.</p><p>Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You<br>will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the<br>right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and<br>parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the<br>validation loss.</p><p>Let’s try this on our movie review classification network. Our original network was as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>original_model = models.Sequential()<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>original_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>original_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                       loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                       metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Now let’s try to replace it with this smaller network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model = models.Sequential()<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>smaller_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>smaller_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                      loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                      metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Here’s a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of<br>the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">original_hist = original_model.fit(x_train, y_train,<br>                                   epochs=<span class="hljs-number">20</span>,<br>                                   batch_size=<span class="hljs-number">512</span>,<br>                                   validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4594 - acc: 0.8188 - val_loss: 0.3429 - val_acc: 0.8812Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.2659 - acc: 0.9075 - val_loss: 0.2873 - val_acc: 0.8905Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2060 - acc: 0.9276 - val_loss: 0.2827 - val_acc: 0.8879Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.1697 - acc: 0.9401 - val_loss: 0.2921 - val_acc: 0.8851Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.1495 - acc: 0.9470 - val_loss: 0.3100 - val_acc: 0.8812Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1283 - acc: 0.9572 - val_loss: 0.3336 - val_acc: 0.8748Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1121 - acc: 0.9624 - val_loss: 0.3987 - val_acc: 0.8593Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.0994 - acc: 0.9670 - val_loss: 0.3788 - val_acc: 0.8702Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.0889 - acc: 0.9716 - val_loss: 0.4242 - val_acc: 0.8603Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.0782 - acc: 0.9757 - val_loss: 0.4256 - val_acc: 0.8653Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.0691 - acc: 0.9792 - val_loss: 0.4515 - val_acc: 0.8638Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.0603 - acc: 0.9820 - val_loss: 0.5102 - val_acc: 0.8610Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.0518 - acc: 0.9851 - val_loss: 0.5281 - val_acc: 0.8587Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0446 - acc: 0.9873 - val_loss: 0.5441 - val_acc: 0.8589Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0367 - acc: 0.9903 - val_loss: 0.5777 - val_acc: 0.8574Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0313 - acc: 0.9922 - val_loss: 0.6377 - val_acc: 0.8555Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0247 - acc: 0.9941 - val_loss: 0.7269 - val_acc: 0.8501Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0203 - acc: 0.9956 - val_loss: 0.6920 - val_acc: 0.8516Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0156 - acc: 0.9970 - val_loss: 0.7689 - val_acc: 0.8425Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0144 - acc: 0.9966 - val_loss: 0.7694 - val_acc: 0.8487</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model_hist = smaller_model.fit(x_train, y_train,<br>                                       epochs=<span class="hljs-number">20</span>,<br>                                       batch_size=<span class="hljs-number">512</span>,<br>                                       validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 2s - loss: 0.5737 - acc: 0.8049 - val_loss: 0.4826 - val_acc: 0.8616Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3973 - acc: 0.8866 - val_loss: 0.3699 - val_acc: 0.8776Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2985 - acc: 0.9054 - val_loss: 0.3140 - val_acc: 0.8860Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2428 - acc: 0.9189 - val_loss: 0.2913 - val_acc: 0.8870Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2085 - acc: 0.9290 - val_loss: 0.2809 - val_acc: 0.8897Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1849 - acc: 0.9360 - val_loss: 0.2772 - val_acc: 0.8899Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1666 - acc: 0.9430 - val_loss: 0.2835 - val_acc: 0.8863Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1515 - acc: 0.9487 - val_loss: 0.2909 - val_acc: 0.8850Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1388 - acc: 0.9526 - val_loss: 0.2984 - val_acc: 0.8842Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1285 - acc: 0.9569 - val_loss: 0.3102 - val_acc: 0.8818Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1194 - acc: 0.9599 - val_loss: 0.3219 - val_acc: 0.8794Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1105 - acc: 0.9648 - val_loss: 0.3379 - val_acc: 0.8774Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1035 - acc: 0.9674 - val_loss: 0.3532 - val_acc: 0.8730Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0963 - acc: 0.9688 - val_loss: 0.3651 - val_acc: 0.8731Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0895 - acc: 0.9724 - val_loss: 0.3858 - val_acc: 0.8703Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0838 - acc: 0.9734 - val_loss: 0.4157 - val_acc: 0.8654Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0785 - acc: 0.9764 - val_loss: 0.4214 - val_acc: 0.8677Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0733 - acc: 0.9784 - val_loss: 0.4390 - val_acc: 0.8644Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0685 - acc: 0.9796 - val_loss: 0.4539 - val_acc: 0.8638Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0638 - acc: 0.9822 - val_loss: 0.4744 - val_acc: 0.8617</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>)<br>original_val_loss = original_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br>smaller_model_val_loss = smaller_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># b+ is for &quot;blue cross&quot;</span><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, smaller_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Smaller model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_12_0.png" alt="output"></p><p>As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance<br>degrades much more slowly once it starts overfitting.</p><p>Now, for kicks, let’s add to this benchmark a network that has much more capacity, far more than the problem would warrant:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model = models.Sequential()<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>bigger_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>bigger_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                     loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                     metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_hist = bigger_model.fit(x_train, y_train,<br>                                     epochs=<span class="hljs-number">20</span>,<br>                                     batch_size=<span class="hljs-number">512</span>,<br>                                     validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4539 - acc: 0.8011 - val_loss: 0.4150 - val_acc: 0.8229Epoch 2/2025000/25000 [==============================] - 3s - loss: 0.2148 - acc: 0.9151 - val_loss: 0.2742 - val_acc: 0.8901Epoch 3/2025000/25000 [==============================] - 3s - loss: 0.1217 - acc: 0.9544 - val_loss: 0.5442 - val_acc: 0.7975Epoch 4/2025000/25000 [==============================] - 3s - loss: 0.0552 - acc: 0.9835 - val_loss: 0.4316 - val_acc: 0.8842Epoch 5/2025000/25000 [==============================] - 3s - loss: 0.0662 - acc: 0.9888 - val_loss: 0.5098 - val_acc: 0.8822Epoch 6/2025000/25000 [==============================] - 3s - loss: 0.0017 - acc: 0.9998 - val_loss: 0.6867 - val_acc: 0.8811Epoch 7/2025000/25000 [==============================] - 3s - loss: 0.1019 - acc: 0.9882 - val_loss: 0.6737 - val_acc: 0.8800Epoch 8/2025000/25000 [==============================] - 3s - loss: 0.0735 - acc: 0.9896 - val_loss: 0.6185 - val_acc: 0.8772Epoch 9/2025000/25000 [==============================] - 3s - loss: 3.4759e-04 - acc: 1.0000 - val_loss: 0.7328 - val_acc: 0.8818Epoch 10/2025000/25000 [==============================] - 3s - loss: 0.0504 - acc: 0.9912 - val_loss: 0.7092 - val_acc: 0.8791Epoch 11/2025000/25000 [==============================] - 3s - loss: 0.0589 - acc: 0.9919 - val_loss: 0.6831 - val_acc: 0.8785Epoch 12/2025000/25000 [==============================] - 3s - loss: 1.9067e-04 - acc: 1.0000 - val_loss: 0.8005 - val_acc: 0.8784Epoch 13/2025000/25000 [==============================] - 3s - loss: 0.0623 - acc: 0.9916 - val_loss: 0.7540 - val_acc: 0.8740Epoch 14/2025000/25000 [==============================] - 3s - loss: 0.0274 - acc: 0.9954 - val_loss: 0.7806 - val_acc: 0.8670Epoch 15/2025000/25000 [==============================] - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.8107 - val_acc: 0.8783Epoch 16/2025000/25000 [==============================] - 3s - loss: 0.0445 - acc: 0.9943 - val_loss: 0.8394 - val_acc: 0.8702Epoch 17/2025000/25000 [==============================] - 3s - loss: 0.0268 - acc: 0.9959 - val_loss: 0.7708 - val_acc: 0.8745Epoch 18/2025000/25000 [==============================] - 3s - loss: 7.7057e-04 - acc: 1.0000 - val_loss: 0.8885 - val_acc: 0.8738Epoch 19/2025000/25000 [==============================] - 3s - loss: 0.0297 - acc: 0.9962 - val_loss: 0.8419 - val_acc: 0.8728Epoch 20/2025000/25000 [==============================] - 3s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.8896 - val_acc: 0.8682</code></pre><p>Here’s how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the<br>crosses are the initial network.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_val_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_17_0.png" alt="output"></p><p>The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also<br>more noisy.</p><p>Meanwhile, here are the training losses for our two networks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">original_train_loss = original_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>bigger_model_train_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br><br>plt.plot(epochs, original_train_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_train_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_19_0.png" alt="output"></p><p>As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be<br>able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large<br>difference between the training and validation loss).</p><h2 id="Adding-weight-regularization"><a href="#Adding-weight-regularization" class="headerlink" title="Adding weight regularization"></a>Adding weight regularization</h2><p>You may be familiar with <em>Occam’s Razor</em> principle: given two explanations for something, the explanation most likely to be correct is the<br>“simplest” one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some<br>training data and a network architecture, there are multiple sets of weights values (multiple <em>models</em>) that could explain the data, and<br>simpler models are less likely to overfit than complex ones.</p><p>A “simple model” in this context is a model where the distribution of parameter values has less entropy (or a model with fewer<br>parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity<br>of a network by forcing its weights to only take small values, which makes the distribution of weight values more “regular”. This is called<br>“weight regularization”, and it is done by adding to the loss function of the network a <em>cost</em> associated with having large weights. This<br>cost comes in two flavors:</p><ul><li>L1 regularization, where the cost added is proportional to the <em>absolute value of the weights coefficients</em> (i.e. to what is called the<br>“L1 norm” of the weights).</li><li>L2 regularization, where the cost added is proportional to the <em>square of the value of the weights coefficients</em> (i.e. to what is called<br>the “L2 norm” of the weights). L2 regularization is also called <em>weight decay</em> in the context of neural networks. Don’t let the different<br>name confuse you: weight decay is mathematically the exact same as L2 regularization.</li></ul><p>In Keras, weight regularization is added by passing <em>weight regularizer instances</em> to layers as keyword arguments. Let’s add L2 weight<br>regularization to our movie review classification network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br>l2_model = models.Sequential()<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>l2_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                 loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                 metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p><code>l2(0.001)</code> means that every coefficient in the weight matrix of the layer will add <code>0.001 * weight_coefficient_value</code> to the total loss of<br>the network. Note that because this penalty is <em>only added at training time</em>, the loss for this network will be much higher at training<br>than at test time.</p><p>Here’s the impact of our L2 regularization penalty:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_hist = l2_model.fit(x_train, y_train,<br>                             epochs=<span class="hljs-number">20</span>,<br>                             batch_size=<span class="hljs-number">512</span>,<br>                             validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4880 - acc: 0.8218 - val_loss: 0.3820 - val_acc: 0.8798Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.9068 - val_loss: 0.3353 - val_acc: 0.8896Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2742 - acc: 0.9185 - val_loss: 0.3306 - val_acc: 0.8898Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2489 - acc: 0.9288 - val_loss: 0.3363 - val_acc: 0.8866Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2420 - acc: 0.9318 - val_loss: 0.3492 - val_acc: 0.8820Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2322 - acc: 0.9359 - val_loss: 0.3567 - val_acc: 0.8788Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2254 - acc: 0.9385 - val_loss: 0.3632 - val_acc: 0.8787Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.2219 - acc: 0.9380 - val_loss: 0.3630 - val_acc: 0.8794Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.2162 - acc: 0.9430 - val_loss: 0.3704 - val_acc: 0.8763Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.2144 - acc: 0.9428 - val_loss: 0.3876 - val_acc: 0.8727Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.2091 - acc: 0.9439 - val_loss: 0.3883 - val_acc: 0.8724Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.2061 - acc: 0.9455 - val_loss: 0.3870 - val_acc: 0.8740Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.2069 - acc: 0.9445 - val_loss: 0.4073 - val_acc: 0.8714Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.2028 - acc: 0.9475 - val_loss: 0.3976 - val_acc: 0.8714Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1998 - acc: 0.9472 - val_loss: 0.4362 - val_acc: 0.8670Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.2019 - acc: 0.9462 - val_loss: 0.4088 - val_acc: 0.8711Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1953 - acc: 0.9495 - val_loss: 0.4185 - val_acc: 0.8698Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1945 - acc: 0.9508 - val_loss: 0.4371 - val_acc: 0.8674Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1934 - acc: 0.9486 - val_loss: 0.4136 - val_acc: 0.8699Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1924 - acc: 0.9504 - val_loss: 0.4200 - val_acc: 0.8704</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_val_loss = l2_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, l2_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;L2-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0p.png" alt="output"></p><p>As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses),<br>even though both models have the same number of parameters.</p><p>As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br><span class="hljs-comment"># L1 regularization</span><br>regularizers.l1(<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># L1 and L2 regularization at the same time</span><br>regularizers.l1_l2(l1=<span class="hljs-number">0.001</span>, l2=<span class="hljs-number">0.001</span>)<br></code></pre></td></tr></table></figure><h2 id="Adding-dropout"><a href="#Adding-dropout" class="headerlink" title="Adding dropout"></a>Adding dropout</h2><p>Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his<br>students at the University of Toronto. Dropout, applied to a layer, consists of randomly “dropping out” (i.e. setting to zero) a number of<br>output features of the layer during training. Let’s say a given layer would normally have returned a vector <code>[0.2, 0.5, 1.3, 0.8, 1.1]</code> for a<br>given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. <code>[0, 0.5, 1.3, 0, 1.1]</code>. The “dropout rate” is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test<br>time, no units are dropped out, and instead the layer’s output values are scaled down by a factor equal to the dropout rate, so as to<br>balance for the fact that more units are active than at training time.</p><p>Consider a Numpy matrix containing the output of a layer, <code>layer_output</code>, of shape <code>(batch_size, features)</code>. At training time, we would be<br>zero-ing out at random a fraction of the values in the matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time: we drop out 50% of the units in the output</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br></code></pre></td></tr></table></figure><p>At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the<br>units):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At test time:</span><br>layer_output *= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is<br>often the way it is implemented in practice:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time:</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br><span class="hljs-comment"># Note that we are scaling *up* rather scaling *down* in this case</span><br>layer_output /= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among<br>other things, by a fraud prevention mechanism used by banks — in his own words: <em>“I went to my bank. The tellers kept changing and I asked<br>one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation<br>between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each<br>example would prevent conspiracies and thus reduce overfitting”</em>.</p><p>The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what<br>Hinton refers to as “conspiracies”), which the network would start memorizing if no noise was present. </p><p>In Keras you can introduce dropout in a network via the <code>Dropout</code> layer, which gets applied to the output of layer right before it, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br></code></pre></td></tr></table></figure><p>Let’s add two <code>Dropout</code> layers in our IMDB network to see how well they do at reducing overfitting:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model = models.Sequential()<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>dpt_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                  loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                  metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_hist = dpt_model.fit(x_train, y_train,<br>                               epochs=<span class="hljs-number">20</span>,<br>                               batch_size=<span class="hljs-number">512</span>,<br>                               validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.6035 - acc: 0.6678 - val_loss: 0.4704 - val_acc: 0.8651Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.4622 - acc: 0.8002 - val_loss: 0.3612 - val_acc: 0.8724Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.3731 - acc: 0.8553 - val_loss: 0.2960 - val_acc: 0.8904Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.8855 - val_loss: 0.2772 - val_acc: 0.8917Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2762 - acc: 0.9033 - val_loss: 0.2803 - val_acc: 0.8889Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2454 - acc: 0.9172 - val_loss: 0.2823 - val_acc: 0.8892Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2178 - acc: 0.9281 - val_loss: 0.2982 - val_acc: 0.8877Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1994 - acc: 0.9351 - val_loss: 0.3101 - val_acc: 0.8875Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1832 - acc: 0.9400 - val_loss: 0.3318 - val_acc: 0.8860Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1692 - acc: 0.9434 - val_loss: 0.3534 - val_acc: 0.8841Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1590 - acc: 0.9483 - val_loss: 0.3689 - val_acc: 0.8830Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1499 - acc: 0.9496 - val_loss: 0.4107 - val_acc: 0.8776Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1405 - acc: 0.9539 - val_loss: 0.4114 - val_acc: 0.8782Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.1333 - acc: 0.9562 - val_loss: 0.4549 - val_acc: 0.8771Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1267 - acc: 0.9572 - val_loss: 0.4579 - val_acc: 0.8800Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.1225 - acc: 0.9580 - val_loss: 0.4843 - val_acc: 0.8772Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1233 - acc: 0.9590 - val_loss: 0.4783 - val_acc: 0.8761Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1212 - acc: 0.9601 - val_loss: 0.5051 - val_acc: 0.8740Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1153 - acc: 0.9618 - val_loss: 0.5451 - val_acc: 0.8747Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1155 - acc: 0.9621 - val_loss: 0.5358 - val_acc: 0.8738</code></pre><p>Let’s plot the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_val_loss = dpt_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, dpt_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Dropout-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_41_0.png" alt="output"></p><p>Again, a clear improvement over the reference network.</p><p>To recap: here the most common ways to prevent overfitting in neural networks:</p><ul><li>Getting more training data.</li><li>Reducing the capacity of the network.</li><li>Adding weight regularization.</li><li>Adding dropout.</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-4</title>
    <link href="/2021/04/22/DL-with-python-4/"/>
    <url>/2021/04/22/DL-with-python-4/</url>
    
    <content type="html"><![CDATA[<p>3.7-predicting-house-prices</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Predicting-house-prices-a-regression-example"><a href="#Predicting-house-prices-a-regression-example" class="headerlink" title="Predicting house prices: a regression example"></a>Predicting house prices: a regression example</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In our two previous examples, we were considering classification problems, where the goal was to predict a single discrete label of an<br>input data point. Another common type of machine learning problem is “regression”, which consists of predicting a continuous value instead<br>of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a<br>software project will take to complete, given its specifications.</p><p>Do not mix up “regression” with the algorithm “logistic regression”: confusingly, “logistic regression” is not a regression algorithm,<br>it is a classification algorithm.</p><h2 id="The-Boston-Housing-Price-dataset"><a href="#The-Boston-Housing-Price-dataset" class="headerlink" title="The Boston Housing Price dataset"></a>The Boston Housing Price dataset</h2><p>We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the<br>suburb at the time, such as the crime rate, the local property tax rate, etc.</p><p>The dataset we will be using has another interesting difference from our two previous examples: it has very few data points, only 506 in<br>total, split between 404 training samples and 102 test samples, and each “feature” in the input data (e.g. the crime rate is a feature) has<br>a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12,<br>others between 0 and 100…</p><p>Let’s take a look at the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> boston_housing<br><br>(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data.shape<br></code></pre></td></tr></table></figure><pre><code>(404, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data.shape<br></code></pre></td></tr></table></figure><pre><code>(102, 13)</code></pre><p>As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as<br>follow:</p><ol><li>Per capita crime rate.</li><li>Proportion of residential land zoned for lots over 25,000 square feet.</li><li>Proportion of non-retail business acres per town.</li><li>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</li><li>Nitric oxides concentration (parts per 10 million).</li><li>Average number of rooms per dwelling.</li><li>Proportion of owner-occupied units built prior to 1940.</li><li>Weighted distances to five Boston employment centres.</li><li>Index of accessibility to radial highways.</li><li>Full-value property-tax rate per $10,000.</li><li>Pupil-teacher ratio by town.</li><li>1000 <em> (Bk - 0.63) *</em> 2 where Bk is the proportion of Black people by town.</li><li>% lower status of the population.</li></ol><p>The targets are the median values of owner-occupied homes, in thousands of dollars:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_targets<br></code></pre></td></tr></table></figure><pre><code>array([ 15.2,  42.3,  50. ,  21.1,  17.7,  18.5,  11.3,  15.6,  15.6,        14.4,  12.1,  17.9,  23.1,  19.9,  15.7,   8.8,  50. ,  22.5,        24.1,  27.5,  10.9,  30.8,  32.9,  24. ,  18.5,  13.3,  22.9,        34.7,  16.6,  17.5,  22.3,  16.1,  14.9,  23.1,  34.9,  25. ,        13.9,  13.1,  20.4,  20. ,  15.2,  24.7,  22.2,  16.7,  12.7,        15.6,  18.4,  21. ,  30.1,  15.1,  18.7,   9.6,  31.5,  24.8,        19.1,  22. ,  14.5,  11. ,  32. ,  29.4,  20.3,  24.4,  14.6,        19.5,  14.1,  14.3,  15.6,  10.5,   6.3,  19.3,  19.3,  13.4,        36.4,  17.8,  13.5,  16.5,   8.3,  14.3,  16. ,  13.4,  28.6,        43.5,  20.2,  22. ,  23. ,  20.7,  12.5,  48.5,  14.6,  13.4,        23.7,  50. ,  21.7,  39.8,  38.7,  22.2,  34.9,  22.5,  31.1,        28.7,  46. ,  41.7,  21. ,  26.6,  15. ,  24.4,  13.3,  21.2,        11.7,  21.7,  19.4,  50. ,  22.8,  19.7,  24.7,  36.2,  14.2,        18.9,  18.3,  20.6,  24.6,  18.2,   8.7,  44. ,  10.4,  13.2,        21.2,  37. ,  30.7,  22.9,  20. ,  19.3,  31.7,  32. ,  23.1,        18.8,  10.9,  50. ,  19.6,   5. ,  14.4,  19.8,  13.8,  19.6,        23.9,  24.5,  25. ,  19.9,  17.2,  24.6,  13.5,  26.6,  21.4,        11.9,  22.6,  19.6,   8.5,  23.7,  23.1,  22.4,  20.5,  23.6,        18.4,  35.2,  23.1,  27.9,  20.6,  23.7,  28. ,  13.6,  27.1,        23.6,  20.6,  18.2,  21.7,  17.1,   8.4,  25.3,  13.8,  22.2,        18.4,  20.7,  31.6,  30.5,  20.3,   8.8,  19.2,  19.4,  23.1,        23. ,  14.8,  48.8,  22.6,  33.4,  21.1,  13.6,  32.2,  13.1,        23.4,  18.9,  23.9,  11.8,  23.3,  22.8,  19.6,  16.7,  13.4,        22.2,  20.4,  21.8,  26.4,  14.9,  24.1,  23.8,  12.3,  29.1,        21. ,  19.5,  23.3,  23.8,  17.8,  11.5,  21.7,  19.9,  25. ,        33.4,  28.5,  21.4,  24.3,  27.5,  33.1,  16.2,  23.3,  48.3,        22.9,  22.8,  13.1,  12.7,  22.6,  15. ,  15.3,  10.5,  24. ,        18.5,  21.7,  19.5,  33.2,  23.2,   5. ,  19.1,  12.7,  22.3,        10.2,  13.9,  16.3,  17. ,  20.1,  29.9,  17.2,  37.3,  45.4,        17.8,  23.2,  29. ,  22. ,  18. ,  17.4,  34.6,  20.1,  25. ,        15.6,  24.8,  28.2,  21.2,  21.4,  23.8,  31. ,  26.2,  17.4,        37.9,  17.5,  20. ,   8.3,  23.9,   8.4,  13.8,   7.2,  11.7,        17.1,  21.6,  50. ,  16.1,  20.4,  20.6,  21.4,  20.6,  36.5,         8.5,  24.8,  10.8,  21.9,  17.3,  18.9,  36.2,  14.9,  18.2,        33.3,  21.8,  19.7,  31.6,  24.8,  19.4,  22.8,   7.5,  44.8,        16.8,  18.7,  50. ,  50. ,  19.5,  20.1,  50. ,  17.2,  20.8,        19.3,  41.3,  20.4,  20.5,  13.8,  16.5,  23.9,  20.6,  31.5,        23.3,  16.8,  14. ,  33.8,  36.1,  12.8,  18.3,  18.7,  19.1,        29. ,  30.1,  50. ,  50. ,  22. ,  11.9,  37.6,  50. ,  22.7,        20.8,  23.5,  27.9,  50. ,  19.3,  23.9,  22.6,  15.2,  21.7,        19.2,  43.8,  20.3,  33.2,  19.9,  22.5,  32.7,  22. ,  17.1,        19. ,  15. ,  16.1,  25.1,  23.7,  28.7,  37.2,  22.6,  16.4,        25. ,  29.8,  22.1,  17.4,  18.1,  30.3,  17.5,  24.7,  12.6,        26.5,  28.7,  13.3,  10.4,  24.4,  23. ,  20. ,  17.8,   7. ,        11.8,  24.4,  13.8,  19.4,  25.2,  19.4,  19.4,  29.1])</code></pre><p>The prices are typically between $10,000 and $50,000. If that sounds cheap, remember this was the mid-1970s, and these prices are not<br>inflation-adjusted.</p><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to<br>automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal<br>with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we<br>will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a<br>unit standard deviation. This is easily done in Numpy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">mean = train_data.mean(axis=<span class="hljs-number">0</span>)<br>train_data -= mean<br>std = train_data.std(axis=<span class="hljs-number">0</span>)<br>train_data /= std<br><br>test_data -= mean<br>test_data /= std<br></code></pre></td></tr></table></figure><p>Note that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our<br>workflow any quantity computed on the test data, even for something as simple as data normalization.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Because so few samples are available, we will be using a very small network with two<br>hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using<br>a small network is one way to mitigate overfitting.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>():</span><br>    <span class="hljs-comment"># Because we will need to instantiate</span><br>    <span class="hljs-comment"># the same model multiple times,</span><br>    <span class="hljs-comment"># we use a function to construct it.</span><br>    model = models.Sequential()<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                           input_shape=(train_data.shape[<span class="hljs-number">1</span>],)))<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>    model.add(layers.Dense(<span class="hljs-number">1</span>))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;mse&#x27;</span>, metrics=[<span class="hljs-string">&#x27;mae&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>Our network ends with a single unit, and no activation (i.e. it will be linear layer).<br>This is a typical setup for scalar regression (i.e. regression where we are trying to predict a single continuous value).<br>Applying an activation function would constrain the range that the output can take; for instance if<br>we applied a <code>sigmoid</code> activation function to our last layer, the network could only learn to predict values between 0 and 1. Here, because<br>the last layer is purely linear, the network is free to learn to predict values in any range.</p><p>Note that we are compiling the network with the <code>mse</code> loss function — Mean Squared Error, the square of the difference between the<br>predictions and the targets, a widely used loss function for regression problems.</p><p>We are also monitoring a new metric during training: <code>mae</code>. This stands for Mean Absolute Error. It is simply the absolute value of the<br>difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by<br>$500 on average.</p><h2 id="Validating-our-approach-using-K-fold-validation"><a href="#Validating-our-approach-using-K-fold-validation" class="headerlink" title="Validating our approach using K-fold validation"></a>Validating our approach using K-fold validation</h2><p>To evaluate our network while we keep adjusting its parameters (such as the number of epochs used for training), we could simply split the<br>data into a training set and a validation set, as we were doing in our previous examples. However, because we have so few data points, the<br>validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot<br>depending on <em>which</em> data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a<br>high <em>variance</em> with regard to the validation split. This would prevent us from reliably evaluating our model.</p><p>The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions<br>(typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining<br>partition. The validation score for the model used would then be the average of the K validation scores obtained.</p><p>In terms of code, this is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>k = <span class="hljs-number">4</span><br>num_val_samples = <span class="hljs-built_in">len</span>(train_data) // k<br>num_epochs = <span class="hljs-number">100</span><br>all_scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    model.fit(partial_train_data, partial_train_targets,<br>              epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Evaluate the model on the validation data</span><br>    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="hljs-number">0</span>)<br>    all_scores.append(val_mae)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">all_scores<br></code></pre></td></tr></table></figure><pre><code>[2.0750808349930412, 2.117215852926273, 2.9140411863232605, 2.4288365227161068]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean(all_scores)<br></code></pre></td></tr></table></figure><pre><code>2.3837935992396706</code></pre><p>As you can notice, the different runs do indeed show rather different validation scores, from 2.1 to 2.9. Their average (2.4) is a much more<br>reliable metric than any single of these scores — that’s the entire point of K-fold cross-validation. In this case, we are off by $2,400 on<br>average, which is still significant considering that the prices range from $10,000 to $50,000. </p><p>Let’s try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop<br>to save the per-epoch validation score log:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><br><span class="hljs-comment"># Some memory clean-up</span><br>K.clear_session()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">500</span><br>all_mae_histories = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    history = model.fit(partial_train_data, partial_train_targets,<br>                        validation_data=(val_data, val_targets),<br>                        epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    mae_history = history.history[<span class="hljs-string">&#x27;val_mean_absolute_error&#x27;</span>]<br>    all_mae_histories.append(mae_history)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><p>We can then compute the average of the per-epoch MAE scores for all folds:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">average_mae_history = [<br>    np.mean([x[i] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_mae_histories]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs)]<br></code></pre></td></tr></table></figure><p>Let’s plot this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(average_mae_history) + <span class="hljs-number">1</span>), average_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0.png" alt="output"></p><p>It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let’s:</p><ul><li>Omit the first 10 data points, which are on a different scale from the rest of the curve.</li><li>Replace each point with an exponential moving average of the previous points, to obtain a smooth curve.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smooth_curve</span>(<span class="hljs-params">points, factor=<span class="hljs-number">0.9</span></span>):</span><br>  smoothed_points = []<br>  <span class="hljs-keyword">for</span> point <span class="hljs-keyword">in</span> points:<br>    <span class="hljs-keyword">if</span> smoothed_points:<br>      previous = smoothed_points[-<span class="hljs-number">1</span>]<br>      smoothed_points.append(previous * factor + point * (<span class="hljs-number">1</span> - factor))<br>    <span class="hljs-keyword">else</span>:<br>      smoothed_points.append(point)<br>  <span class="hljs-keyword">return</span> smoothed_points<br><br>smooth_mae_history = smooth_curve(average_mae_history[<span class="hljs-number">10</span>:])<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(smooth_mae_history) + <span class="hljs-number">1</span>), smooth_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_28_0.png" alt="output"></p><p>According to this plot, it seems that validation MAE stops improving significantly after 80 epochs. Past that point, we start overfitting.</p><p>Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we<br>can train a final “production” model on all of the training data, with the best parameters, then look at its performance on the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Get a fresh, compiled model.</span><br>model = build_model()<br><span class="hljs-comment"># Train it on the entirety of the data.</span><br>model.fit(train_data, train_targets,<br>          epochs=<span class="hljs-number">80</span>, batch_size=<span class="hljs-number">16</span>, verbose=<span class="hljs-number">0</span>)<br>test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)<br></code></pre></td></tr></table></figure><pre><code> 32/102 [========&gt;.....................] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_mae_score<br></code></pre></td></tr></table></figure><pre><code>2.5532484335057877</code></pre><p>We are still off by about $2,550.</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>Regression is done using different loss functions from classification; Mean Squared Error (MSE) is a commonly used loss function for<br>regression.</li><li>Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally the concept of “accuracy”<br>does not apply for regression. A common regression metric is Mean Absolute Error (MAE).</li><li>When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.</li><li>When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.</li><li>When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two),<br>in order to avoid severe overfitting.</li></ul><p>This example concludes our series of three introductory practical examples. You are now able to handle common types of problems with vector data input:</p><ul><li>Binary (2-class) classification.</li><li>Multi-class, single-label classification.</li><li>Scalar regression.</li></ul><p>In the next chapter, you will acquire a more formal understanding of some of the concepts you have encountered in these first examples,<br>such as data preprocessing, model evaluation, and overfitting.</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-3</title>
    <link href="/2021/04/22/DL-with-python-3/"/>
    <url>/2021/04/22/DL-with-python-3/</url>
    
    <content type="html"><![CDATA[<p>3.6-classifying-newswires</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Classifying-newswires-a-multi-class-classification-example"><a href="#Classifying-newswires-a-multi-class-classification-example" class="headerlink" title="Classifying newswires: a multi-class classification example"></a>Classifying newswires: a multi-class classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network.<br>But what happens when you have more than two classes? </p><p>In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many<br>classes, this problem is an instance of “multi-class classification”, and since each data point should be classified into only one<br>category, the problem is more specifically an instance of “single-label, multi-class classification”. If each data point could have<br>belonged to multiple categories (in our case, topics) then we would be facing a “multi-label, multi-class classification” problem.</p><h2 id="The-Reuters-dataset"><a href="#The-Reuters-dataset" class="headerlink" title="The Reuters dataset"></a>The Reuters dataset</h2><p>We will be working with the <em>Reuters dataset</em>, a set of short newswires and their topics, published by Reuters in 1986. It’s a very simple,<br>widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each<br>topic has at least 10 examples in the training set.</p><p>Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s take a look right away:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> reuters<br><br>(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>Like with the IMDB dataset, the argument <code>num_words=10000</code> restricts the data to the 10,000 most frequently occurring words found in the<br>data.</p><p>We have 8,982 training examples and 2,246 test examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_data)<br></code></pre></td></tr></table></figure><pre><code>8982</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_data)<br></code></pre></td></tr></table></figure><pre><code>2246</code></pre><p>As with the IMDB reviews, each example is a list of integers (word indices):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]</code></pre><p>Here’s how you can decode it back to words, in case you are curious:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">word_index = reuters.get_word_index()<br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># Note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_newswire = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_newswire<br></code></pre></td></tr></table></figure><pre><code>&#39;? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3&#39;</code></pre><p>The label associated with an example is an integer between 0 and 45: a topic index.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We can vectorize the data with the exact same code as in our previous example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a “one-hot”<br>encoding. One-hot encoding is a widely used format for categorical data, also called “categorical encoding”.<br>For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1.<br>In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_one_hot</span>(<span class="hljs-params">labels, dimension=<span class="hljs-number">46</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(labels), dimension))<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        results[i, label] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training labels</span><br>one_hot_train_labels = to_one_hot(train_labels)<br><span class="hljs-comment"># Our vectorized test labels</span><br>one_hot_test_labels = to_one_hot(test_labels)<br></code></pre></td></tr></table></figure><p>Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils.np_utils <span class="hljs-keyword">import</span> to_categorical<br><br>one_hot_train_labels = to_categorical(train_labels)<br>one_hot_test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to<br>classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the<br>dimensionality of the output space is much larger. </p><p>In a stack of <code>Dense</code> layers like what we were using, each layer can only access information present in the output of the previous layer.<br>If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each<br>layer can potentially become an “information bottleneck”. In our previous example, we were using 16-dimensional intermediate layers, but a<br>16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks,<br>permanently dropping relevant information.</p><p>For this reason we will use larger layers. Let’s go with 64 units:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>There are two other things you should note about this architecture:</p><ul><li>We are ending the network with a <code>Dense</code> layer of size 46. This means that for each input sample, our network will output a<br>46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.</li><li>The last layer uses a <code>softmax</code> activation. You have already seen this pattern in the MNIST example. It means that the network will<br>output a <em>probability distribution</em> over the 46 different output classes, i.e. for every input sample, the network will produce a<br>46-dimensional output vector where <code>output[i]</code> is the probability that the sample belongs to class <code>i</code>. The 46 scores will sum to 1.</li></ul><p>The best loss function to use in this case is <code>categorical_crossentropy</code>. It measures the distance between two probability distributions:<br>in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the<br>distance between these two distributions, we train our network to output something as close as possible to the true labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>Let’s set apart 1,000 samples in our training data to use as a validation set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">1000</span>]<br>partial_x_train = x_train[<span class="hljs-number">1000</span>:]<br><br>y_val = one_hot_train_labels[:<span class="hljs-number">1000</span>]<br>partial_y_train = one_hot_train_labels[<span class="hljs-number">1000</span>:]<br></code></pre></td></tr></table></figure><p>Now let’s train our network for 20 epochs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 1s - loss: 2.5241 - acc: 0.4952 - val_loss: 1.7263 - val_acc: 0.6100Epoch 2/207982/7982 [==============================] - 0s - loss: 1.4500 - acc: 0.6854 - val_loss: 1.3478 - val_acc: 0.7070Epoch 3/207982/7982 [==============================] - 0s - loss: 1.0979 - acc: 0.7643 - val_loss: 1.1736 - val_acc: 0.7460Epoch 4/207982/7982 [==============================] - 0s - loss: 0.8723 - acc: 0.8178 - val_loss: 1.0880 - val_acc: 0.7490Epoch 5/207982/7982 [==============================] - 0s - loss: 0.7045 - acc: 0.8477 - val_loss: 0.9822 - val_acc: 0.7760Epoch 6/207982/7982 [==============================] - 0s - loss: 0.5660 - acc: 0.8792 - val_loss: 0.9379 - val_acc: 0.8030Epoch 7/207982/7982 [==============================] - 0s - loss: 0.4569 - acc: 0.9037 - val_loss: 0.9039 - val_acc: 0.8050Epoch 8/207982/7982 [==============================] - 0s - loss: 0.3668 - acc: 0.9238 - val_loss: 0.9279 - val_acc: 0.7890Epoch 9/207982/7982 [==============================] - 0s - loss: 0.3000 - acc: 0.9326 - val_loss: 0.8835 - val_acc: 0.8070Epoch 10/207982/7982 [==============================] - 0s - loss: 0.2505 - acc: 0.9434 - val_loss: 0.8967 - val_acc: 0.8150Epoch 11/207982/7982 [==============================] - 0s - loss: 0.2155 - acc: 0.9473 - val_loss: 0.9080 - val_acc: 0.8110Epoch 12/207982/7982 [==============================] - 0s - loss: 0.1853 - acc: 0.9506 - val_loss: 0.9025 - val_acc: 0.8140Epoch 13/207982/7982 [==============================] - 0s - loss: 0.1680 - acc: 0.9524 - val_loss: 0.9268 - val_acc: 0.8100Epoch 14/207982/7982 [==============================] - 0s - loss: 0.1512 - acc: 0.9562 - val_loss: 0.9500 - val_acc: 0.8130Epoch 15/207982/7982 [==============================] - 0s - loss: 0.1371 - acc: 0.9559 - val_loss: 0.9621 - val_acc: 0.8090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.1306 - acc: 0.9553 - val_loss: 1.0152 - val_acc: 0.8050Epoch 17/207982/7982 [==============================] - 0s - loss: 0.1210 - acc: 0.9575 - val_loss: 1.0262 - val_acc: 0.8010Epoch 18/207982/7982 [==============================] - 0s - loss: 0.1185 - acc: 0.9570 - val_loss: 1.0354 - val_acc: 0.8040Epoch 19/207982/7982 [==============================] - 0s - loss: 0.1128 - acc: 0.9598 - val_loss: 1.0841 - val_acc: 0.8010Epoch 20/207982/7982 [==============================] - 0s - loss: 0.1097 - acc: 0.9594 - val_loss: 1.0707 - val_acc: 0.8040</code></pre><p>Let’s display its loss and accuracy curves:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(loss) + <span class="hljs-number">1</span>)<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_29_0.png" alt="output"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_30_0.png" alt="output"></p><p>It seems that the network starts overfitting after 8 epochs. Let’s train a new network from scratch for 8 epochs, then let’s evaluate it on<br>the test set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">8</span>,<br>          batch_size=<span class="hljs-number">512</span>,<br>          validation_data=(x_val, y_val))<br>results = model.evaluate(x_test, one_hot_test_labels)<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/87982/7982 [==============================] - 0s - loss: 2.6118 - acc: 0.4667 - val_loss: 1.7207 - val_acc: 0.6360Epoch 2/87982/7982 [==============================] - 0s - loss: 1.3998 - acc: 0.7107 - val_loss: 1.2645 - val_acc: 0.7360Epoch 3/87982/7982 [==============================] - 0s - loss: 1.0343 - acc: 0.7839 - val_loss: 1.0994 - val_acc: 0.7700Epoch 4/87982/7982 [==============================] - 0s - loss: 0.8114 - acc: 0.8329 - val_loss: 1.0252 - val_acc: 0.7820Epoch 5/87982/7982 [==============================] - 0s - loss: 0.6466 - acc: 0.8628 - val_loss: 0.9536 - val_acc: 0.8070Epoch 6/87982/7982 [==============================] - 0s - loss: 0.5271 - acc: 0.8894 - val_loss: 0.9187 - val_acc: 0.8110Epoch 7/87982/7982 [==============================] - 0s - loss: 0.4193 - acc: 0.9126 - val_loss: 0.9051 - val_acc: 0.8120Epoch 8/87982/7982 [==============================] - 0s - loss: 0.3478 - acc: 0.9258 - val_loss: 0.8891 - val_acc: 0.81601952/2246 [=========================&gt;....] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure><pre><code>[0.98764628548762257, 0.77693677651807869]</code></pre><p>Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier<br>would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><br>test_labels_copy = copy.copy(test_labels)<br>np.random.shuffle(test_labels_copy)<br><span class="hljs-built_in">float</span>(np.<span class="hljs-built_in">sum</span>(np.array(test_labels) == np.array(test_labels_copy))) / <span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>0.18477292965271594</code></pre><h2 id="Generating-predictions-on-new-data"><a href="#Generating-predictions-on-new-data" class="headerlink" title="Generating predictions on new data"></a>Generating predictions on new data</h2><p>We can verify that the <code>predict</code> method of our model instance returns a probability distribution over all 46 topics. Let’s generate topic<br>predictions for all of the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions = model.predict(x_test)<br></code></pre></td></tr></table></figure><p>Each entry in <code>predictions</code> is a vector of length 46:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions[<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></table></figure><pre><code>(46,)</code></pre><p>The coefficients in this vector sum to 1:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.<span class="hljs-built_in">sum</span>(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>0.99999994</code></pre><p>The largest entry is the predicted class, i.e. the class with the highest probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.argmax(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="A-different-way-to-handle-the-labels-and-the-loss"><a href="#A-different-way-to-handle-the-labels-and-the-loss" class="headerlink" title="A different way to handle the labels and the loss"></a>A different way to handle the labels and the loss</h2><p>We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y_train = np.array(train_labels)<br>y_test = np.array(test_labels)<br></code></pre></td></tr></table></figure><p>The only thing it would change is the choice of the loss function. Our previous loss, <code>categorical_crossentropy</code>, expects the labels to<br>follow a categorical encoding. With integer labels, we should use <code>sparse_categorical_crossentropy</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>This new loss function is still mathematically the same as <code>categorical_crossentropy</code>; it just has a different interface.</p><h2 id="On-the-importance-of-having-sufficiently-large-intermediate-layers"><a href="#On-the-importance-of-having-sufficiently-large-intermediate-layers" class="headerlink" title="On the importance of having sufficiently large intermediate layers"></a>On the importance of having sufficiently large intermediate layers</h2><p>We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden<br>units. Now let’s try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than<br>46-dimensional, e.g. 4-dimensional.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">20</span>,<br>          batch_size=<span class="hljs-number">128</span>,<br>          validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740Epoch 2/207982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990Epoch 3/207982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390Epoch 4/207982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570Epoch 5/207982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700Epoch 6/207982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800Epoch 7/207982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810Epoch 8/207982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960Epoch 9/207982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050Epoch 10/207982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020Epoch 11/207982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020Epoch 12/207982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080Epoch 13/207982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110Epoch 14/207982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080Epoch 15/207982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080Epoch 17/207982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060Epoch 18/207982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950Epoch 19/207982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930Epoch 20/207982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060&lt;keras.callbacks.History at 0x7f8ce7cdb9b0&gt;</code></pre><p>Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to<br>compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is<br>too low-dimensional. The network is able to cram <em>most</em> of the necessary information into these 8-dimensional representations, but not all<br>of it.</p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>Try using larger or smaller layers: 32 units, 128 units…</li><li>We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers.</li></ul><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>If you are trying to classify data points between N classes, your network should end with a <code>Dense</code> layer of size N.</li><li>In a single-label, multi-class classification problem, your network should end with a <code>softmax</code> activation, so that it will output a<br>probability distribution over the N output classes.</li><li><em>Categorical crossentropy</em> is almost always the loss function you should use for such problems. It minimizes the distance between the<br>probability distributions output by the network, and the true distribution of the targets.</li><li>There are two ways to handle labels in multi-class classification:<br>  <strong> Encoding the labels via “categorical encoding” (also known as “one-hot encoding”) and using <code>categorical_crossentropy</code> as your loss<br>function.  </strong> Encoding the labels as integers and using the <code>sparse_categorical_crossentropy</code> loss function.</li><li>If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having<br>intermediate layers that are too small.</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-2</title>
    <link href="/2021/04/22/DL-with-python-2/"/>
    <url>/2021/04/22/DL-with-python-2/</url>
    
    <content type="html"><![CDATA[<p>3.5-classifying-movie-reviews</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="Classifying-movie-reviews-a-binary-classification-example"><a href="#Classifying-movie-reviews-a-binary-classification-example" class="headerlink" title="Classifying movie reviews: a binary classification example"></a>Classifying movie reviews: a binary classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we<br>will learn to classify movie reviews into “positive” reviews and “negative” reviews, just based on the text content of the reviews.</p><h2 id="The-IMDB-dataset"><a href="#The-IMDB-dataset" class="headerlink" title="The IMDB dataset"></a>The IMDB dataset</h2><p>We’ll be working with “IMDB dataset”, a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000<br>reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.</p><p>Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to<br>train it! Just because a model performs well on its training data doesn’t mean that it will perform well on data it has never seen, and<br>what you actually care about is your model’s performance on new data (since you already know the labels of your training data — obviously<br>you don’t need your model to predict those). For instance, it is possible that your model could end up merely <em>memorizing</em> a mapping between<br>your training samples and their targets — which would be completely useless for the task of predicting targets for data never seen before.<br>We will go over this point in much more detail in the next chapter.</p><p>Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words)<br>have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.</p><p>The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>The argument <code>num_words=10000</code> means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words<br>will be discarded. This allows us to work with vector data of manageable size.</p><p>The variables <code>train_data</code> and <code>test_data</code> are lists of reviews, each review being a list of word indices (encoding a sequence of words).<br><code>train_labels</code> and <code>test_labels</code> are lists of 0s and 1s, where 0 stands for “negative” and 1 stands for “positive”:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>1</code></pre><p>Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">max</span>([<span class="hljs-built_in">max</span>(sequence) <span class="hljs-keyword">for</span> sequence <span class="hljs-keyword">in</span> train_data])<br></code></pre></td></tr></table></figure><pre><code>9999</code></pre><p>For kicks, here’s how you can quickly decode one of these reviews back to English words:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># word_index is a dictionary mapping words to an integer index</span><br>word_index = imdb.get_word_index()<br><span class="hljs-comment"># We reverse it, mapping integer indices to words</span><br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># We decode the review; note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_review = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_review<br></code></pre></td></tr></table></figure><pre><code>&quot;? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all&quot;</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:</p><ul><li>We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape <code>(samples, word_indices)</code>,<br>then use as first layer in our network a layer capable of handling such integer tensors (the <code>Embedding</code> layer, which we will cover in<br>detail later in the book).</li><li>We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence<br><code>[3, 5]</code> into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as<br>first layer in our network a <code>Dense</code> layer, capable of handling floating point vector data.</li></ul><p>We will go with the latter solution. Let’s vectorize our data, which we will do manually for maximum clarity:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>Here’s what our samples look like now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>array([0., 1., 1., ..., 0., 0., 0.])</code></pre><p>We should also vectorize our labels, which is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Now our data is ready to be fed into a neural network.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of<br>network that performs well on such a problem would be a simple stack of fully-connected (<code>Dense</code>) layers with <code>relu</code> activations: <code>Dense(16, activation=&#39;relu&#39;)</code></p><p>The argument being passed to each <code>Dense</code> layer (16) is the number of “hidden units” of the layer. What’s a hidden unit? It’s a dimension<br>in the representation space of the layer. You may remember from the previous chapter that each such <code>Dense</code> layer with a <code>relu</code> activation implements<br>the following chain of tensor operations:</p><p><code>output = relu(dot(W, input) + b)</code></p><p>Having 16 hidden units means that the weight matrix <code>W</code> will have shape <code>(input_dimension, 16)</code>, i.e. the dot product with <code>W</code> will project the<br>input data onto a 16-dimensional representation space (and then we would add the bias vector <code>b</code> and apply the <code>relu</code> operation). You can<br>intuitively understand the dimensionality of your representation space as “how much freedom you are allowing the network to have when<br>learning internal representations”. Having more hidden units (a higher-dimensional representation space) allows your network to learn more<br>complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that<br>will improve performance on the training data but not on the test data).</p><p>There are two key architecture decisions to be made about such stack of dense layers:</p><ul><li>How many layers to use.</li><li>How many “hidden units” to chose for each layer.</li></ul><p>In the next chapter, you will learn formal principles to guide you in making these choices.<br>For the time being, you will have to trust us with the following architecture choice:<br>two intermediate layers with 16 hidden units each,<br>and a third layer which will output the scalar prediction regarding the sentiment of the current review.<br>The intermediate layers will use <code>relu</code> as their “activation function”,<br>and the final layer will use a sigmoid activation so as to output a probability<br>(a score between 0 and 1, indicating how likely the sample is to have the target “1”, i.e. how likely the review is to be positive).<br>A <code>relu</code> (rectified linear unit) is a function meant to zero-out negative values,<br>while a sigmoid “squashes” arbitrary values into the <code>[0, 1]</code> interval, thus outputting something that can be interpreted as a probability.</p><p>Here’s what our network looks like:</p><p><img src="https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png" alt="3-layer network"></p><p>And here’s the Keras implementation, very similar to the MNIST example you saw previously:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network<br>is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the <code>binary_crossentropy</code> loss.<br>It isn’t the only viable choice: you could use, for instance, <code>mean_squared_error</code>. But crossentropy is usually the best choice when you<br>are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the “distance”<br>between probability distributions, or in our case, between the ground-truth distribution and our predictions.</p><p>Here’s the step where we configure our model with the <code>rmsprop</code> optimizer and the <code>binary_crossentropy</code> loss function. Note that we will<br>also monitor accuracy during training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>We are passing our optimizer, loss function and metrics as strings, which is possible because <code>rmsprop</code>, <code>binary_crossentropy</code> and<br><code>accuracy</code> are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss<br>function or metric function. This former can be done by passing an optimizer class instance as the <code>optimizer</code> argument:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>The latter can be done by passing function objects as the <code>loss</code> or <code>metrics</code> arguments:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> losses<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> metrics<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=losses.binary_crossentropy,<br>              metrics=[metrics.binary_accuracy])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a “validation set” by<br>setting apart 10,000 samples from the original training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">10000</span>]<br>partial_x_train = x_train[<span class="hljs-number">10000</span>:]<br><br>y_val = y_train[:<span class="hljs-number">10000</span>]<br>partial_y_train = y_train[<span class="hljs-number">10000</span>:]<br></code></pre></td></tr></table></figure><p>We will now train our model for 20 epochs (20 iterations over all samples in the <code>x_train</code> and <code>y_train</code> tensors), in mini-batches of 512<br>samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the<br>validation data as the <code>validation_data</code> argument:</p><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-20-90a50ee7d258&gt; in &lt;module&gt;----&gt; 1 history = model.fit(partial_x_train,      2                     partial_y_train,      3                     epochs=20,      4                     batch_size=512,      5                     validation_data=(x_val, y_val))D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 572. MiB for an array with shape (15000, 10000) and data type float32</code></pre><p>On CPU, this will take less than two seconds per epoch — training is over in 20 seconds. At the end of every epoch, there is a slight pause<br>as the model computes its loss and accuracy on the 10,000 samples of the validation data.</p><p>Note that the call to <code>model.fit()</code> returns a <code>History</code> object. This object has a member <code>history</code>, which is a dictionary containing data<br>about everything that happened during training. Let’s take a look at it:</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">history_dict = history.history<br>history_dict.keys()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-21-fc51a3f187a7&gt; in &lt;module&gt;----&gt; 1 history_dict = history.history      2 history_dict.keys()NameError: name &#39;history&#39; is not defined</code></pre><p>It contains 4 entries: one per metric that was being monitored, during training and during validation. Let’s use Matplotlib to plot the<br>training and validation loss side by side, as well as the training and validation accuracy:</p><h2 id="-2"><a href="#-2" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(acc) + <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br><span class="hljs-comment"># b is for &quot;solid blue line&quot;</span><br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-22-fa6555deaae9&gt; in &lt;module&gt;      1 import matplotlib.pyplot as plt      2----&gt; 3 acc = history.history[&#39;acc&#39;]      4 val_acc = history.history[&#39;val_acc&#39;]      5 loss = history.history[&#39;loss&#39;]NameError: name &#39;history&#39; is not defined</code></pre><h2 id="-3"><a href="#-3" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br>acc_values = history_dict[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc_values = history_dict[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-23-cc3500a51e0d&gt; in &lt;module&gt;      1 plt.clf()   # clear figure----&gt; 2 acc_values = history_dict[&#39;acc&#39;]      3 val_acc_values = history_dict[&#39;val_acc&#39;]      4      5 plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)NameError: name &#39;history_dict&#39; is not defined&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary<br>slightly due to a different random initialization of your network.</p><p>As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That’s what you would<br>expect when running gradient descent optimization — the quantity you are trying to minimize should get lower with every iteration. But that<br>isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning<br>against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen<br>before. In precise terms, what you are seeing is “overfitting”: after the second epoch, we are over-optimizing on the training data, and we<br>ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.</p><p>In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can<br>leverage to mitigate overfitting, which we will cover in the next chapter.</p><p>Let’s train a new network from scratch for four epochs, then evaluate it on our test data:</p><h2 id="-4"><a href="#-4" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>model.fit(x_train, y_train, epochs=<span class="hljs-number">4</span>, batch_size=<span class="hljs-number">512</span>)<br>results = model.evaluate(x_test, y_test)<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-24-82373714f0fe&gt; in &lt;module&gt;      8               metrics=[&#39;accuracy&#39;])      9---&gt; 10 model.fit(x_train, y_train, epochs=4, batch_size=512)     11 results = model.evaluate(x_test, y_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><h2 id="-5"><a href="#-5" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-25-100f62972f2f&gt; in &lt;module&gt;----&gt; 1 resultsNameError: name &#39;results&#39; is not defined</code></pre><p>Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.</p><h2 id="Using-a-trained-network-to-generate-predictions-on-new-data"><a href="#Using-a-trained-network-to-generate-predictions-on-new-data" class="headerlink" title="Using a trained network to generate predictions on new data"></a>Using a trained network to generate predictions on new data</h2><p>After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive<br>by using the <code>predict</code> method:</p><h2 id="-6"><a href="#-6" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.predict(x_test)<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-26-a556f0421074&gt; in &lt;module&gt;----&gt; 1 model.predict(x_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)   1596                         &#39;. Consider setting it to AutoShardPolicy.DATA.&#39;)   1597-&gt; 1598       data_handler = data_adapter.DataHandler(   1599           x=x,   1600           batch_size=batch_size,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><p>As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). </p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.</li><li>Try to use layers with more hidden units or less hidden units: 32 units, 64 units…</li><li>Try to use the <code>mse</code> loss function instead of <code>binary_crossentropy</code>.</li><li>Try to use the <code>tanh</code> activation (an activation that was popular in the early days of neural networks) instead of <code>relu</code>.</li></ul><p>These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be<br>improved!</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Here’s what you should take away from this example:</p><ul><li>There’s usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it — as tensors — into a neural<br>network. In the case of sequences of words, they can be encoded as binary vectors — but there are other encoding options too.</li><li>Stacks of <code>Dense</code> layers with <code>relu</code> activations can solve a wide range of problems (including sentiment classification), and you will<br>likely use them frequently.</li><li>In a binary classification problem (two output classes), your network should end with a <code>Dense</code> layer with 1 unit and a <code>sigmoid</code> activation,<br>i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.</li><li>With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is <code>binary_crossentropy</code>.</li><li>The <code>rmsprop</code> optimizer is generally a good enough choice of optimizer, whatever your problem. That’s one less thing for you to worry<br>about.</li><li>As they get better on their training data, neural networks eventually start <em>overfitting</em> and end up obtaining increasingly worse results on data<br>never-seen-before. Make sure to always monitor performance on data that is outside of the training set.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><p><strong>不知道这个破电脑怎么一回事，*的，流下贫穷的泪水</strong></p><p><img src="/img/4.22.1.png" alt="艹"></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL with python-1</title>
    <link href="/2021/04/20/DL-with-python-1/"/>
    <url>/2021/04/20/DL-with-python-1/</url>
    
    <content type="html"><![CDATA[<p>2.1-a-first-look-at-a-neural-network</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="A-first-look-at-a-neural-network"><a href="#A-first-look-at-a-neural-network" class="headerlink" title="A first look at a neural network"></a>A first look at a neural network</h1><p>This notebook contains the code samples found in Chapter 2, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify<br>hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this<br>first example right away. You probably haven’t even installed Keras yet. Don’t worry, that is perfectly fine. In the next chapter, we will<br>review each element in our example and explain them in detail. So don’t worry if some steps seem arbitrary or look like magic to you!<br>We’ve got to start somewhere.</p><p>The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10<br>categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been<br>around for almost as long as the field itself and has been very intensively studied. It’s a set of 60,000 training images, plus 10,000 test<br>images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST<br>as the “Hello World” of deep learning — it’s what you do to verify that your algorithms are working as expected. As you become a machine<br>learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.</p><p>The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist<br><br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br></code></pre></td></tr></table></figure><p><code>train_images</code> and <code>train_labels</code> form the “training set”, the data that the model will learn from. The model will then be tested on the<br>“test set”, <code>test_images</code> and <code>test_labels</code>. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging<br>from 0 to 9. There is a one-to-one correspondence between the images and the labels.</p><p>Let’s have a look at the training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images.shape<br></code></pre></td></tr></table></figure><pre><code>(60000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_labels)<br></code></pre></td></tr></table></figure><pre><code>60000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels<br></code></pre></td></tr></table></figure><pre><code>array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</code></pre><p>Let’s have a look at the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_images.shape<br></code></pre></td></tr></table></figure><pre><code>(10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_labels<br></code></pre></td></tr></table></figure><pre><code>array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</code></pre><p>Our workflow will be as follow: first we will present our neural network with the training data, <code>train_images</code> and <code>train_labels</code>. The<br>network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for <code>test_images</code>, and we<br>will verify if these predictions match the labels from <code>test_labels</code>.</p><p>Let’s build our network — again, remember that you aren’t supposed to understand everything about this example just yet.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>network = models.Sequential()<br>network.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>,)))<br>network.add(layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>The core building block of neural networks is the “layer”, a data-processing module which you can conceive as a “filter” for data. Some<br>data comes in, and comes out in a more useful form. Precisely, layers extract <em>representations</em> out of the data fed into them — hopefully<br>representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers<br>which will implement a form of progressive “data distillation”. A deep learning model is like a sieve for data processing, made of a<br>succession of increasingly refined data filters — the “layers”.</p><p>Here our network consists of a sequence of two <code>Dense</code> layers, which are densely-connected (also called “fully-connected”) neural layers.<br>The second (and last) layer is a 10-way “softmax” layer, which means it will return an array of 10 probability scores (summing to 1). Each<br>score will be the probability that the current digit image belongs to one of our 10 digit classes.</p><p>To make our network ready for training, we need to pick three more things, as part of “compilation” step:</p><ul><li>A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be<br>able to steer itself in the right direction.</li><li>An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.</li><li>Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly<br>classified).</li></ul><p>The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">network.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>                metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in<br>the <code>[0, 1]</code> interval. Previously, our training images for instance were stored in an array of shape <code>(60000, 28, 28)</code> of type <code>uint8</code> with<br>values in the <code>[0, 255]</code> interval. We transform it into a <code>float32</code> array of shape <code>(60000, 28 * 28)</code> with values between 0 and 1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images = train_images.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>test_images = test_images.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br></code></pre></td></tr></table></figure><p>We also need to categorically encode the labels, a step which we explain in chapter 3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><p>We are now ready to train our network, which in Keras is done via a call to the <code>fit</code> method of the network:<br>we “fit” the model to its training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">network.fit(train_images, train_labels, epochs=<span class="hljs-number">5</span>, batch_size=<span class="hljs-number">128</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/5469/469 [==============================] - 2s 5ms/step - loss: 0.0272 - accuracy: 0.9920Epoch 2/5469/469 [==============================] - 2s 5ms/step - loss: 0.0212 - accuracy: 0.9939Epoch 3/5469/469 [==============================] - 2s 5ms/step - loss: 0.0159 - accuracy: 0.9955Epoch 4/5469/469 [==============================] - 2s 4ms/step - loss: 0.0123 - accuracy: 0.9964Epoch 5/5469/469 [==============================] - 2s 4ms/step - loss: 0.0101 - accuracy: 0.9973&lt;tensorflow.python.keras.callbacks.History at 0x2bb3ae958b0&gt;</code></pre><p>Two quantities are being displayed during training: the “loss” of the network over the training data, and the accuracy of the network over<br>the training data.</p><p>We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loss, test_acc = network.evaluate(test_images, test_labels)<br></code></pre></td></tr></table></figure><pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.0721 - accuracy: 0.9808</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;test_acc:&#x27;</span>, test_acc)<br></code></pre></td></tr></table></figure><pre><code>test_acc: 0.9807999730110168</code></pre><p>Our test set accuracy turns out to be 97.8% — that’s quite a bit lower than the training set accuracy.<br>This gap between training accuracy and test accuracy is an example of “overfitting”,<br>the fact that machine learning models tend to perform worse on new data than on their training data.<br>Overfitting will be a central topic in chapter 3.</p><p>This concludes our very first example — you just saw how we could build and a train a neural network to classify handwritten digits, in<br>less than 20 lines of Python code. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really<br>going on behind the scenes. You will learn about “tensors”, the data-storing objects going into the network, about tensor operations, which<br>layers are made of, and about gradient descent, which allows our network to learn from its training examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>Coding</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一篇突然爆红网络的博士论文致谢</title>
    <link href="/2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
    <url>/2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/4.19.png" alt="他的世界本无光，他把自己活成了光。"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s Maching Learning?</title>
    <link href="/2021/04/17/What-s-Maching-Learning/"/>
    <url>/2021/04/17/What-s-Maching-Learning/</url>
    
    <content type="html"><![CDATA[<p>贴上了黄博的github笔记</p><span id="more"></span><h2 id="M-D-L的一些资源"><a href="#M-D-L的一些资源" class="headerlink" title="M(D)L的一些资源"></a>M(D)L的一些资源</h2><ol><li><p>黄海广github： <a href="https://github.com/fengdu78">https://github.com/fengdu78</a></p></li><li><p>ML：笔记在线阅读： <a href="http://www.ai-start.com/ml2014/">http://www.ai-start.com/ml2014/</a></p></li><li><p>ML： github：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p></li><li><p>DL：笔记在线阅读： <a href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p></li><li><p>DL： github： <a href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></p></li><li><p>数据科学笔记：github：<a href="https://github.com/fengdu78/Data-Science-Notes">https://github.com/fengdu78/Data-Science-Notes</a></p></li><li><p>统计学习方法代码复现：github：<a href="https://github.com/wzyonggege/">https://github.com/wzyonggege/</a></p></li><li><p>statistical-learning-method or <a href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p></li><li><p>李宏毅ML教材习题解答及实现： <a href="https://github.com/Doraemonzzz/Learning-from-data">https://github.com/Doraemonzzz/Learning-from-data</a></p></li><li><p>李宏毅讲义： <a href="https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg">https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg</a> key：h74o</p></li><li><p>吴恩达有关tensorflow的介绍：对应4笔记p251</p></li><li><p>tensorflow简要介绍： <a href="https://link.zhihu.com/?target=https%3A//github.com/aymericdamien/TensorFlow-Examples">https://link.zhihu.com/?target=https%3A//github.com/aymericdamien/TensorFlow-Examples</a></p></li><li><p>DL with python：<a href="https://github.com/fchollet/deep-learning-with-python-notebooks">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p></li><li><p>pytorch入门：<a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p></li><li><p>sql： 语法查询：<a href="http://www.w3school.com.cn/sql/index.asp">http://www.w3school.com.cn/sql/index.asp</a></p></li><li><p>sql： 语法题库：<a href="https://leetcode-cn.com/problemset/database/">https://leetcode-cn.com/problemset/database/</a></p></li><li><p>Ubuntu 18.04深度学习环境配置（CUDA9.0+CUDNN7.4+TensorFlow1.8：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect</a></p></li><li><p>徐亦达老师课件及下载（中文目录）：<a href="https://github.com/roboticcam/machine-learning-notes">https://github.com/roboticcam/machine-learning-notes</a></p></li><li><p>华校专老师笔记： <a href="https://github.com/huaxz1986">https://github.com/huaxz1986</a></p></li><li><p>华校专网站： <a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></p></li><li><p>论文排班教程： <a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect</a></p></li><li><p>zotero论文管理工具：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect</a></p></li><li><p>tf，pytorch，keras样例资源：</p></li></ol><h2 id="一、TensorFlow"><a href="#一、TensorFlow" class="headerlink" title="一、TensorFlow"></a>一、TensorFlow</h2><h3 id="资源地址"><a href="#资源地址" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p><h3 id="资源介绍"><a href="#资源介绍" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>本资源旨在通过示例轻松深入了解TensorFlow。 为了便于阅读，它包括notebook和带注释的源代码。</p><p>它适合想要找到关于TensorFlow的清晰简洁示例的初学者。 除了传统的“原始”TensorFlow实现，您还可以找到最新的TensorFlow API实践（例如layers,estimator,dataset, ……）。</p><p>最后更新（07/25/2018）：添加新示例（GBDT，Word2Vec）和 TF1.9兼容性！ （TF v1.9 +推荐）。</p><h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，TensorFlow 1.8+</p><h3 id="资源目录"><a href="#资源目录" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-先决条件"><a href="#0-先决条件" class="headerlink" title="0  - 先决条件"></a>0  - 先决条件</h4><p>机器学习简介</p><p>MNIST数据集简介</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1  - 简介"></a>1  - 简介</h4><p>Hello World(包含notebook和py源代码)。非常简单的例子，学习如何使用TensorFlow打印“hello world”。</p><p>基本操作(包含notebook和py源代码)。一个涵盖TensorFlow基本操作的简单示例。</p><p>TensorFlow Eager API基础知识(包含notebook和py源代码)。开始使用TensorFlow的Eager API。</p><h4 id="2-基础模型"><a href="#2-基础模型" class="headerlink" title="2  - 基础模型"></a>2  - 基础模型</h4><p>线性回归(包含notebook和py源代码)。使用TensorFlow实现线性回归。</p><p>线性回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现线性回归。</p><p>Logistic回归(包含notebook和py源代码)。使用TensorFlow实现Logistic回归。</p><p>Logistic回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现Logistic回归。</p><p>最近邻(包含notebook和py源代码)。使用TensorFlow实现最近邻算法。</p><p>K-Means(包含notebook和py源代码)。使用TensorFlow构建K-Means分类器。</p><p>随机森林(包含notebook和py源代码)。使用TensorFlow构建随机森林分类器。</p><p>Gradient Boosted Decision Tree（GBDT）(包含notebook和py源代码)。使用TensorFlow构建梯度提升决策树（GBDT）。</p><p>Word2Vec（词嵌入）(包含notebook和py源代码)。使用TensorFlow从Wikipedia数据构建词嵌入模型（Word2Vec）。</p><h4 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3  - 神经网络"></a>3  - 神经网络</h4><h5 id="监督学习部分"><a href="#监督学习部分" class="headerlink" title="监督学习部分"></a>监督学习部分</h5><p>简单神经网络(包含notebook和py源代码)。构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>简单神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建一个简单的神经网络（如：Multi-layer Perceptron）来对MNIST数字数据集进行分类。</p><p>简单神经网络（Eager API）(包含notebook和py源代码)。使用TensorFlow Eager API构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。</p><p>卷积神经网络(包含notebook和py源代码)。构建卷积神经网络以对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>卷积神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建卷积神经网络，对MNIST数字数据集进行分类。</p><p>递归神经网络（LSTM）(包含notebook和py源代码)。构建递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>双向LSTM(包含notebook和py源代码)。构建双向递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>动态LSTM(包含notebook和py源代码)。构建一个递归神经网络（LSTM），执行动态计算以对不同长度的序列进行分类。</p><h5 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h5><p>自动编码器(包含notebook和py源代码)。构建自动编码器以将图像编码为较低维度并重新构建它。</p><p>变分自动编码器（(包含notebook和py源代码)。构建变分自动编码器（VAE），对噪声进行编码和生成图像。</p><p>GAN（Generative Adversarial Networks）(包含notebook和py源代码)。构建生成对抗网络（GAN）以从噪声生成图像。</p><p>DCGAN（Deep Convolutional Generative Adversarial Networks）(包含notebook和py源代码)。构建深度卷积生成对抗网络（DCGAN）以从噪声生成图像。</p><h4 id="4-工具"><a href="#4-工具" class="headerlink" title="4  - 工具"></a>4  - 工具</h4><p>保存和还原模型(包含notebook和py源代码)。使用TensorFlow保存和还原模型。</p><p>Tensorboard  - 图形和损失可视化(包含notebook和py源代码)。使用Tensorboard可视化计算图并绘制损失。</p><p>Tensorboard  - 高级可视化(包含notebook和py源代码)。深入了解Tensorboard;可视化变量，梯度等……</p><h4 id="5-数据管理"><a href="#5-数据管理" class="headerlink" title="5  - 数据管理"></a>5  - 数据管理</h4><p>构建图像数据集(包含notebook和py源代码)。使用TensorFlow数据队列，从图像文件夹或数据集文件构建您自己的图像数据集。</p><p>TensorFlow数据集API(包含notebook和py源代码)。引入TensorFlow数据集API以优化输入数据管道。</p><h4 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6  - 多GPU"></a>6  - 多GPU</h4><p>多GPU的基本操作(包含notebook和py源代码)。在TensorFlow中引入多GPU的简单示例。</p><p>在多GPU上训练神经网络(包含notebook和py源代码)。一个清晰简单的TensorFlow实现，用于在多个GPU上训练卷积神经网络。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>一些示例需要MNIST数据集进行训练和测试。官方网站：<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p><h2 id="二、Keras"><a href="#二、Keras" class="headerlink" title="二、Keras"></a>二、Keras</h2><h3 id="资源地址-1"><a href="#资源地址-1" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks">https://github.com/erhwenkuo/deep-learning-with-keras-notebooks</a></p><h3 id="资源介绍-1"><a href="#资源介绍-1" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个github的repository主要是ErhWen Kuo在学习Keras的一些记录及练习。希望在学习过程中发现到一些好的信息与示例也可以对想要学习使用Keras来解决问题的同学带来帮助。这些notebooks主要是使用Python 3.6与Keras 2.1.1版本跑在一台配置Nivida 1080Ti的Windows 10的机台所产生的结果，但有些部份会参杂一些Tensorflow与其它的函式库的介绍。</p><h3 id="配置环境-1"><a href="#配置环境-1" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，Keras 2.1.1</p><h3 id="资源目录-1"><a href="#资源目录-1" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-图象数据集-工具介绍"><a href="#0-图象数据集-工具介绍" class="headerlink" title="0.图象数据集/工具介绍"></a>0.图象数据集/工具介绍</h4><h5 id="0-0-COCO-API解说与简单示例"><a href="#0-0-COCO-API解说与简单示例" class="headerlink" title="0.0: COCO API解说与简单示例"></a>0.0: COCO API解说与简单示例</h5><h5 id="0-1-土炮自制扑克牌图象数据集"><a href="#0-1-土炮自制扑克牌图象数据集" class="headerlink" title="0.1:土炮自制扑克牌图象数据集"></a>0.1:土炮自制扑克牌图象数据集</h5><h5 id="0-2-使用Pillow来进行图像处理"><a href="#0-2-使用Pillow来进行图像处理" class="headerlink" title="0.2:使用Pillow来进行图像处理"></a>0.2:使用Pillow来进行图像处理</h5><h4 id="1-Keras-API示例"><a href="#1-Keras-API示例" class="headerlink" title="1.Keras API示例"></a>1.Keras API示例</h4><h5 id="1-0-使用图像增强来进行深度学习"><a href="#1-0-使用图像增强来进行深度学习" class="headerlink" title="1.0:使用图像增强来进行深度学习"></a>1.0:使用图像增强来进行深度学习</h5><h5 id="1-1-如何使用Keras函数式API进行深度学习"><a href="#1-1-如何使用Keras函数式API进行深度学习" class="headerlink" title="1.1:如何使用Keras函数式API进行深度学习"></a>1.1:如何使用Keras函数式API进行深度学习</h5><h5 id="1-2-从零开始构建VGG网络来学习Keras"><a href="#1-2-从零开始构建VGG网络来学习Keras" class="headerlink" title="1.2:从零开始构建VGG网络来学习Keras"></a>1.2:从零开始构建VGG网络来学习Keras</h5><h5 id="1-3-使用预训练的模型来分类照片中的物体"><a href="#1-3-使用预训练的模型来分类照片中的物体" class="headerlink" title="1.3:使用预训练的模型来分类照片中的物体"></a>1.3:使用预训练的模型来分类照片中的物体</h5><h5 id="1-4-使用图像增强来训练小数据集"><a href="#1-4-使用图像增强来训练小数据集" class="headerlink" title="1.4:使用图像增强来训练小数据集"></a>1.4:使用图像增强来训练小数据集</h5><h5 id="1-5-使用预先训练的卷积网络模型"><a href="#1-5-使用预先训练的卷积网络模型" class="headerlink" title="1.5:使用预先训练的卷积网络模型"></a>1.5:使用预先训练的卷积网络模型</h5><h5 id="1-6-卷积网络模型学习到什么的可视化"><a href="#1-6-卷积网络模型学习到什么的可视化" class="headerlink" title="1.6:卷积网络模型学习到什么的可视化"></a>1.6:卷积网络模型学习到什么的可视化</h5><h5 id="1-7-构建自动编码器（Autoencoder）"><a href="#1-7-构建自动编码器（Autoencoder）" class="headerlink" title="1.7:构建自动编码器（Autoencoder）"></a>1.7:构建自动编码器（Autoencoder）</h5><h5 id="1-8-序列到序列（Seq-to-Seq）学习介绍"><a href="#1-8-序列到序列（Seq-to-Seq）学习介绍" class="headerlink" title="1.8:序列到序列（Seq-to-Seq）学习介绍"></a>1.8:序列到序列（Seq-to-Seq）学习介绍</h5><h5 id="1-9-One-hot编码工具程序介绍"><a href="#1-9-One-hot编码工具程序介绍" class="headerlink" title="1.9: One-hot编码工具程序介绍"></a>1.9: One-hot编码工具程序介绍</h5><h5 id="1-10-循环神经网络（RNN）介绍"><a href="#1-10-循环神经网络（RNN）介绍" class="headerlink" title="1.10:循环神经网络（RNN）介绍"></a>1.10:循环神经网络（RNN）介绍</h5><h5 id="1-11-LSTM的返回序列和返回状态之间的区别"><a href="#1-11-LSTM的返回序列和返回状态之间的区别" class="headerlink" title="1.11: LSTM的返回序列和返回状态之间的区别"></a>1.11: LSTM的返回序列和返回状态之间的区别</h5><h5 id="1-12-用LSTM来学习英文字母表顺序"><a href="#1-12-用LSTM来学习英文字母表顺序" class="headerlink" title="1.12:用LSTM来学习英文字母表顺序"></a>1.12:用LSTM来学习英文字母表顺序</h5><h4 id="2-图像分类（Image-Classification）"><a href="#2-图像分类（Image-Classification）" class="headerlink" title="2.图像分类（Image Classification）"></a>2.图像分类（Image Classification）</h4><h5 id="2-0-Julia（Chars74K）字母图像分类"><a href="#2-0-Julia（Chars74K）字母图像分类" class="headerlink" title="2.0: Julia（Chars74K）字母图像分类"></a>2.0: Julia（Chars74K）字母图像分类</h5><h5 id="2-1-交通标志图像分类"><a href="#2-1-交通标志图像分类" class="headerlink" title="2.1:交通标志图像分类"></a>2.1:交通标志图像分类</h5><h5 id="2-2-辛普森卡通图像角色分类"><a href="#2-2-辛普森卡通图像角色分类" class="headerlink" title="2.2:辛普森卡通图像角色分类"></a>2.2:辛普森卡通图像角色分类</h5><h5 id="2-3-时尚服饰图像分类"><a href="#2-3-时尚服饰图像分类" class="headerlink" title="2.3:时尚服饰图像分类"></a>2.3:时尚服饰图像分类</h5><h5 id="2-4-人脸关键点辨识"><a href="#2-4-人脸关键点辨识" class="headerlink" title="2.4:人脸关键点辨识"></a>2.4:人脸关键点辨识</h5><h5 id="2-5-Captcha验证码分类"><a href="#2-5-Captcha验证码分类" class="headerlink" title="2.5: Captcha验证码分类"></a>2.5: Captcha验证码分类</h5><h5 id="2-6-Mnist手写图像分类（MLP）"><a href="#2-6-Mnist手写图像分类（MLP）" class="headerlink" title="2.6: Mnist手写图像分类（MLP）"></a>2.6: Mnist手写图像分类（MLP）</h5><h5 id="2-7-Mnist手写图像分类（CNN）"><a href="#2-7-Mnist手写图像分类（CNN）" class="headerlink" title="2.7: Mnist手写图像分类（CNN）"></a>2.7: Mnist手写图像分类（CNN）</h5><h4 id="3-目标检测（Object-Recognition）"><a href="#3-目标检测（Object-Recognition）" class="headerlink" title="3.目标检测（Object Recognition）"></a>3.目标检测（Object Recognition）</h4><h5 id="3-0-YOLO目标检测算法概念与介绍"><a href="#3-0-YOLO目标检测算法概念与介绍" class="headerlink" title="3.0: YOLO目标检测算法概念与介绍"></a>3.0: YOLO目标检测算法概念与介绍</h5><h5 id="3-1-YOLOv2目标检测示例"><a href="#3-1-YOLOv2目标检测示例" class="headerlink" title="3.1: YOLOv2目标检测示例"></a>3.1: YOLOv2目标检测示例</h5><h5 id="3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整"><a href="#3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整" class="headerlink" title="3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整"></a>3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整</h5><h5 id="3-3-浣熊（Racoon）检测-YOLOv2模型的使用"><a href="#3-3-浣熊（Racoon）检测-YOLOv2模型的使用" class="headerlink" title="3.3:浣熊（Racoon）检测-YOLOv2模型的使用"></a>3.3:浣熊（Racoon）检测-YOLOv2模型的使用</h5><h5 id="3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"><a href="#3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整" class="headerlink" title="3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"></a>3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整</h5><h5 id="3-5-双手（Hands）检测-YOLOv2模型训练与调整"><a href="#3-5-双手（Hands）检测-YOLOv2模型训练与调整" class="headerlink" title="3.5:双手（Hands）检测-YOLOv2模型训练与调整"></a>3.5:双手（Hands）检测-YOLOv2模型训练与调整</h5><h5 id="3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"><a href="#3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整" class="headerlink" title="3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"></a>3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整</h5><h5 id="3-7-MS-COCO图象检测-YOLOv2模型训练与调整"><a href="#3-7-MS-COCO图象检测-YOLOv2模型训练与调整" class="headerlink" title="3.7: MS COCO图象检测-YOLOv2模型训练与调整"></a>3.7: MS COCO图象检测-YOLOv2模型训练与调整</h5><h4 id="4-物体分割（Object-Segmentation）"><a href="#4-物体分割（Object-Segmentation）" class="headerlink" title="4.物体分割（Object Segmentation）"></a>4.物体分割（Object Segmentation）</h4><h4 id="5-关键点检测（Keypoint-Detection）"><a href="#5-关键点检测（Keypoint-Detection）" class="headerlink" title="5.关键点检测（Keypoint Detection）"></a>5.关键点检测（Keypoint Detection）</h4><h4 id="6-图象标题（Image-Caption）"><a href="#6-图象标题（Image-Caption）" class="headerlink" title="6.图象标题（Image Caption）"></a>6.图象标题（Image Caption）</h4><h4 id="7-人脸检测识别（Face-Detection-Recognition）"><a href="#7-人脸检测识别（Face-Detection-Recognition）" class="headerlink" title="7.人脸检测识别（Face Detection/Recognition）"></a>7.人脸检测识别（Face Detection/Recognition）</h4><h5 id="7-0-人脸检测-OpenCV（Haar特征分类器）"><a href="#7-0-人脸检测-OpenCV（Haar特征分类器）" class="headerlink" title="7.0:人脸检测- OpenCV（Haar特征分类器）"></a>7.0:人脸检测- OpenCV（Haar特征分类器）</h5><h5 id="7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）"><a href="#7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）" class="headerlink" title="7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）"></a>7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）</h5><h5 id="7-2-人脸识别-脸部检测、对齐-amp-裁剪"><a href="#7-2-人脸识别-脸部检测、对齐-amp-裁剪" class="headerlink" title="7.2:人脸识别-脸部检测、对齐&amp;裁剪"></a>7.2:人脸识别-脸部检测、对齐&amp;裁剪</h5><h5 id="7-3-人脸识别-人脸部特征提取-amp-人脸分类器"><a href="#7-3-人脸识别-人脸部特征提取-amp-人脸分类器" class="headerlink" title="7.3:人脸识别-人脸部特征提取&amp;人脸分类器"></a>7.3:人脸识别-人脸部特征提取&amp;人脸分类器</h5><h5 id="7-4-人脸识别-转换、对齐、裁剪、特征提取与比对"><a href="#7-4-人脸识别-转换、对齐、裁剪、特征提取与比对" class="headerlink" title="7.4:人脸识别-转换、对齐、裁剪、特征提取与比对"></a>7.4:人脸识别-转换、对齐、裁剪、特征提取与比对</h5><h5 id="7-5-脸部关键点检测（dlib）"><a href="#7-5-脸部关键点检测（dlib）" class="headerlink" title="7.5:脸部关键点检测（dlib）"></a>7.5:脸部关键点检测（dlib）</h5><h5 id="7-6-头部姿态（Head-pose）估计（dlib）"><a href="#7-6-头部姿态（Head-pose）估计（dlib）" class="headerlink" title="7.6:头部姿态（Head pose）估计（dlib）"></a>7.6:头部姿态（Head pose）估计（dlib）</h5><h4 id="8-自然语言处理（Natural-Language-Processing）"><a href="#8-自然语言处理（Natural-Language-Processing）" class="headerlink" title="8.自然语言处理（Natural Language Processing）"></a>8.自然语言处理（Natural Language Processing）</h4><h5 id="8-0-词嵌入（word-embeddings）介绍"><a href="#8-0-词嵌入（word-embeddings）介绍" class="headerlink" title="8.0:词嵌入（word embeddings）介绍"></a>8.0:词嵌入（word embeddings）介绍</h5><h5 id="8-1-使用结巴（jieba）进行中文分词"><a href="#8-1-使用结巴（jieba）进行中文分词" class="headerlink" title="8.1:使用结巴（jieba）进行中文分词"></a>8.1:使用结巴（jieba）进行中文分词</h5><h5 id="8-2-Word2vec词嵌入（word-embeddings）的基本概念"><a href="#8-2-Word2vec词嵌入（word-embeddings）的基本概念" class="headerlink" title="8.2: Word2vec词嵌入（word embeddings）的基本概念"></a>8.2: Word2vec词嵌入（word embeddings）的基本概念</h5><h5 id="8-3-使用结巴（jieba）进行歌词分析"><a href="#8-3-使用结巴（jieba）进行歌词分析" class="headerlink" title="8.3:使用结巴（jieba）进行歌词分析"></a>8.3:使用结巴（jieba）进行歌词分析</h5><h5 id="8-4-使用gensim训练中文词向量（word2vec）"><a href="#8-4-使用gensim训练中文词向量（word2vec）" class="headerlink" title="8.4:使用gensim训练中文词向量（word2vec）"></a>8.4:使用gensim训练中文词向量（word2vec）</h5><h2 id="三、Pytorch"><a href="#三、Pytorch" class="headerlink" title="三、Pytorch"></a>三、Pytorch</h2><h3 id="资源地址-2"><a href="#资源地址-2" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p><h3 id="资源介绍-2"><a href="#资源介绍-2" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个资源为深度学习研究人员提供了学习PyTorch的教程代码大多数模型都使用少于30行代码实现。 在开始本教程之前，建议先看完Pytorch官方教程。</p><h3 id="配置环境-2"><a href="#配置环境-2" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 2.7或者3.5以上，pytorch 0.4</p><h3 id="资源目录-2"><a href="#资源目录-2" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1.基础知识"></a>1.基础知识</h4><p>PyTorch基础知识</p><p>线性回归</p><p>Logistic回归</p><p>前馈神经网络</p><h4 id="2-中级"><a href="#2-中级" class="headerlink" title="2.中级"></a>2.中级</h4><p>卷积神经网络</p><p>深度残差网络</p><p>递归神经网络</p><p>双向递归神经网络</p><p>语言模型（RNN-LM）</p><h4 id="3-高级"><a href="#3-高级" class="headerlink" title="3.高级"></a>3.高级</h4><p>生成性对抗网络</p><p>变分自动编码器</p><p>神经风格转移</p><p>图像字幕（CNN-RNN）</p><h4 id="4-工具-1"><a href="#4-工具-1" class="headerlink" title="4.工具"></a>4.工具</h4><p>PyTorch中的TensorBoard</p>]]></content>
    
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>my first blog</title>
    <link href="/2021/04/17/my-first-blog/"/>
    <url>/2021/04/17/my-first-blog/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><p>先发一篇试试^_^</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Test</title>
    <link href="/2021/04/17/Test/"/>
    <url>/2021/04/17/Test/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><p>A blog for test.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/04/16/hello-world/"/>
    <url>/2021/04/16/hello-world/</url>
    
    <content type="html"><![CDATA[<p>搞了**的整整两天，我的个人blog终于能在云端访问了^^^^</p><span id="more"></span><p>After two fucking days, Lao Tzu’s personal blog is finally online, fuck</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
