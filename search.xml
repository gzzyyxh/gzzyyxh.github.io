<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DL with python-5</title>
      <link href="2021/04/22/DL-with-python-5/"/>
      <url>2021/04/22/DL-with-python-5/</url>
      
        <content type="html"><![CDATA[<p>4.4-overfitting-and-underfitting</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Overfitting-and-underfitting"><a href="#Overfitting-and-underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In all the examples we saw in the previous chapter – movie review sentiment prediction, topic classification, and house price regression –<br>we could notice that the performance of our model on the held-out validation data would always peak after a few epochs and would then start<br>degrading, i.e. our model would quickly start to <em>overfit</em> to the training data. Overfitting happens in every single machine learning<br>problem. Learning how to deal with overfitting is essential to mastering machine learning.</p><p>The fundamental issue in machine learning is the tension between optimization and generalization. “Optimization” refers to the process of<br>adjusting a model to get the best performance possible on the training data (the “learning” in “machine learning”), while “generalization”<br>refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of<br>course, but you do not control generalization; you can only adjust the model based on its training data.</p><p>At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on<br>test data. While this is happening, your model is said to be <em>under-fit</em>: there is still progress to be made; the network hasn’t yet<br>modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops<br>improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns<br>that are specific to the training data but that are misleading or irrelevant when it comes to new data.</p><p>To prevent a model from learning misleading or irrelevant patterns found in the training data, <em>the best solution is of course to get<br>more training data</em>. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution<br>is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to<br>store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most<br>prominent patterns, which have a better chance of generalizing well.</p><p>The processing of fighting overfitting in this way is called <em>regularization</em>. Let’s review some of the most common regularization<br>techniques, and let’s apply them in practice to improve our movie classification model from  the previous chapter.</p><p>Note: in this notebook we will be using the IMDB test set as our validation set. It doesn’t matter in this context.</p><p>Let’s prepare the data using the code from Chapter 3, Section 5:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="Fighting-overfitting"><a href="#Fighting-overfitting" class="headerlink" title="Fighting overfitting"></a>Fighting overfitting</h1><h2 id="Reducing-the-network’s-size"><a href="#Reducing-the-network’s-size" class="headerlink" title="Reducing the network’s size"></a>Reducing the network’s size</h2><p>The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is<br>determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is<br>often referred to as the model’s “capacity”. Intuitively, a model with more parameters will have more “memorization capacity” and therefore<br>will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any<br>generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the<br>MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying<br>new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge<br>is generalization, not fitting.</p><p>On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in<br>order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets<br>– precisely the type of representations that we are interested in. At the same time, keep in mind that you should be using models that have<br>enough parameters that they won’t be underfitting: your model shouldn’t be starved for memorization resources. There is a compromise to be<br>found between “too much capacity” and “not enough capacity”.</p><p>Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You<br>will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the<br>right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and<br>parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the<br>validation loss.</p><p>Let’s try this on our movie review classification network. Our original network was as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>original_model = models.Sequential()<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>original_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>original_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                       loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                       metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Now let’s try to replace it with this smaller network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model = models.Sequential()<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>smaller_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>smaller_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                      loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                      metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Here’s a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of<br>the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">original_hist = original_model.fit(x_train, y_train,<br>                                   epochs=<span class="hljs-number">20</span>,<br>                                   batch_size=<span class="hljs-number">512</span>,<br>                                   validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4594 - acc: 0.8188 - val_loss: 0.3429 - val_acc: 0.8812Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.2659 - acc: 0.9075 - val_loss: 0.2873 - val_acc: 0.8905Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2060 - acc: 0.9276 - val_loss: 0.2827 - val_acc: 0.8879Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.1697 - acc: 0.9401 - val_loss: 0.2921 - val_acc: 0.8851Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.1495 - acc: 0.9470 - val_loss: 0.3100 - val_acc: 0.8812Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1283 - acc: 0.9572 - val_loss: 0.3336 - val_acc: 0.8748Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1121 - acc: 0.9624 - val_loss: 0.3987 - val_acc: 0.8593Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.0994 - acc: 0.9670 - val_loss: 0.3788 - val_acc: 0.8702Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.0889 - acc: 0.9716 - val_loss: 0.4242 - val_acc: 0.8603Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.0782 - acc: 0.9757 - val_loss: 0.4256 - val_acc: 0.8653Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.0691 - acc: 0.9792 - val_loss: 0.4515 - val_acc: 0.8638Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.0603 - acc: 0.9820 - val_loss: 0.5102 - val_acc: 0.8610Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.0518 - acc: 0.9851 - val_loss: 0.5281 - val_acc: 0.8587Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0446 - acc: 0.9873 - val_loss: 0.5441 - val_acc: 0.8589Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0367 - acc: 0.9903 - val_loss: 0.5777 - val_acc: 0.8574Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0313 - acc: 0.9922 - val_loss: 0.6377 - val_acc: 0.8555Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0247 - acc: 0.9941 - val_loss: 0.7269 - val_acc: 0.8501Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0203 - acc: 0.9956 - val_loss: 0.6920 - val_acc: 0.8516Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0156 - acc: 0.9970 - val_loss: 0.7689 - val_acc: 0.8425Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0144 - acc: 0.9966 - val_loss: 0.7694 - val_acc: 0.8487</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model_hist = smaller_model.fit(x_train, y_train,<br>                                       epochs=<span class="hljs-number">20</span>,<br>                                       batch_size=<span class="hljs-number">512</span>,<br>                                       validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 2s - loss: 0.5737 - acc: 0.8049 - val_loss: 0.4826 - val_acc: 0.8616Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3973 - acc: 0.8866 - val_loss: 0.3699 - val_acc: 0.8776Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2985 - acc: 0.9054 - val_loss: 0.3140 - val_acc: 0.8860Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2428 - acc: 0.9189 - val_loss: 0.2913 - val_acc: 0.8870Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2085 - acc: 0.9290 - val_loss: 0.2809 - val_acc: 0.8897Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1849 - acc: 0.9360 - val_loss: 0.2772 - val_acc: 0.8899Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1666 - acc: 0.9430 - val_loss: 0.2835 - val_acc: 0.8863Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1515 - acc: 0.9487 - val_loss: 0.2909 - val_acc: 0.8850Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1388 - acc: 0.9526 - val_loss: 0.2984 - val_acc: 0.8842Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1285 - acc: 0.9569 - val_loss: 0.3102 - val_acc: 0.8818Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1194 - acc: 0.9599 - val_loss: 0.3219 - val_acc: 0.8794Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1105 - acc: 0.9648 - val_loss: 0.3379 - val_acc: 0.8774Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1035 - acc: 0.9674 - val_loss: 0.3532 - val_acc: 0.8730Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0963 - acc: 0.9688 - val_loss: 0.3651 - val_acc: 0.8731Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0895 - acc: 0.9724 - val_loss: 0.3858 - val_acc: 0.8703Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0838 - acc: 0.9734 - val_loss: 0.4157 - val_acc: 0.8654Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0785 - acc: 0.9764 - val_loss: 0.4214 - val_acc: 0.8677Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0733 - acc: 0.9784 - val_loss: 0.4390 - val_acc: 0.8644Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0685 - acc: 0.9796 - val_loss: 0.4539 - val_acc: 0.8638Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0638 - acc: 0.9822 - val_loss: 0.4744 - val_acc: 0.8617</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>)<br>original_val_loss = original_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br>smaller_model_val_loss = smaller_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># b+ is for &quot;blue cross&quot;</span><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, smaller_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Smaller model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_12_0.png" alt="output"></p><p>As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance<br>degrades much more slowly once it starts overfitting.</p><p>Now, for kicks, let’s add to this benchmark a network that has much more capacity, far more than the problem would warrant:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model = models.Sequential()<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>bigger_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>bigger_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                     loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                     metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_hist = bigger_model.fit(x_train, y_train,<br>                                     epochs=<span class="hljs-number">20</span>,<br>                                     batch_size=<span class="hljs-number">512</span>,<br>                                     validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4539 - acc: 0.8011 - val_loss: 0.4150 - val_acc: 0.8229Epoch 2/2025000/25000 [==============================] - 3s - loss: 0.2148 - acc: 0.9151 - val_loss: 0.2742 - val_acc: 0.8901Epoch 3/2025000/25000 [==============================] - 3s - loss: 0.1217 - acc: 0.9544 - val_loss: 0.5442 - val_acc: 0.7975Epoch 4/2025000/25000 [==============================] - 3s - loss: 0.0552 - acc: 0.9835 - val_loss: 0.4316 - val_acc: 0.8842Epoch 5/2025000/25000 [==============================] - 3s - loss: 0.0662 - acc: 0.9888 - val_loss: 0.5098 - val_acc: 0.8822Epoch 6/2025000/25000 [==============================] - 3s - loss: 0.0017 - acc: 0.9998 - val_loss: 0.6867 - val_acc: 0.8811Epoch 7/2025000/25000 [==============================] - 3s - loss: 0.1019 - acc: 0.9882 - val_loss: 0.6737 - val_acc: 0.8800Epoch 8/2025000/25000 [==============================] - 3s - loss: 0.0735 - acc: 0.9896 - val_loss: 0.6185 - val_acc: 0.8772Epoch 9/2025000/25000 [==============================] - 3s - loss: 3.4759e-04 - acc: 1.0000 - val_loss: 0.7328 - val_acc: 0.8818Epoch 10/2025000/25000 [==============================] - 3s - loss: 0.0504 - acc: 0.9912 - val_loss: 0.7092 - val_acc: 0.8791Epoch 11/2025000/25000 [==============================] - 3s - loss: 0.0589 - acc: 0.9919 - val_loss: 0.6831 - val_acc: 0.8785Epoch 12/2025000/25000 [==============================] - 3s - loss: 1.9067e-04 - acc: 1.0000 - val_loss: 0.8005 - val_acc: 0.8784Epoch 13/2025000/25000 [==============================] - 3s - loss: 0.0623 - acc: 0.9916 - val_loss: 0.7540 - val_acc: 0.8740Epoch 14/2025000/25000 [==============================] - 3s - loss: 0.0274 - acc: 0.9954 - val_loss: 0.7806 - val_acc: 0.8670Epoch 15/2025000/25000 [==============================] - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.8107 - val_acc: 0.8783Epoch 16/2025000/25000 [==============================] - 3s - loss: 0.0445 - acc: 0.9943 - val_loss: 0.8394 - val_acc: 0.8702Epoch 17/2025000/25000 [==============================] - 3s - loss: 0.0268 - acc: 0.9959 - val_loss: 0.7708 - val_acc: 0.8745Epoch 18/2025000/25000 [==============================] - 3s - loss: 7.7057e-04 - acc: 1.0000 - val_loss: 0.8885 - val_acc: 0.8738Epoch 19/2025000/25000 [==============================] - 3s - loss: 0.0297 - acc: 0.9962 - val_loss: 0.8419 - val_acc: 0.8728Epoch 20/2025000/25000 [==============================] - 3s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.8896 - val_acc: 0.8682</code></pre><p>Here’s how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the<br>crosses are the initial network.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_val_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_17_0.png" alt="output"></p><p>The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also<br>more noisy.</p><p>Meanwhile, here are the training losses for our two networks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">original_train_loss = original_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>bigger_model_train_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br><br>plt.plot(epochs, original_train_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_train_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_19_0.png" alt="output"></p><p>As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be<br>able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large<br>difference between the training and validation loss).</p><h2 id="Adding-weight-regularization"><a href="#Adding-weight-regularization" class="headerlink" title="Adding weight regularization"></a>Adding weight regularization</h2><p>You may be familiar with <em>Occam’s Razor</em> principle: given two explanations for something, the explanation most likely to be correct is the<br>“simplest” one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some<br>training data and a network architecture, there are multiple sets of weights values (multiple <em>models</em>) that could explain the data, and<br>simpler models are less likely to overfit than complex ones.</p><p>A “simple model” in this context is a model where the distribution of parameter values has less entropy (or a model with fewer<br>parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity<br>of a network by forcing its weights to only take small values, which makes the distribution of weight values more “regular”. This is called<br>“weight regularization”, and it is done by adding to the loss function of the network a <em>cost</em> associated with having large weights. This<br>cost comes in two flavors:</p><ul><li>L1 regularization, where the cost added is proportional to the <em>absolute value of the weights coefficients</em> (i.e. to what is called the<br>“L1 norm” of the weights).</li><li>L2 regularization, where the cost added is proportional to the <em>square of the value of the weights coefficients</em> (i.e. to what is called<br>the “L2 norm” of the weights). L2 regularization is also called <em>weight decay</em> in the context of neural networks. Don’t let the different<br>name confuse you: weight decay is mathematically the exact same as L2 regularization.</li></ul><p>In Keras, weight regularization is added by passing <em>weight regularizer instances</em> to layers as keyword arguments. Let’s add L2 weight<br>regularization to our movie review classification network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br>l2_model = models.Sequential()<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>l2_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                 loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                 metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p><code>l2(0.001)</code> means that every coefficient in the weight matrix of the layer will add <code>0.001 * weight_coefficient_value</code> to the total loss of<br>the network. Note that because this penalty is <em>only added at training time</em>, the loss for this network will be much higher at training<br>than at test time.</p><p>Here’s the impact of our L2 regularization penalty:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_hist = l2_model.fit(x_train, y_train,<br>                             epochs=<span class="hljs-number">20</span>,<br>                             batch_size=<span class="hljs-number">512</span>,<br>                             validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4880 - acc: 0.8218 - val_loss: 0.3820 - val_acc: 0.8798Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.9068 - val_loss: 0.3353 - val_acc: 0.8896Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2742 - acc: 0.9185 - val_loss: 0.3306 - val_acc: 0.8898Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2489 - acc: 0.9288 - val_loss: 0.3363 - val_acc: 0.8866Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2420 - acc: 0.9318 - val_loss: 0.3492 - val_acc: 0.8820Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2322 - acc: 0.9359 - val_loss: 0.3567 - val_acc: 0.8788Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2254 - acc: 0.9385 - val_loss: 0.3632 - val_acc: 0.8787Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.2219 - acc: 0.9380 - val_loss: 0.3630 - val_acc: 0.8794Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.2162 - acc: 0.9430 - val_loss: 0.3704 - val_acc: 0.8763Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.2144 - acc: 0.9428 - val_loss: 0.3876 - val_acc: 0.8727Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.2091 - acc: 0.9439 - val_loss: 0.3883 - val_acc: 0.8724Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.2061 - acc: 0.9455 - val_loss: 0.3870 - val_acc: 0.8740Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.2069 - acc: 0.9445 - val_loss: 0.4073 - val_acc: 0.8714Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.2028 - acc: 0.9475 - val_loss: 0.3976 - val_acc: 0.8714Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1998 - acc: 0.9472 - val_loss: 0.4362 - val_acc: 0.8670Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.2019 - acc: 0.9462 - val_loss: 0.4088 - val_acc: 0.8711Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1953 - acc: 0.9495 - val_loss: 0.4185 - val_acc: 0.8698Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1945 - acc: 0.9508 - val_loss: 0.4371 - val_acc: 0.8674Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1934 - acc: 0.9486 - val_loss: 0.4136 - val_acc: 0.8699Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1924 - acc: 0.9504 - val_loss: 0.4200 - val_acc: 0.8704</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_val_loss = l2_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, l2_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;L2-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0p.png" alt="output"></p><p>As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses),<br>even though both models have the same number of parameters.</p><p>As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br><span class="hljs-comment"># L1 regularization</span><br>regularizers.l1(<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># L1 and L2 regularization at the same time</span><br>regularizers.l1_l2(l1=<span class="hljs-number">0.001</span>, l2=<span class="hljs-number">0.001</span>)<br></code></pre></td></tr></table></figure><h2 id="Adding-dropout"><a href="#Adding-dropout" class="headerlink" title="Adding dropout"></a>Adding dropout</h2><p>Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his<br>students at the University of Toronto. Dropout, applied to a layer, consists of randomly “dropping out” (i.e. setting to zero) a number of<br>output features of the layer during training. Let’s say a given layer would normally have returned a vector <code>[0.2, 0.5, 1.3, 0.8, 1.1]</code> for a<br>given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. <code>[0, 0.5,  1.3, 0, 1.1]</code>. The “dropout rate” is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test<br>time, no units are dropped out, and instead the layer’s output values are scaled down by a factor equal to the dropout rate, so as to<br>balance for the fact that more units are active than at training time.</p><p>Consider a Numpy matrix containing the output of a layer, <code>layer_output</code>, of shape <code>(batch_size, features)</code>. At training time, we would be<br>zero-ing out at random a fraction of the values in the matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time: we drop out 50% of the units in the output</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br></code></pre></td></tr></table></figure><p>At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the<br>units):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At test time:</span><br>layer_output *= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is<br>often the way it is implemented in practice:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time:</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br><span class="hljs-comment"># Note that we are scaling *up* rather scaling *down* in this case</span><br>layer_output /= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among<br>other things, by a fraud prevention mechanism used by banks – in his own words: <em>“I went to my bank. The tellers kept changing and I asked<br>one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation<br>between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each<br>example would prevent conspiracies and thus reduce overfitting”</em>.</p><p>The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what<br>Hinton refers to as “conspiracies”), which the network would start memorizing if no noise was present. </p><p>In Keras you can introduce dropout in a network via the <code>Dropout</code> layer, which gets applied to the output of layer right before it, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br></code></pre></td></tr></table></figure><p>Let’s add two <code>Dropout</code> layers in our IMDB network to see how well they do at reducing overfitting:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model = models.Sequential()<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>dpt_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                  loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                  metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_hist = dpt_model.fit(x_train, y_train,<br>                               epochs=<span class="hljs-number">20</span>,<br>                               batch_size=<span class="hljs-number">512</span>,<br>                               validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.6035 - acc: 0.6678 - val_loss: 0.4704 - val_acc: 0.8651Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.4622 - acc: 0.8002 - val_loss: 0.3612 - val_acc: 0.8724Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.3731 - acc: 0.8553 - val_loss: 0.2960 - val_acc: 0.8904Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.8855 - val_loss: 0.2772 - val_acc: 0.8917Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2762 - acc: 0.9033 - val_loss: 0.2803 - val_acc: 0.8889Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2454 - acc: 0.9172 - val_loss: 0.2823 - val_acc: 0.8892Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2178 - acc: 0.9281 - val_loss: 0.2982 - val_acc: 0.8877Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1994 - acc: 0.9351 - val_loss: 0.3101 - val_acc: 0.8875Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1832 - acc: 0.9400 - val_loss: 0.3318 - val_acc: 0.8860Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1692 - acc: 0.9434 - val_loss: 0.3534 - val_acc: 0.8841Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1590 - acc: 0.9483 - val_loss: 0.3689 - val_acc: 0.8830Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1499 - acc: 0.9496 - val_loss: 0.4107 - val_acc: 0.8776Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1405 - acc: 0.9539 - val_loss: 0.4114 - val_acc: 0.8782Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.1333 - acc: 0.9562 - val_loss: 0.4549 - val_acc: 0.8771Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1267 - acc: 0.9572 - val_loss: 0.4579 - val_acc: 0.8800Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.1225 - acc: 0.9580 - val_loss: 0.4843 - val_acc: 0.8772Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1233 - acc: 0.9590 - val_loss: 0.4783 - val_acc: 0.8761Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1212 - acc: 0.9601 - val_loss: 0.5051 - val_acc: 0.8740Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1153 - acc: 0.9618 - val_loss: 0.5451 - val_acc: 0.8747Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1155 - acc: 0.9621 - val_loss: 0.5358 - val_acc: 0.8738</code></pre><p>Let’s plot the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_val_loss = dpt_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, dpt_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Dropout-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_41_0.png" alt="output"></p><p>Again, a clear improvement over the reference network.</p><p>To recap: here the most common ways to prevent overfitting in neural networks:</p><ul><li>Getting more training data.</li><li>Reducing the capacity of the network.</li><li>Adding weight regularization.</li><li>Adding dropout.</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-4</title>
      <link href="2021/04/22/DL-with-python-4/"/>
      <url>2021/04/22/DL-with-python-4/</url>
      
        <content type="html"><![CDATA[<p>3.7-predicting-house-prices</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Predicting-house-prices-a-regression-example"><a href="#Predicting-house-prices-a-regression-example" class="headerlink" title="Predicting house prices: a regression example"></a>Predicting house prices: a regression example</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In our two previous examples, we were considering classification problems, where the goal was to predict a single discrete label of an<br>input data point. Another common type of machine learning problem is “regression”, which consists of predicting a continuous value instead<br>of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a<br>software project will take to complete, given its specifications.</p><p>Do not mix up “regression” with the algorithm “logistic regression”: confusingly, “logistic regression” is not a regression algorithm,<br>it is a classification algorithm.</p><h2 id="The-Boston-Housing-Price-dataset"><a href="#The-Boston-Housing-Price-dataset" class="headerlink" title="The Boston Housing Price dataset"></a>The Boston Housing Price dataset</h2><p>We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the<br>suburb at the time, such as the crime rate, the local property tax rate, etc.</p><p>The dataset we will be using has another interesting difference from our two previous examples: it has very few data points, only 506 in<br>total, split between 404 training samples and 102 test samples, and each “feature” in the input data (e.g. the crime rate is a feature) has<br>a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12,<br>others between 0 and 100…</p><p>Let’s take a look at the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> boston_housing<br><br>(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data.shape<br></code></pre></td></tr></table></figure><pre><code>(404, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data.shape<br></code></pre></td></tr></table></figure><pre><code>(102, 13)</code></pre><p>As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as<br>follow:</p><ol><li>Per capita crime rate.</li><li>Proportion of residential land zoned for lots over 25,000 square feet.</li><li>Proportion of non-retail business acres per town.</li><li>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</li><li>Nitric oxides concentration (parts per 10 million).</li><li>Average number of rooms per dwelling.</li><li>Proportion of owner-occupied units built prior to 1940.</li><li>Weighted distances to five Boston employment centres.</li><li>Index of accessibility to radial highways.</li><li>Full-value property-tax rate per $10,000.</li><li>Pupil-teacher ratio by town.</li><li>1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.</li><li>% lower status of the population.</li></ol><p>The targets are the median values of owner-occupied homes, in thousands of dollars:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_targets<br></code></pre></td></tr></table></figure><pre><code>array([ 15.2,  42.3,  50. ,  21.1,  17.7,  18.5,  11.3,  15.6,  15.6,        14.4,  12.1,  17.9,  23.1,  19.9,  15.7,   8.8,  50. ,  22.5,        24.1,  27.5,  10.9,  30.8,  32.9,  24. ,  18.5,  13.3,  22.9,        34.7,  16.6,  17.5,  22.3,  16.1,  14.9,  23.1,  34.9,  25. ,        13.9,  13.1,  20.4,  20. ,  15.2,  24.7,  22.2,  16.7,  12.7,        15.6,  18.4,  21. ,  30.1,  15.1,  18.7,   9.6,  31.5,  24.8,        19.1,  22. ,  14.5,  11. ,  32. ,  29.4,  20.3,  24.4,  14.6,        19.5,  14.1,  14.3,  15.6,  10.5,   6.3,  19.3,  19.3,  13.4,        36.4,  17.8,  13.5,  16.5,   8.3,  14.3,  16. ,  13.4,  28.6,        43.5,  20.2,  22. ,  23. ,  20.7,  12.5,  48.5,  14.6,  13.4,        23.7,  50. ,  21.7,  39.8,  38.7,  22.2,  34.9,  22.5,  31.1,        28.7,  46. ,  41.7,  21. ,  26.6,  15. ,  24.4,  13.3,  21.2,        11.7,  21.7,  19.4,  50. ,  22.8,  19.7,  24.7,  36.2,  14.2,        18.9,  18.3,  20.6,  24.6,  18.2,   8.7,  44. ,  10.4,  13.2,        21.2,  37. ,  30.7,  22.9,  20. ,  19.3,  31.7,  32. ,  23.1,        18.8,  10.9,  50. ,  19.6,   5. ,  14.4,  19.8,  13.8,  19.6,        23.9,  24.5,  25. ,  19.9,  17.2,  24.6,  13.5,  26.6,  21.4,        11.9,  22.6,  19.6,   8.5,  23.7,  23.1,  22.4,  20.5,  23.6,        18.4,  35.2,  23.1,  27.9,  20.6,  23.7,  28. ,  13.6,  27.1,        23.6,  20.6,  18.2,  21.7,  17.1,   8.4,  25.3,  13.8,  22.2,        18.4,  20.7,  31.6,  30.5,  20.3,   8.8,  19.2,  19.4,  23.1,        23. ,  14.8,  48.8,  22.6,  33.4,  21.1,  13.6,  32.2,  13.1,        23.4,  18.9,  23.9,  11.8,  23.3,  22.8,  19.6,  16.7,  13.4,        22.2,  20.4,  21.8,  26.4,  14.9,  24.1,  23.8,  12.3,  29.1,        21. ,  19.5,  23.3,  23.8,  17.8,  11.5,  21.7,  19.9,  25. ,        33.4,  28.5,  21.4,  24.3,  27.5,  33.1,  16.2,  23.3,  48.3,        22.9,  22.8,  13.1,  12.7,  22.6,  15. ,  15.3,  10.5,  24. ,        18.5,  21.7,  19.5,  33.2,  23.2,   5. ,  19.1,  12.7,  22.3,        10.2,  13.9,  16.3,  17. ,  20.1,  29.9,  17.2,  37.3,  45.4,        17.8,  23.2,  29. ,  22. ,  18. ,  17.4,  34.6,  20.1,  25. ,        15.6,  24.8,  28.2,  21.2,  21.4,  23.8,  31. ,  26.2,  17.4,        37.9,  17.5,  20. ,   8.3,  23.9,   8.4,  13.8,   7.2,  11.7,        17.1,  21.6,  50. ,  16.1,  20.4,  20.6,  21.4,  20.6,  36.5,         8.5,  24.8,  10.8,  21.9,  17.3,  18.9,  36.2,  14.9,  18.2,        33.3,  21.8,  19.7,  31.6,  24.8,  19.4,  22.8,   7.5,  44.8,        16.8,  18.7,  50. ,  50. ,  19.5,  20.1,  50. ,  17.2,  20.8,        19.3,  41.3,  20.4,  20.5,  13.8,  16.5,  23.9,  20.6,  31.5,        23.3,  16.8,  14. ,  33.8,  36.1,  12.8,  18.3,  18.7,  19.1,        29. ,  30.1,  50. ,  50. ,  22. ,  11.9,  37.6,  50. ,  22.7,        20.8,  23.5,  27.9,  50. ,  19.3,  23.9,  22.6,  15.2,  21.7,        19.2,  43.8,  20.3,  33.2,  19.9,  22.5,  32.7,  22. ,  17.1,        19. ,  15. ,  16.1,  25.1,  23.7,  28.7,  37.2,  22.6,  16.4,        25. ,  29.8,  22.1,  17.4,  18.1,  30.3,  17.5,  24.7,  12.6,        26.5,  28.7,  13.3,  10.4,  24.4,  23. ,  20. ,  17.8,   7. ,        11.8,  24.4,  13.8,  19.4,  25.2,  19.4,  19.4,  29.1])</code></pre><p>The prices are typically between $10,000 and $50,000. If that sounds cheap, remember this was the mid-1970s, and these prices are not<br>inflation-adjusted.</p><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to<br>automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal<br>with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we<br>will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a<br>unit standard deviation. This is easily done in Numpy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">mean = train_data.mean(axis=<span class="hljs-number">0</span>)<br>train_data -= mean<br>std = train_data.std(axis=<span class="hljs-number">0</span>)<br>train_data /= std<br><br>test_data -= mean<br>test_data /= std<br></code></pre></td></tr></table></figure><p>Note that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our<br>workflow any quantity computed on the test data, even for something as simple as data normalization.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Because so few samples are available, we will be using a very small network with two<br>hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using<br>a small network is one way to mitigate overfitting.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>():</span><br>    <span class="hljs-comment"># Because we will need to instantiate</span><br>    <span class="hljs-comment"># the same model multiple times,</span><br>    <span class="hljs-comment"># we use a function to construct it.</span><br>    model = models.Sequential()<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                           input_shape=(train_data.shape[<span class="hljs-number">1</span>],)))<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>    model.add(layers.Dense(<span class="hljs-number">1</span>))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;mse&#x27;</span>, metrics=[<span class="hljs-string">&#x27;mae&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>Our network ends with a single unit, and no activation (i.e. it will be linear layer).<br>This is a typical setup for scalar regression (i.e. regression where we are trying to predict a single continuous value).<br>Applying an activation function would constrain the range that the output can take; for instance if<br>we applied a <code>sigmoid</code> activation function to our last layer, the network could only learn to predict values between 0 and 1. Here, because<br>the last layer is purely linear, the network is free to learn to predict values in any range.</p><p>Note that we are compiling the network with the <code>mse</code> loss function – Mean Squared Error, the square of the difference between the<br>predictions and the targets, a widely used loss function for regression problems.</p><p>We are also monitoring a new metric during training: <code>mae</code>. This stands for Mean Absolute Error. It is simply the absolute value of the<br>difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by<br>$500 on average.</p><h2 id="Validating-our-approach-using-K-fold-validation"><a href="#Validating-our-approach-using-K-fold-validation" class="headerlink" title="Validating our approach using K-fold validation"></a>Validating our approach using K-fold validation</h2><p>To evaluate our network while we keep adjusting its parameters (such as the number of epochs used for training), we could simply split the<br>data into a training set and a validation set, as we were doing in our previous examples. However, because we have so few data points, the<br>validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot<br>depending on <em>which</em> data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a<br>high <em>variance</em> with regard to the validation split. This would prevent us from reliably evaluating our model.</p><p>The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions<br>(typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining<br>partition. The validation score for the model used would then be the average of the K validation scores obtained.</p><p>In terms of code, this is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>k = <span class="hljs-number">4</span><br>num_val_samples = <span class="hljs-built_in">len</span>(train_data) // k<br>num_epochs = <span class="hljs-number">100</span><br>all_scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    model.fit(partial_train_data, partial_train_targets,<br>              epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Evaluate the model on the validation data</span><br>    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="hljs-number">0</span>)<br>    all_scores.append(val_mae)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">all_scores<br></code></pre></td></tr></table></figure><pre><code>[2.0750808349930412, 2.117215852926273, 2.9140411863232605, 2.4288365227161068]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean(all_scores)<br></code></pre></td></tr></table></figure><pre><code>2.3837935992396706</code></pre><p>As you can notice, the different runs do indeed show rather different validation scores, from 2.1 to 2.9. Their average (2.4) is a much more<br>reliable metric than any single of these scores – that’s the entire point of K-fold cross-validation. In this case, we are off by $2,400 on<br>average, which is still significant considering that the prices range from $10,000 to $50,000. </p><p>Let’s try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop<br>to save the per-epoch validation score log:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><br><span class="hljs-comment"># Some memory clean-up</span><br>K.clear_session()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">500</span><br>all_mae_histories = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    history = model.fit(partial_train_data, partial_train_targets,<br>                        validation_data=(val_data, val_targets),<br>                        epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    mae_history = history.history[<span class="hljs-string">&#x27;val_mean_absolute_error&#x27;</span>]<br>    all_mae_histories.append(mae_history)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><p>We can then compute the average of the per-epoch MAE scores for all folds:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">average_mae_history = [<br>    np.mean([x[i] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_mae_histories]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs)]<br></code></pre></td></tr></table></figure><p>Let’s plot this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(average_mae_history) + <span class="hljs-number">1</span>), average_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0.png" alt="output"></p><p>It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let’s:</p><ul><li>Omit the first 10 data points, which are on a different scale from the rest of the curve.</li><li>Replace each point with an exponential moving average of the previous points, to obtain a smooth curve.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smooth_curve</span>(<span class="hljs-params">points, factor=<span class="hljs-number">0.9</span></span>):</span><br>  smoothed_points = []<br>  <span class="hljs-keyword">for</span> point <span class="hljs-keyword">in</span> points:<br>    <span class="hljs-keyword">if</span> smoothed_points:<br>      previous = smoothed_points[-<span class="hljs-number">1</span>]<br>      smoothed_points.append(previous * factor + point * (<span class="hljs-number">1</span> - factor))<br>    <span class="hljs-keyword">else</span>:<br>      smoothed_points.append(point)<br>  <span class="hljs-keyword">return</span> smoothed_points<br><br>smooth_mae_history = smooth_curve(average_mae_history[<span class="hljs-number">10</span>:])<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(smooth_mae_history) + <span class="hljs-number">1</span>), smooth_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_28_0.png" alt="output"></p><p>According to this plot, it seems that validation MAE stops improving significantly after 80 epochs. Past that point, we start overfitting.</p><p>Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we<br>can train a final “production” model on all of the training data, with the best parameters, then look at its performance on the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Get a fresh, compiled model.</span><br>model = build_model()<br><span class="hljs-comment"># Train it on the entirety of the data.</span><br>model.fit(train_data, train_targets,<br>          epochs=<span class="hljs-number">80</span>, batch_size=<span class="hljs-number">16</span>, verbose=<span class="hljs-number">0</span>)<br>test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)<br></code></pre></td></tr></table></figure><pre><code> 32/102 [========&gt;.....................] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_mae_score<br></code></pre></td></tr></table></figure><pre><code>2.5532484335057877</code></pre><p>We are still off by about $2,550.</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>Regression is done using different loss functions from classification; Mean Squared Error (MSE) is a commonly used loss function for<br>regression.</li><li>Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally the concept of “accuracy”<br>does not apply for regression. A common regression metric is Mean Absolute Error (MAE).</li><li>When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.</li><li>When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.</li><li>When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two),<br>in order to avoid severe overfitting.</li></ul><p>This example concludes our series of three introductory practical examples. You are now able to handle common types of problems with vector data input:</p><ul><li>Binary (2-class) classification.</li><li>Multi-class, single-label classification.</li><li>Scalar regression.</li></ul><p>In the next chapter, you will acquire a more formal understanding of some of the concepts you have encountered in these first examples,<br>such as data preprocessing, model evaluation, and overfitting.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-3</title>
      <link href="2021/04/22/DL-with-python-3/"/>
      <url>2021/04/22/DL-with-python-3/</url>
      
        <content type="html"><![CDATA[<p>3.6-classifying-newswires</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Classifying-newswires-a-multi-class-classification-example"><a href="#Classifying-newswires-a-multi-class-classification-example" class="headerlink" title="Classifying newswires: a multi-class classification example"></a>Classifying newswires: a multi-class classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network.<br>But what happens when you have more than two classes? </p><p>In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many<br>classes, this problem is an instance of “multi-class classification”, and since each data point should be classified into only one<br>category, the problem is more specifically an instance of “single-label, multi-class classification”. If each data point could have<br>belonged to multiple categories (in our case, topics) then we would be facing a “multi-label, multi-class classification” problem.</p><h2 id="The-Reuters-dataset"><a href="#The-Reuters-dataset" class="headerlink" title="The Reuters dataset"></a>The Reuters dataset</h2><p>We will be working with the <em>Reuters dataset</em>, a set of short newswires and their topics, published by Reuters in 1986. It’s a very simple,<br>widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each<br>topic has at least 10 examples in the training set.</p><p>Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s take a look right away:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> reuters<br><br>(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>Like with the IMDB dataset, the argument <code>num_words=10000</code> restricts the data to the 10,000 most frequently occurring words found in the<br>data.</p><p>We have 8,982 training examples and 2,246 test examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_data)<br></code></pre></td></tr></table></figure><pre><code>8982</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_data)<br></code></pre></td></tr></table></figure><pre><code>2246</code></pre><p>As with the IMDB reviews, each example is a list of integers (word indices):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]</code></pre><p>Here’s how you can decode it back to words, in case you are curious:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">word_index = reuters.get_word_index()<br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># Note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_newswire = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_newswire<br></code></pre></td></tr></table></figure><pre><code>&#39;? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3&#39;</code></pre><p>The label associated with an example is an integer between 0 and 45: a topic index.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We can vectorize the data with the exact same code as in our previous example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a “one-hot”<br>encoding. One-hot encoding is a widely used format for categorical data, also called “categorical encoding”.<br>For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1.<br>In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_one_hot</span>(<span class="hljs-params">labels, dimension=<span class="hljs-number">46</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(labels), dimension))<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        results[i, label] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training labels</span><br>one_hot_train_labels = to_one_hot(train_labels)<br><span class="hljs-comment"># Our vectorized test labels</span><br>one_hot_test_labels = to_one_hot(test_labels)<br></code></pre></td></tr></table></figure><p>Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils.np_utils <span class="hljs-keyword">import</span> to_categorical<br><br>one_hot_train_labels = to_categorical(train_labels)<br>one_hot_test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to<br>classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the<br>dimensionality of the output space is much larger. </p><p>In a stack of <code>Dense</code> layers like what we were using, each layer can only access information present in the output of the previous layer.<br>If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each<br>layer can potentially become an “information bottleneck”. In our previous example, we were using 16-dimensional intermediate layers, but a<br>16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks,<br>permanently dropping relevant information.</p><p>For this reason we will use larger layers. Let’s go with 64 units:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>There are two other things you should note about this architecture:</p><ul><li>We are ending the network with a <code>Dense</code> layer of size 46. This means that for each input sample, our network will output a<br>46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.</li><li>The last layer uses a <code>softmax</code> activation. You have already seen this pattern in the MNIST example. It means that the network will<br>output a <em>probability distribution</em> over the 46 different output classes, i.e. for every input sample, the network will produce a<br>46-dimensional output vector where <code>output[i]</code> is the probability that the sample belongs to class <code>i</code>. The 46 scores will sum to 1.</li></ul><p>The best loss function to use in this case is <code>categorical_crossentropy</code>. It measures the distance between two probability distributions:<br>in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the<br>distance between these two distributions, we train our network to output something as close as possible to the true labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>Let’s set apart 1,000 samples in our training data to use as a validation set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">1000</span>]<br>partial_x_train = x_train[<span class="hljs-number">1000</span>:]<br><br>y_val = one_hot_train_labels[:<span class="hljs-number">1000</span>]<br>partial_y_train = one_hot_train_labels[<span class="hljs-number">1000</span>:]<br></code></pre></td></tr></table></figure><p>Now let’s train our network for 20 epochs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 1s - loss: 2.5241 - acc: 0.4952 - val_loss: 1.7263 - val_acc: 0.6100Epoch 2/207982/7982 [==============================] - 0s - loss: 1.4500 - acc: 0.6854 - val_loss: 1.3478 - val_acc: 0.7070Epoch 3/207982/7982 [==============================] - 0s - loss: 1.0979 - acc: 0.7643 - val_loss: 1.1736 - val_acc: 0.7460Epoch 4/207982/7982 [==============================] - 0s - loss: 0.8723 - acc: 0.8178 - val_loss: 1.0880 - val_acc: 0.7490Epoch 5/207982/7982 [==============================] - 0s - loss: 0.7045 - acc: 0.8477 - val_loss: 0.9822 - val_acc: 0.7760Epoch 6/207982/7982 [==============================] - 0s - loss: 0.5660 - acc: 0.8792 - val_loss: 0.9379 - val_acc: 0.8030Epoch 7/207982/7982 [==============================] - 0s - loss: 0.4569 - acc: 0.9037 - val_loss: 0.9039 - val_acc: 0.8050Epoch 8/207982/7982 [==============================] - 0s - loss: 0.3668 - acc: 0.9238 - val_loss: 0.9279 - val_acc: 0.7890Epoch 9/207982/7982 [==============================] - 0s - loss: 0.3000 - acc: 0.9326 - val_loss: 0.8835 - val_acc: 0.8070Epoch 10/207982/7982 [==============================] - 0s - loss: 0.2505 - acc: 0.9434 - val_loss: 0.8967 - val_acc: 0.8150Epoch 11/207982/7982 [==============================] - 0s - loss: 0.2155 - acc: 0.9473 - val_loss: 0.9080 - val_acc: 0.8110Epoch 12/207982/7982 [==============================] - 0s - loss: 0.1853 - acc: 0.9506 - val_loss: 0.9025 - val_acc: 0.8140Epoch 13/207982/7982 [==============================] - 0s - loss: 0.1680 - acc: 0.9524 - val_loss: 0.9268 - val_acc: 0.8100Epoch 14/207982/7982 [==============================] - 0s - loss: 0.1512 - acc: 0.9562 - val_loss: 0.9500 - val_acc: 0.8130Epoch 15/207982/7982 [==============================] - 0s - loss: 0.1371 - acc: 0.9559 - val_loss: 0.9621 - val_acc: 0.8090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.1306 - acc: 0.9553 - val_loss: 1.0152 - val_acc: 0.8050Epoch 17/207982/7982 [==============================] - 0s - loss: 0.1210 - acc: 0.9575 - val_loss: 1.0262 - val_acc: 0.8010Epoch 18/207982/7982 [==============================] - 0s - loss: 0.1185 - acc: 0.9570 - val_loss: 1.0354 - val_acc: 0.8040Epoch 19/207982/7982 [==============================] - 0s - loss: 0.1128 - acc: 0.9598 - val_loss: 1.0841 - val_acc: 0.8010Epoch 20/207982/7982 [==============================] - 0s - loss: 0.1097 - acc: 0.9594 - val_loss: 1.0707 - val_acc: 0.8040</code></pre><p>Let’s display its loss and accuracy curves:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(loss) + <span class="hljs-number">1</span>)<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_29_0.png" alt="output"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_30_0.png" alt="output"></p><p>It seems that the network starts overfitting after 8 epochs. Let’s train a new network from scratch for 8 epochs, then let’s evaluate it on<br>the test set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">8</span>,<br>          batch_size=<span class="hljs-number">512</span>,<br>          validation_data=(x_val, y_val))<br>results = model.evaluate(x_test, one_hot_test_labels)<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/87982/7982 [==============================] - 0s - loss: 2.6118 - acc: 0.4667 - val_loss: 1.7207 - val_acc: 0.6360Epoch 2/87982/7982 [==============================] - 0s - loss: 1.3998 - acc: 0.7107 - val_loss: 1.2645 - val_acc: 0.7360Epoch 3/87982/7982 [==============================] - 0s - loss: 1.0343 - acc: 0.7839 - val_loss: 1.0994 - val_acc: 0.7700Epoch 4/87982/7982 [==============================] - 0s - loss: 0.8114 - acc: 0.8329 - val_loss: 1.0252 - val_acc: 0.7820Epoch 5/87982/7982 [==============================] - 0s - loss: 0.6466 - acc: 0.8628 - val_loss: 0.9536 - val_acc: 0.8070Epoch 6/87982/7982 [==============================] - 0s - loss: 0.5271 - acc: 0.8894 - val_loss: 0.9187 - val_acc: 0.8110Epoch 7/87982/7982 [==============================] - 0s - loss: 0.4193 - acc: 0.9126 - val_loss: 0.9051 - val_acc: 0.8120Epoch 8/87982/7982 [==============================] - 0s - loss: 0.3478 - acc: 0.9258 - val_loss: 0.8891 - val_acc: 0.81601952/2246 [=========================&gt;....] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure><pre><code>[0.98764628548762257, 0.77693677651807869]</code></pre><p>Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier<br>would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><br>test_labels_copy = copy.copy(test_labels)<br>np.random.shuffle(test_labels_copy)<br><span class="hljs-built_in">float</span>(np.<span class="hljs-built_in">sum</span>(np.array(test_labels) == np.array(test_labels_copy))) / <span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>0.18477292965271594</code></pre><h2 id="Generating-predictions-on-new-data"><a href="#Generating-predictions-on-new-data" class="headerlink" title="Generating predictions on new data"></a>Generating predictions on new data</h2><p>We can verify that the <code>predict</code> method of our model instance returns a probability distribution over all 46 topics. Let’s generate topic<br>predictions for all of the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions = model.predict(x_test)<br></code></pre></td></tr></table></figure><p>Each entry in <code>predictions</code> is a vector of length 46:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions[<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></table></figure><pre><code>(46,)</code></pre><p>The coefficients in this vector sum to 1:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.<span class="hljs-built_in">sum</span>(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>0.99999994</code></pre><p>The largest entry is the predicted class, i.e. the class with the highest probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.argmax(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="A-different-way-to-handle-the-labels-and-the-loss"><a href="#A-different-way-to-handle-the-labels-and-the-loss" class="headerlink" title="A different way to handle the labels and the loss"></a>A different way to handle the labels and the loss</h2><p>We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y_train = np.array(train_labels)<br>y_test = np.array(test_labels)<br></code></pre></td></tr></table></figure><p>The only thing it would change is the choice of the loss function. Our previous loss, <code>categorical_crossentropy</code>, expects the labels to<br>follow a categorical encoding. With integer labels, we should use <code>sparse_categorical_crossentropy</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>This new loss function is still mathematically the same as <code>categorical_crossentropy</code>; it just has a different interface.</p><h2 id="On-the-importance-of-having-sufficiently-large-intermediate-layers"><a href="#On-the-importance-of-having-sufficiently-large-intermediate-layers" class="headerlink" title="On the importance of having sufficiently large intermediate layers"></a>On the importance of having sufficiently large intermediate layers</h2><p>We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden<br>units. Now let’s try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than<br>46-dimensional, e.g. 4-dimensional.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">20</span>,<br>          batch_size=<span class="hljs-number">128</span>,<br>          validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740Epoch 2/207982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990Epoch 3/207982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390Epoch 4/207982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570Epoch 5/207982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700Epoch 6/207982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800Epoch 7/207982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810Epoch 8/207982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960Epoch 9/207982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050Epoch 10/207982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020Epoch 11/207982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020Epoch 12/207982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080Epoch 13/207982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110Epoch 14/207982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080Epoch 15/207982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080Epoch 17/207982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060Epoch 18/207982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950Epoch 19/207982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930Epoch 20/207982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060&lt;keras.callbacks.History at 0x7f8ce7cdb9b0&gt;</code></pre><p>Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to<br>compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is<br>too low-dimensional. The network is able to cram <em>most</em> of the necessary information into these 8-dimensional representations, but not all<br>of it.</p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>Try using larger or smaller layers: 32 units, 128 units…</li><li>We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers.</li></ul><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>If you are trying to classify data points between N classes, your network should end with a <code>Dense</code> layer of size N.</li><li>In a single-label, multi-class classification problem, your network should end with a <code>softmax</code> activation, so that it will output a<br>probability distribution over the N output classes.</li><li><em>Categorical crossentropy</em> is almost always the loss function you should use for such problems. It minimizes the distance between the<br>probability distributions output by the network, and the true distribution of the targets.</li><li>There are two ways to handle labels in multi-class classification:<br>  ** Encoding the labels via “categorical encoding” (also known as “one-hot encoding”) and using <code>categorical_crossentropy</code> as your loss<br>function.<br>  ** Encoding the labels as integers and using the <code>sparse_categorical_crossentropy</code> loss function.</li><li>If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having<br>intermediate layers that are too small.</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-2</title>
      <link href="2021/04/22/DL-with-python-2/"/>
      <url>2021/04/22/DL-with-python-2/</url>
      
        <content type="html"><![CDATA[<p>3.5-classifying-movie-reviews</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="Classifying-movie-reviews-a-binary-classification-example"><a href="#Classifying-movie-reviews-a-binary-classification-example" class="headerlink" title="Classifying movie reviews: a binary classification example"></a>Classifying movie reviews: a binary classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we<br>will learn to classify movie reviews into “positive” reviews and “negative” reviews, just based on the text content of the reviews.</p><h2 id="The-IMDB-dataset"><a href="#The-IMDB-dataset" class="headerlink" title="The IMDB dataset"></a>The IMDB dataset</h2><p>We’ll be working with “IMDB dataset”, a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000<br>reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.</p><p>Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to<br>train it! Just because a model performs well on its training data doesn’t mean that it will perform well on data it has never seen, and<br>what you actually care about is your model’s performance on new data (since you already know the labels of your training data – obviously<br>you don’t need your model to predict those). For instance, it is possible that your model could end up merely <em>memorizing</em> a mapping between<br>your training samples and their targets – which would be completely useless for the task of predicting targets for data never seen before.<br>We will go over this point in much more detail in the next chapter.</p><p>Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words)<br>have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.</p><p>The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>The argument <code>num_words=10000</code> means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words<br>will be discarded. This allows us to work with vector data of manageable size.</p><p>The variables <code>train_data</code> and <code>test_data</code> are lists of reviews, each review being a list of word indices (encoding a sequence of words).<br><code>train_labels</code> and <code>test_labels</code> are lists of 0s and 1s, where 0 stands for “negative” and 1 stands for “positive”:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>1</code></pre><p>Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">max</span>([<span class="hljs-built_in">max</span>(sequence) <span class="hljs-keyword">for</span> sequence <span class="hljs-keyword">in</span> train_data])<br></code></pre></td></tr></table></figure><pre><code>9999</code></pre><p>For kicks, here’s how you can quickly decode one of these reviews back to English words:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># word_index is a dictionary mapping words to an integer index</span><br>word_index = imdb.get_word_index()<br><span class="hljs-comment"># We reverse it, mapping integer indices to words</span><br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># We decode the review; note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_review = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_review<br></code></pre></td></tr></table></figure><pre><code>&quot;? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all&quot;</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:</p><ul><li>We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape <code>(samples, word_indices)</code>,<br>then use as first layer in our network a layer capable of handling such integer tensors (the <code>Embedding</code> layer, which we will cover in<br>detail later in the book).</li><li>We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence<br><code>[3, 5]</code> into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as<br>first layer in our network a <code>Dense</code> layer, capable of handling floating point vector data.</li></ul><p>We will go with the latter solution. Let’s vectorize our data, which we will do manually for maximum clarity:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>Here’s what our samples look like now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>array([0., 1., 1., ..., 0., 0., 0.])</code></pre><p>We should also vectorize our labels, which is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Now our data is ready to be fed into a neural network.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of<br>network that performs well on such a problem would be a simple stack of fully-connected (<code>Dense</code>) layers with <code>relu</code> activations: <code>Dense(16,  activation=&#39;relu&#39;)</code></p><p>The argument being passed to each <code>Dense</code> layer (16) is the number of “hidden units” of the layer. What’s a hidden unit? It’s a dimension<br>in the representation space of the layer. You may remember from the previous chapter that each such <code>Dense</code> layer with a <code>relu</code> activation implements<br>the following chain of tensor operations:</p><p><code>output = relu(dot(W, input) + b)</code></p><p>Having 16 hidden units means that the weight matrix <code>W</code> will have shape <code>(input_dimension, 16)</code>, i.e. the dot product with <code>W</code> will project the<br>input data onto a 16-dimensional representation space (and then we would add the bias vector <code>b</code> and apply the <code>relu</code> operation). You can<br>intuitively understand the dimensionality of your representation space as “how much freedom you are allowing the network to have when<br>learning internal representations”. Having more hidden units (a higher-dimensional representation space) allows your network to learn more<br>complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that<br>will improve performance on the training data but not on the test data).</p><p>There are two key architecture decisions to be made about such stack of dense layers:</p><ul><li>How many layers to use.</li><li>How many “hidden units” to chose for each layer.</li></ul><p>In the next chapter, you will learn formal principles to guide you in making these choices.<br>For the time being, you will have to trust us with the following architecture choice:<br>two intermediate layers with 16 hidden units each,<br>and a third layer which will output the scalar prediction regarding the sentiment of the current review.<br>The intermediate layers will use <code>relu</code> as their “activation function”,<br>and the final layer will use a sigmoid activation so as to output a probability<br>(a score between 0 and 1, indicating how likely the sample is to have the target “1”, i.e. how likely the review is to be positive).<br>A <code>relu</code> (rectified linear unit) is a function meant to zero-out negative values,<br>while a sigmoid “squashes” arbitrary values into the <code>[0, 1]</code> interval, thus outputting something that can be interpreted as a probability.</p><p>Here’s what our network looks like:</p><p><img src="https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png" alt="3-layer network"></p><p>And here’s the Keras implementation, very similar to the MNIST example you saw previously:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network<br>is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the <code>binary_crossentropy</code> loss.<br>It isn’t the only viable choice: you could use, for instance, <code>mean_squared_error</code>. But crossentropy is usually the best choice when you<br>are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the “distance”<br>between probability distributions, or in our case, between the ground-truth distribution and our predictions.</p><p>Here’s the step where we configure our model with the <code>rmsprop</code> optimizer and the <code>binary_crossentropy</code> loss function. Note that we will<br>also monitor accuracy during training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>We are passing our optimizer, loss function and metrics as strings, which is possible because <code>rmsprop</code>, <code>binary_crossentropy</code> and<br><code>accuracy</code> are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss<br>function or metric function. This former can be done by passing an optimizer class instance as the <code>optimizer</code> argument:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>The latter can be done by passing function objects as the <code>loss</code> or <code>metrics</code> arguments:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> losses<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> metrics<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=losses.binary_crossentropy,<br>              metrics=[metrics.binary_accuracy])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a “validation set” by<br>setting apart 10,000 samples from the original training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">10000</span>]<br>partial_x_train = x_train[<span class="hljs-number">10000</span>:]<br><br>y_val = y_train[:<span class="hljs-number">10000</span>]<br>partial_y_train = y_train[<span class="hljs-number">10000</span>:]<br></code></pre></td></tr></table></figure><p>We will now train our model for 20 epochs (20 iterations over all samples in the <code>x_train</code> and <code>y_train</code> tensors), in mini-batches of 512<br>samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the<br>validation data as the <code>validation_data</code> argument:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------MemoryError                               Traceback (most recent call last)&lt;ipython-input-20-90a50ee7d258&gt; in &lt;module&gt;----&gt; 1 history = model.fit(partial_x_train,      2                     partial_y_train,      3                     epochs=20,      4                     batch_size=512,      5                     validation_data=(x_val, y_val))D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 572. MiB for an array with shape (15000, 10000) and data type float32</code></pre><p>On CPU, this will take less than two seconds per epoch – training is over in 20 seconds. At the end of every epoch, there is a slight pause<br>as the model computes its loss and accuracy on the 10,000 samples of the validation data.</p><p>Note that the call to <code>model.fit()</code> returns a <code>History</code> object. This object has a member <code>history</code>, which is a dictionary containing data<br>about everything that happened during training. Let’s take a look at it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">history_dict = history.history<br>history_dict.keys()<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)&lt;ipython-input-21-fc51a3f187a7&gt; in &lt;module&gt;----&gt; 1 history_dict = history.history      2 history_dict.keys()NameError: name &#39;history&#39; is not defined</code></pre><p>It contains 4 entries: one per metric that was being monitored, during training and during validation. Let’s use Matplotlib to plot the<br>training and validation loss side by side, as well as the training and validation accuracy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(acc) + <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br><span class="hljs-comment"># b is for &quot;solid blue line&quot;</span><br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)&lt;ipython-input-22-fa6555deaae9&gt; in &lt;module&gt;      1 import matplotlib.pyplot as plt      2----&gt; 3 acc = history.history[&#39;acc&#39;]      4 val_acc = history.history[&#39;val_acc&#39;]      5 loss = history.history[&#39;loss&#39;]NameError: name &#39;history&#39; is not defined</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br>acc_values = history_dict[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc_values = history_dict[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)&lt;ipython-input-23-cc3500a51e0d&gt; in &lt;module&gt;      1 plt.clf()   # clear figure----&gt; 2 acc_values = history_dict[&#39;acc&#39;]      3 val_acc_values = history_dict[&#39;val_acc&#39;]      4      5 plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)NameError: name &#39;history_dict&#39; is not defined&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary<br>slightly due to a different random initialization of your network.</p><p>As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That’s what you would<br>expect when running gradient descent optimization – the quantity you are trying to minimize should get lower with every iteration. But that<br>isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning<br>against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen<br>before. In precise terms, what you are seeing is “overfitting”: after the second epoch, we are over-optimizing on the training data, and we<br>ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.</p><p>In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can<br>leverage to mitigate overfitting, which we will cover in the next chapter.</p><p>Let’s train a new network from scratch for four epochs, then evaluate it on our test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>model.fit(x_train, y_train, epochs=<span class="hljs-number">4</span>, batch_size=<span class="hljs-number">512</span>)<br>results = model.evaluate(x_test, y_test)<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------MemoryError                               Traceback (most recent call last)&lt;ipython-input-24-82373714f0fe&gt; in &lt;module&gt;      8               metrics=[&#39;accuracy&#39;])      9---&gt; 10 model.fit(x_train, y_train, epochs=4, batch_size=512)     11 results = model.evaluate(x_test, y_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)&lt;ipython-input-25-100f62972f2f&gt; in &lt;module&gt;----&gt; 1 resultsNameError: name &#39;results&#39; is not defined</code></pre><p>Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.</p><h2 id="Using-a-trained-network-to-generate-predictions-on-new-data"><a href="#Using-a-trained-network-to-generate-predictions-on-new-data" class="headerlink" title="Using a trained network to generate predictions on new data"></a>Using a trained network to generate predictions on new data</h2><p>After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive<br>by using the <code>predict</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.predict(x_test)<br></code></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------MemoryError                               Traceback (most recent call last)&lt;ipython-input-26-a556f0421074&gt; in &lt;module&gt;----&gt; 1 model.predict(x_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)   1596                         &#39;. Consider setting it to AutoShardPolicy.DATA.&#39;)   1597-&gt; 1598       data_handler = data_adapter.DataHandler(   1599           x=x,   1600           batch_size=batch_size,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><p>As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). </p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.</li><li>Try to use layers with more hidden units or less hidden units: 32 units, 64 units…</li><li>Try to use the <code>mse</code> loss function instead of <code>binary_crossentropy</code>.</li><li>Try to use the <code>tanh</code> activation (an activation that was popular in the early days of neural networks) instead of <code>relu</code>.</li></ul><p>These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be<br>improved!</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Here’s what you should take away from this example:</p><ul><li>There’s usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it – as tensors – into a neural<br>network. In the case of sequences of words, they can be encoded as binary vectors – but there are other encoding options too.</li><li>Stacks of <code>Dense</code> layers with <code>relu</code> activations can solve a wide range of problems (including sentiment classification), and you will<br>likely use them frequently.</li><li>In a binary classification problem (two output classes), your network should end with a <code>Dense</code> layer with 1 unit and a <code>sigmoid</code> activation,<br>i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.</li><li>With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is <code>binary_crossentropy</code>.</li><li>The <code>rmsprop</code> optimizer is generally a good enough choice of optimizer, whatever your problem. That’s one less thing for you to worry<br>about.</li><li>As they get better on their training data, neural networks eventually start <em>overfitting</em> and end up obtaining increasingly worse results on data<br>never-seen-before. Make sure to always monitor performance on data that is outside of the training set.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><p><strong>不知道这个破电脑怎么一回事，*的，流下贫穷的泪水</strong></p><p><img src="/img/4.22.1.png" alt="艹"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-1</title>
      <link href="2021/04/20/DL-with-python-1/"/>
      <url>2021/04/20/DL-with-python-1/</url>
      
        <content type="html"><![CDATA[<p>2.1-a-first-look-at-a-neural-network</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="A-first-look-at-a-neural-network"><a href="#A-first-look-at-a-neural-network" class="headerlink" title="A first look at a neural network"></a>A first look at a neural network</h1><p>This notebook contains the code samples found in Chapter 2, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify<br>hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this<br>first example right away. You probably haven’t even installed Keras yet. Don’t worry, that is perfectly fine. In the next chapter, we will<br>review each element in our example and explain them in detail. So don’t worry if some steps seem arbitrary or look like magic to you!<br>We’ve got to start somewhere.</p><p>The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10<br>categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been<br>around for almost as long as the field itself and has been very intensively studied. It’s a set of 60,000 training images, plus 10,000 test<br>images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST<br>as the “Hello World” of deep learning – it’s what you do to verify that your algorithms are working as expected. As you become a machine<br>learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.</p><p>The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist<br><br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br></code></pre></td></tr></table></figure><p><code>train_images</code> and <code>train_labels</code> form the “training set”, the data that the model will learn from. The model will then be tested on the<br>“test set”, <code>test_images</code> and <code>test_labels</code>. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging<br>from 0 to 9. There is a one-to-one correspondence between the images and the labels.</p><p>Let’s have a look at the training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images.shape<br></code></pre></td></tr></table></figure><pre><code>(60000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_labels)<br></code></pre></td></tr></table></figure><pre><code>60000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels<br></code></pre></td></tr></table></figure><pre><code>array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</code></pre><p>Let’s have a look at the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_images.shape<br></code></pre></td></tr></table></figure><pre><code>(10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_labels<br></code></pre></td></tr></table></figure><pre><code>array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</code></pre><p>Our workflow will be as follow: first we will present our neural network with the training data, <code>train_images</code> and <code>train_labels</code>. The<br>network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for <code>test_images</code>, and we<br>will verify if these predictions match the labels from <code>test_labels</code>.</p><p>Let’s build our network – again, remember that you aren’t supposed to understand everything about this example just yet.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>network = models.Sequential()<br>network.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>,)))<br>network.add(layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>The core building block of neural networks is the “layer”, a data-processing module which you can conceive as a “filter” for data. Some<br>data comes in, and comes out in a more useful form. Precisely, layers extract <em>representations</em> out of the data fed into them – hopefully<br>representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers<br>which will implement a form of progressive “data distillation”. A deep learning model is like a sieve for data processing, made of a<br>succession of increasingly refined data filters – the “layers”.</p><p>Here our network consists of a sequence of two <code>Dense</code> layers, which are densely-connected (also called “fully-connected”) neural layers.<br>The second (and last) layer is a 10-way “softmax” layer, which means it will return an array of 10 probability scores (summing to 1). Each<br>score will be the probability that the current digit image belongs to one of our 10 digit classes.</p><p>To make our network ready for training, we need to pick three more things, as part of “compilation” step:</p><ul><li>A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be<br>able to steer itself in the right direction.</li><li>An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.</li><li>Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly<br>classified).</li></ul><p>The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">network.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>                metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in<br>the <code>[0, 1]</code> interval. Previously, our training images for instance were stored in an array of shape <code>(60000, 28, 28)</code> of type <code>uint8</code> with<br>values in the <code>[0, 255]</code> interval. We transform it into a <code>float32</code> array of shape <code>(60000, 28 * 28)</code> with values between 0 and 1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images = train_images.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>test_images = test_images.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br></code></pre></td></tr></table></figure><p>We also need to categorically encode the labels, a step which we explain in chapter 3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><p>We are now ready to train our network, which in Keras is done via a call to the <code>fit</code> method of the network:<br>we “fit” the model to its training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">network.fit(train_images, train_labels, epochs=<span class="hljs-number">5</span>, batch_size=<span class="hljs-number">128</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/5469/469 [==============================] - 2s 5ms/step - loss: 0.0272 - accuracy: 0.9920Epoch 2/5469/469 [==============================] - 2s 5ms/step - loss: 0.0212 - accuracy: 0.9939Epoch 3/5469/469 [==============================] - 2s 5ms/step - loss: 0.0159 - accuracy: 0.9955Epoch 4/5469/469 [==============================] - 2s 4ms/step - loss: 0.0123 - accuracy: 0.9964Epoch 5/5469/469 [==============================] - 2s 4ms/step - loss: 0.0101 - accuracy: 0.9973&lt;tensorflow.python.keras.callbacks.History at 0x2bb3ae958b0&gt;</code></pre><p>Two quantities are being displayed during training: the “loss” of the network over the training data, and the accuracy of the network over<br>the training data.</p><p>We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loss, test_acc = network.evaluate(test_images, test_labels)<br></code></pre></td></tr></table></figure><pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.0721 - accuracy: 0.9808</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;test_acc:&#x27;</span>, test_acc)<br></code></pre></td></tr></table></figure><pre><code>test_acc: 0.9807999730110168</code></pre><p>Our test set accuracy turns out to be 97.8% – that’s quite a bit lower than the training set accuracy.<br>This gap between training accuracy and test accuracy is an example of “overfitting”,<br>the fact that machine learning models tend to perform worse on new data than on their training data.<br>Overfitting will be a central topic in chapter 3.</p><p>This concludes our very first example – you just saw how we could build and a train a neural network to classify handwritten digits, in<br>less than 20 lines of Python code. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really<br>going on behind the scenes. You will learn about “tensors”, the data-storing objects going into the network, about tensor operations, which<br>layers are made of, and about gradient descent, which allows our network to learn from its training examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一篇突然爆红网络的博士论文致谢</title>
      <link href="2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
      <url>2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/4.19.png" alt="他的世界本无光，他把自己活成了光。"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>What&#39;s Maching Learning?</title>
      <link href="2021/04/17/What-s-Maching-Learning/"/>
      <url>2021/04/17/What-s-Maching-Learning/</url>
      
        <content type="html"><![CDATA[<p>贴上了黄博的github笔记</p><span id="more"></span><h2 id="M-D-L的一些资源"><a href="#M-D-L的一些资源" class="headerlink" title="M(D)L的一些资源"></a>M(D)L的一些资源</h2><ol><li><p>黄海广github： <a href="https://github.com/fengdu78">https://github.com/fengdu78</a></p></li><li><p>ML：笔记在线阅读： <a href="http://www.ai-start.com/ml2014/">http://www.ai-start.com/ml2014/</a></p></li><li><p>ML： github：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p></li><li><p>DL：笔记在线阅读： <a href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p></li><li><p>DL： github： <a href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></p></li><li><p>数据科学笔记：github：<a href="https://github.com/fengdu78/Data-Science-Notes">https://github.com/fengdu78/Data-Science-Notes</a></p></li><li><p>统计学习方法代码复现：github：<a href="https://github.com/wzyonggege/">https://github.com/wzyonggege/</a></p></li><li><p>statistical-learning-method or <a href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p></li><li><p>李宏毅ML教材习题解答及实现： <a href="https://github.com/Doraemonzzz/Learning-from-data">https://github.com/Doraemonzzz/Learning-from-data</a></p></li><li><p>李宏毅讲义： <a href="https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg">https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg</a> key：h74o</p></li><li><p>吴恩达有关tensorflow的介绍：对应4笔记p251</p></li><li><p>tensorflow简要介绍： <a href="https://link.zhihu.com/?target=https://github.com/aymericdamien/TensorFlow-Examples">https://link.zhihu.com/?target=https%3A//github.com/aymericdamien/TensorFlow-Examples</a></p></li><li><p>DL with python：<a href="https://github.com/fchollet/deep-learning-with-python-notebooks">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p></li><li><p>pytorch入门：<a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p></li><li><p>sql： 语法查询：<a href="http://www.w3school.com.cn/sql/index.asp">http://www.w3school.com.cn/sql/index.asp</a></p></li><li><p>sql： 语法题库：<a href="https://leetcode-cn.com/problemset/database/">https://leetcode-cn.com/problemset/database/</a></p></li><li><p>Ubuntu 18.04深度学习环境配置（CUDA9.0+CUDNN7.4+TensorFlow1.8：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect</a></p></li><li><p>徐亦达老师课件及下载（中文目录）：<a href="https://github.com/roboticcam/machine-learning-notes">https://github.com/roboticcam/machine-learning-notes</a></p></li><li><p>华校专老师笔记： <a href="https://github.com/huaxz1986">https://github.com/huaxz1986</a></p></li><li><p>华校专网站： <a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></p></li><li><p>论文排班教程： <a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect</a></p></li><li><p>zotero论文管理工具：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect</a></p></li><li><p>tf，pytorch，keras样例资源：</p></li></ol><h2 id="一、TensorFlow"><a href="#一、TensorFlow" class="headerlink" title="一、TensorFlow"></a>一、TensorFlow</h2><h3 id="资源地址"><a href="#资源地址" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p><h3 id="资源介绍"><a href="#资源介绍" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>本资源旨在通过示例轻松深入了解TensorFlow。 为了便于阅读，它包括notebook和带注释的源代码。</p><p>它适合想要找到关于TensorFlow的清晰简洁示例的初学者。 除了传统的“原始”TensorFlow实现，您还可以找到最新的TensorFlow API实践（例如layers,estimator,dataset, ……）。</p><p>最后更新（07/25/2018）：添加新示例（GBDT，Word2Vec）和 TF1.9兼容性！ （TF v1.9 +推荐）。</p><h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，TensorFlow 1.8+</p><h3 id="资源目录"><a href="#资源目录" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-先决条件"><a href="#0-先决条件" class="headerlink" title="0  - 先决条件"></a>0  - 先决条件</h4><p>机器学习简介</p><p>MNIST数据集简介</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1  - 简介"></a>1  - 简介</h4><p>Hello World(包含notebook和py源代码)。非常简单的例子，学习如何使用TensorFlow打印“hello world”。</p><p>基本操作(包含notebook和py源代码)。一个涵盖TensorFlow基本操作的简单示例。</p><p>TensorFlow Eager API基础知识(包含notebook和py源代码)。开始使用TensorFlow的Eager API。</p><h4 id="2-基础模型"><a href="#2-基础模型" class="headerlink" title="2  - 基础模型"></a>2  - 基础模型</h4><p>线性回归(包含notebook和py源代码)。使用TensorFlow实现线性回归。</p><p>线性回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现线性回归。</p><p>Logistic回归(包含notebook和py源代码)。使用TensorFlow实现Logistic回归。</p><p>Logistic回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现Logistic回归。</p><p>最近邻(包含notebook和py源代码)。使用TensorFlow实现最近邻算法。</p><p>K-Means(包含notebook和py源代码)。使用TensorFlow构建K-Means分类器。</p><p>随机森林(包含notebook和py源代码)。使用TensorFlow构建随机森林分类器。</p><p>Gradient Boosted Decision Tree（GBDT）(包含notebook和py源代码)。使用TensorFlow构建梯度提升决策树（GBDT）。</p><p>Word2Vec（词嵌入）(包含notebook和py源代码)。使用TensorFlow从Wikipedia数据构建词嵌入模型（Word2Vec）。</p><h4 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3  - 神经网络"></a>3  - 神经网络</h4><h5 id="监督学习部分"><a href="#监督学习部分" class="headerlink" title="监督学习部分"></a>监督学习部分</h5><p>简单神经网络(包含notebook和py源代码)。构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>简单神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建一个简单的神经网络（如：Multi-layer Perceptron）来对MNIST数字数据集进行分类。</p><p>简单神经网络（Eager API）(包含notebook和py源代码)。使用TensorFlow Eager API构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。</p><p>卷积神经网络(包含notebook和py源代码)。构建卷积神经网络以对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>卷积神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建卷积神经网络，对MNIST数字数据集进行分类。</p><p>递归神经网络（LSTM）(包含notebook和py源代码)。构建递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>双向LSTM(包含notebook和py源代码)。构建双向递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>动态LSTM(包含notebook和py源代码)。构建一个递归神经网络（LSTM），执行动态计算以对不同长度的序列进行分类。</p><h5 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h5><p>自动编码器(包含notebook和py源代码)。构建自动编码器以将图像编码为较低维度并重新构建它。</p><p>变分自动编码器（(包含notebook和py源代码)。构建变分自动编码器（VAE），对噪声进行编码和生成图像。</p><p>GAN（Generative Adversarial Networks）(包含notebook和py源代码)。构建生成对抗网络（GAN）以从噪声生成图像。</p><p>DCGAN（Deep Convolutional Generative Adversarial Networks）(包含notebook和py源代码)。构建深度卷积生成对抗网络（DCGAN）以从噪声生成图像。</p><h4 id="4-工具"><a href="#4-工具" class="headerlink" title="4  - 工具"></a>4  - 工具</h4><p>保存和还原模型(包含notebook和py源代码)。使用TensorFlow保存和还原模型。</p><p>Tensorboard  - 图形和损失可视化(包含notebook和py源代码)。使用Tensorboard可视化计算图并绘制损失。</p><p>Tensorboard  - 高级可视化(包含notebook和py源代码)。深入了解Tensorboard;可视化变量，梯度等……</p><h4 id="5-数据管理"><a href="#5-数据管理" class="headerlink" title="5  - 数据管理"></a>5  - 数据管理</h4><p>构建图像数据集(包含notebook和py源代码)。使用TensorFlow数据队列，从图像文件夹或数据集文件构建您自己的图像数据集。</p><p>TensorFlow数据集API(包含notebook和py源代码)。引入TensorFlow数据集API以优化输入数据管道。</p><h4 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6  - 多GPU"></a>6  - 多GPU</h4><p>多GPU的基本操作(包含notebook和py源代码)。在TensorFlow中引入多GPU的简单示例。</p><p>在多GPU上训练神经网络(包含notebook和py源代码)。一个清晰简单的TensorFlow实现，用于在多个GPU上训练卷积神经网络。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>一些示例需要MNIST数据集进行训练和测试。官方网站：<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p><h2 id="二、Keras"><a href="#二、Keras" class="headerlink" title="二、Keras"></a>二、Keras</h2><h3 id="资源地址-1"><a href="#资源地址-1" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks">https://github.com/erhwenkuo/deep-learning-with-keras-notebooks</a></p><h3 id="资源介绍-1"><a href="#资源介绍-1" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个github的repository主要是ErhWen Kuo在学习Keras的一些记录及练习。希望在学习过程中发现到一些好的信息与示例也可以对想要学习使用Keras来解决问题的同学带来帮助。这些notebooks主要是使用Python 3.6与Keras 2.1.1版本跑在一台配置Nivida 1080Ti的Windows 10的机台所产生的结果，但有些部份会参杂一些Tensorflow与其它的函式库的介绍。</p><h3 id="配置环境-1"><a href="#配置环境-1" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，Keras 2.1.1</p><h3 id="资源目录-1"><a href="#资源目录-1" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-图象数据集-工具介绍"><a href="#0-图象数据集-工具介绍" class="headerlink" title="0.图象数据集/工具介绍"></a>0.图象数据集/工具介绍</h4><h5 id="0-0-COCO-API解说与简单示例"><a href="#0-0-COCO-API解说与简单示例" class="headerlink" title="0.0: COCO API解说与简单示例"></a>0.0: COCO API解说与简单示例</h5><h5 id="0-1-土炮自制扑克牌图象数据集"><a href="#0-1-土炮自制扑克牌图象数据集" class="headerlink" title="0.1:土炮自制扑克牌图象数据集"></a>0.1:土炮自制扑克牌图象数据集</h5><h5 id="0-2-使用Pillow来进行图像处理"><a href="#0-2-使用Pillow来进行图像处理" class="headerlink" title="0.2:使用Pillow来进行图像处理"></a>0.2:使用Pillow来进行图像处理</h5><h4 id="1-Keras-API示例"><a href="#1-Keras-API示例" class="headerlink" title="1.Keras API示例"></a>1.Keras API示例</h4><h5 id="1-0-使用图像增强来进行深度学习"><a href="#1-0-使用图像增强来进行深度学习" class="headerlink" title="1.0:使用图像增强来进行深度学习"></a>1.0:使用图像增强来进行深度学习</h5><h5 id="1-1-如何使用Keras函数式API进行深度学习"><a href="#1-1-如何使用Keras函数式API进行深度学习" class="headerlink" title="1.1:如何使用Keras函数式API进行深度学习"></a>1.1:如何使用Keras函数式API进行深度学习</h5><h5 id="1-2-从零开始构建VGG网络来学习Keras"><a href="#1-2-从零开始构建VGG网络来学习Keras" class="headerlink" title="1.2:从零开始构建VGG网络来学习Keras"></a>1.2:从零开始构建VGG网络来学习Keras</h5><h5 id="1-3-使用预训练的模型来分类照片中的物体"><a href="#1-3-使用预训练的模型来分类照片中的物体" class="headerlink" title="1.3:使用预训练的模型来分类照片中的物体"></a>1.3:使用预训练的模型来分类照片中的物体</h5><h5 id="1-4-使用图像增强来训练小数据集"><a href="#1-4-使用图像增强来训练小数据集" class="headerlink" title="1.4:使用图像增强来训练小数据集"></a>1.4:使用图像增强来训练小数据集</h5><h5 id="1-5-使用预先训练的卷积网络模型"><a href="#1-5-使用预先训练的卷积网络模型" class="headerlink" title="1.5:使用预先训练的卷积网络模型"></a>1.5:使用预先训练的卷积网络模型</h5><h5 id="1-6-卷积网络模型学习到什么的可视化"><a href="#1-6-卷积网络模型学习到什么的可视化" class="headerlink" title="1.6:卷积网络模型学习到什么的可视化"></a>1.6:卷积网络模型学习到什么的可视化</h5><h5 id="1-7-构建自动编码器（Autoencoder）"><a href="#1-7-构建自动编码器（Autoencoder）" class="headerlink" title="1.7:构建自动编码器（Autoencoder）"></a>1.7:构建自动编码器（Autoencoder）</h5><h5 id="1-8-序列到序列（Seq-to-Seq）学习介绍"><a href="#1-8-序列到序列（Seq-to-Seq）学习介绍" class="headerlink" title="1.8:序列到序列（Seq-to-Seq）学习介绍"></a>1.8:序列到序列（Seq-to-Seq）学习介绍</h5><h5 id="1-9-One-hot编码工具程序介绍"><a href="#1-9-One-hot编码工具程序介绍" class="headerlink" title="1.9: One-hot编码工具程序介绍"></a>1.9: One-hot编码工具程序介绍</h5><h5 id="1-10-循环神经网络（RNN）介绍"><a href="#1-10-循环神经网络（RNN）介绍" class="headerlink" title="1.10:循环神经网络（RNN）介绍"></a>1.10:循环神经网络（RNN）介绍</h5><h5 id="1-11-LSTM的返回序列和返回状态之间的区别"><a href="#1-11-LSTM的返回序列和返回状态之间的区别" class="headerlink" title="1.11: LSTM的返回序列和返回状态之间的区别"></a>1.11: LSTM的返回序列和返回状态之间的区别</h5><h5 id="1-12-用LSTM来学习英文字母表顺序"><a href="#1-12-用LSTM来学习英文字母表顺序" class="headerlink" title="1.12:用LSTM来学习英文字母表顺序"></a>1.12:用LSTM来学习英文字母表顺序</h5><h4 id="2-图像分类（Image-Classification）"><a href="#2-图像分类（Image-Classification）" class="headerlink" title="2.图像分类（Image Classification）"></a>2.图像分类（Image Classification）</h4><h5 id="2-0-Julia（Chars74K）字母图像分类"><a href="#2-0-Julia（Chars74K）字母图像分类" class="headerlink" title="2.0: Julia（Chars74K）字母图像分类"></a>2.0: Julia（Chars74K）字母图像分类</h5><h5 id="2-1-交通标志图像分类"><a href="#2-1-交通标志图像分类" class="headerlink" title="2.1:交通标志图像分类"></a>2.1:交通标志图像分类</h5><h5 id="2-2-辛普森卡通图像角色分类"><a href="#2-2-辛普森卡通图像角色分类" class="headerlink" title="2.2:辛普森卡通图像角色分类"></a>2.2:辛普森卡通图像角色分类</h5><h5 id="2-3-时尚服饰图像分类"><a href="#2-3-时尚服饰图像分类" class="headerlink" title="2.3:时尚服饰图像分类"></a>2.3:时尚服饰图像分类</h5><h5 id="2-4-人脸关键点辨识"><a href="#2-4-人脸关键点辨识" class="headerlink" title="2.4:人脸关键点辨识"></a>2.4:人脸关键点辨识</h5><h5 id="2-5-Captcha验证码分类"><a href="#2-5-Captcha验证码分类" class="headerlink" title="2.5: Captcha验证码分类"></a>2.5: Captcha验证码分类</h5><h5 id="2-6-Mnist手写图像分类（MLP）"><a href="#2-6-Mnist手写图像分类（MLP）" class="headerlink" title="2.6: Mnist手写图像分类（MLP）"></a>2.6: Mnist手写图像分类（MLP）</h5><h5 id="2-7-Mnist手写图像分类（CNN）"><a href="#2-7-Mnist手写图像分类（CNN）" class="headerlink" title="2.7: Mnist手写图像分类（CNN）"></a>2.7: Mnist手写图像分类（CNN）</h5><h4 id="3-目标检测（Object-Recognition）"><a href="#3-目标检测（Object-Recognition）" class="headerlink" title="3.目标检测（Object Recognition）"></a>3.目标检测（Object Recognition）</h4><h5 id="3-0-YOLO目标检测算法概念与介绍"><a href="#3-0-YOLO目标检测算法概念与介绍" class="headerlink" title="3.0: YOLO目标检测算法概念与介绍"></a>3.0: YOLO目标检测算法概念与介绍</h5><h5 id="3-1-YOLOv2目标检测示例"><a href="#3-1-YOLOv2目标检测示例" class="headerlink" title="3.1: YOLOv2目标检测示例"></a>3.1: YOLOv2目标检测示例</h5><h5 id="3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整"><a href="#3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整" class="headerlink" title="3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整"></a>3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整</h5><h5 id="3-3-浣熊（Racoon）检测-YOLOv2模型的使用"><a href="#3-3-浣熊（Racoon）检测-YOLOv2模型的使用" class="headerlink" title="3.3:浣熊（Racoon）检测-YOLOv2模型的使用"></a>3.3:浣熊（Racoon）检测-YOLOv2模型的使用</h5><h5 id="3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"><a href="#3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整" class="headerlink" title="3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"></a>3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整</h5><h5 id="3-5-双手（Hands）检测-YOLOv2模型训练与调整"><a href="#3-5-双手（Hands）检测-YOLOv2模型训练与调整" class="headerlink" title="3.5:双手（Hands）检测-YOLOv2模型训练与调整"></a>3.5:双手（Hands）检测-YOLOv2模型训练与调整</h5><h5 id="3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"><a href="#3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整" class="headerlink" title="3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"></a>3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整</h5><h5 id="3-7-MS-COCO图象检测-YOLOv2模型训练与调整"><a href="#3-7-MS-COCO图象检测-YOLOv2模型训练与调整" class="headerlink" title="3.7: MS COCO图象检测-YOLOv2模型训练与调整"></a>3.7: MS COCO图象检测-YOLOv2模型训练与调整</h5><h4 id="4-物体分割（Object-Segmentation）"><a href="#4-物体分割（Object-Segmentation）" class="headerlink" title="4.物体分割（Object Segmentation）"></a>4.物体分割（Object Segmentation）</h4><h4 id="5-关键点检测（Keypoint-Detection）"><a href="#5-关键点检测（Keypoint-Detection）" class="headerlink" title="5.关键点检测（Keypoint Detection）"></a>5.关键点检测（Keypoint Detection）</h4><h4 id="6-图象标题（Image-Caption）"><a href="#6-图象标题（Image-Caption）" class="headerlink" title="6.图象标题（Image Caption）"></a>6.图象标题（Image Caption）</h4><h4 id="7-人脸检测识别（Face-Detection-Recognition）"><a href="#7-人脸检测识别（Face-Detection-Recognition）" class="headerlink" title="7.人脸检测识别（Face Detection/Recognition）"></a>7.人脸检测识别（Face Detection/Recognition）</h4><h5 id="7-0-人脸检测-OpenCV（Haar特征分类器）"><a href="#7-0-人脸检测-OpenCV（Haar特征分类器）" class="headerlink" title="7.0:人脸检测- OpenCV（Haar特征分类器）"></a>7.0:人脸检测- OpenCV（Haar特征分类器）</h5><h5 id="7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）"><a href="#7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）" class="headerlink" title="7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）"></a>7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）</h5><h5 id="7-2-人脸识别-脸部检测、对齐-amp-裁剪"><a href="#7-2-人脸识别-脸部检测、对齐-amp-裁剪" class="headerlink" title="7.2:人脸识别-脸部检测、对齐&amp;裁剪"></a>7.2:人脸识别-脸部检测、对齐&amp;裁剪</h5><h5 id="7-3-人脸识别-人脸部特征提取-amp-人脸分类器"><a href="#7-3-人脸识别-人脸部特征提取-amp-人脸分类器" class="headerlink" title="7.3:人脸识别-人脸部特征提取&amp;人脸分类器"></a>7.3:人脸识别-人脸部特征提取&amp;人脸分类器</h5><h5 id="7-4-人脸识别-转换、对齐、裁剪、特征提取与比对"><a href="#7-4-人脸识别-转换、对齐、裁剪、特征提取与比对" class="headerlink" title="7.4:人脸识别-转换、对齐、裁剪、特征提取与比对"></a>7.4:人脸识别-转换、对齐、裁剪、特征提取与比对</h5><h5 id="7-5-脸部关键点检测（dlib）"><a href="#7-5-脸部关键点检测（dlib）" class="headerlink" title="7.5:脸部关键点检测（dlib）"></a>7.5:脸部关键点检测（dlib）</h5><h5 id="7-6-头部姿态（Head-pose）估计（dlib）"><a href="#7-6-头部姿态（Head-pose）估计（dlib）" class="headerlink" title="7.6:头部姿态（Head pose）估计（dlib）"></a>7.6:头部姿态（Head pose）估计（dlib）</h5><h4 id="8-自然语言处理（Natural-Language-Processing）"><a href="#8-自然语言处理（Natural-Language-Processing）" class="headerlink" title="8.自然语言处理（Natural Language Processing）"></a>8.自然语言处理（Natural Language Processing）</h4><h5 id="8-0-词嵌入（word-embeddings）介绍"><a href="#8-0-词嵌入（word-embeddings）介绍" class="headerlink" title="8.0:词嵌入（word embeddings）介绍"></a>8.0:词嵌入（word embeddings）介绍</h5><h5 id="8-1-使用结巴（jieba）进行中文分词"><a href="#8-1-使用结巴（jieba）进行中文分词" class="headerlink" title="8.1:使用结巴（jieba）进行中文分词"></a>8.1:使用结巴（jieba）进行中文分词</h5><h5 id="8-2-Word2vec词嵌入（word-embeddings）的基本概念"><a href="#8-2-Word2vec词嵌入（word-embeddings）的基本概念" class="headerlink" title="8.2: Word2vec词嵌入（word embeddings）的基本概念"></a>8.2: Word2vec词嵌入（word embeddings）的基本概念</h5><h5 id="8-3-使用结巴（jieba）进行歌词分析"><a href="#8-3-使用结巴（jieba）进行歌词分析" class="headerlink" title="8.3:使用结巴（jieba）进行歌词分析"></a>8.3:使用结巴（jieba）进行歌词分析</h5><h5 id="8-4-使用gensim训练中文词向量（word2vec）"><a href="#8-4-使用gensim训练中文词向量（word2vec）" class="headerlink" title="8.4:使用gensim训练中文词向量（word2vec）"></a>8.4:使用gensim训练中文词向量（word2vec）</h5><h2 id="三、Pytorch"><a href="#三、Pytorch" class="headerlink" title="三、Pytorch"></a>三、Pytorch</h2><h3 id="资源地址-2"><a href="#资源地址-2" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p><h3 id="资源介绍-2"><a href="#资源介绍-2" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个资源为深度学习研究人员提供了学习PyTorch的教程代码大多数模型都使用少于30行代码实现。 在开始本教程之前，建议先看完Pytorch官方教程。</p><h3 id="配置环境-2"><a href="#配置环境-2" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 2.7或者3.5以上，pytorch 0.4</p><h3 id="资源目录-2"><a href="#资源目录-2" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1.基础知识"></a>1.基础知识</h4><p>PyTorch基础知识</p><p>线性回归</p><p>Logistic回归</p><p>前馈神经网络</p><h4 id="2-中级"><a href="#2-中级" class="headerlink" title="2.中级"></a>2.中级</h4><p>卷积神经网络</p><p>深度残差网络</p><p>递归神经网络</p><p>双向递归神经网络</p><p>语言模型（RNN-LM）</p><h4 id="3-高级"><a href="#3-高级" class="headerlink" title="3.高级"></a>3.高级</h4><p>生成性对抗网络</p><p>变分自动编码器</p><p>神经风格转移</p><p>图像字幕（CNN-RNN）</p><h4 id="4-工具-1"><a href="#4-工具-1" class="headerlink" title="4.工具"></a>4.工具</h4><p>PyTorch中的TensorBoard</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>my first blog</title>
      <link href="2021/04/17/my-first-blog/"/>
      <url>2021/04/17/my-first-blog/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p>先发一篇试试^_^</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Test</title>
      <link href="2021/04/17/Test/"/>
      <url>2021/04/17/Test/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p>A blog for test.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/04/16/hello-world/"/>
      <url>2021/04/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>搞了**的整整两天，我的个人blog终于能在云端访问了^^^^</p><span id="more"></span><p>After two fucking days, Lao Tzu’s personal blog is finally online, fuck</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
