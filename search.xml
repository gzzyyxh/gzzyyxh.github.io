<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>YOLO9000: Better, Faster, Stronger</title>
      <link href="2021/04/29/YOLO9000-Better-Faster-Stronger/"/>
      <url>2021/04/29/YOLO9000-Better-Faster-Stronger/</url>
      
        <content type="html"><![CDATA[<p>YOLOv2模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv2.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="What’s-YOLOv2"><a href="#What’s-YOLOv2" class="headerlink" title="What’s YOLOv2?"></a>What’s YOLOv2?</h1><p>YOLOv2的论文全名为YOLO9000: Better, Faster, Stronger，它斩获了CVPR 2017 Best Paper Honorable Mention。在这篇文章中，作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。所以，这篇文章其实包含两个模型：YOLOv2和YOLO9000，不过后者是在前者基础上提出的，两者模型主体结构是一致的。YOLOv2相比YOLOv1做了很多方面的改进，这也使得YOLOv2的mAP有显著的提升，并且YOLOv2的速度依然很快，保持着自己作为one-stage方法的优势，YOLOv2和Faster R-CNN, SSD等模型的对比如图1所示。这里将首先介绍YOLOv2的改进策略，并给出YOLOv2的TensorFlow实现过程，然后介绍YOLO9000的训练方法。</p><p><img src="/img/4.29/1.png" alt="YOLOv2与其它模型在VOC 2007数据集上的效果对比"></p><h1 id="YOLOv2的改进策略"><a href="#YOLOv2的改进策略" class="headerlink" title="YOLOv2的改进策略"></a>YOLOv2的改进策略</h1><p>YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图2所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。下面详细介绍各个改进策略。</p><p><img src="/img/4.29/2.jpg" alt="YOLOv2相比YOLOv1的改进策略"></p><ol><li>Batch Normalization</li></ol><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p><ol><li>High Resolution Classifier</li></ol><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为<script type="math/tex">224 \times 224</script>的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用<script type="math/tex">224 \times 224</script>分类模型预训练后，将分辨率增加至<script type="math/tex">448 \times 448</script>，并使用这个高分辨率在检测数据集上finetune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用<script type="math/tex">448 \times 448</script>来finetune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p><ol><li>Convolutionlal With Anchor Boxes</li></ol><p>在YOLOv1中，输入图片最终被划分为7<em>7网格，每个单元格预测2个边界框。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比（scales and ratios）的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框（anchor boxes，prior boxes，SSD也采用了先验框）策略。RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体），并且各个位置设置不同尺度和比例的先验框，所以RPN预测的是边界框相对于先验框的offsets值（其实是transform值，详细见Faster R_CNN论文），采用先验框使得模型更容易学习。所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采481</em>418图片作为输入，而是采用416<em>416大小。因为YOLOv2模型下采样的总步长为32,对于416</em>416大小的图片，最终得到的特征图大小为13*13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。所以在YOLOv2设计中要保证最终的特征图有奇数个位置。对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值：<script type="math/tex">(x, y, w, h, c)</script>前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。</p><p>使用anchor boxes之后，YOLOv2的mAP有稍微下降（这里下降的原因，我猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框<script type="math/tex">(7 \times 7 \times 2)</script>，而YOLOv2使用anchor boxes之后可以预测上千个边界框<script type="math/tex">(13 \times 13 \times \text { num_anchors })</script>。所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%。</p><ol><li>Dimension Clusters</li></ol><p>在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： </p><script type="math/tex; mode=display">d(\text { box, centroid })=1-I O U(\text { box }, \text { centroid })</script><p>下图为在VOC和COCO数据集上的聚类分析结果，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取5个聚类中心作为先验框，其相对于图片的大小如右边图所示。对于两个数据集，5个先验框的width和height如下所示（来源：YOLO源码的cfg文件）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">COCO: (<span class="hljs-number">0.57273</span>, <span class="hljs-number">0.677385</span>), (<span class="hljs-number">1.87446</span>, <span class="hljs-number">2.06253</span>), (<span class="hljs-number">3.33843</span>, <span class="hljs-number">5.47434</span>), (<span class="hljs-number">7.88282</span>, <span class="hljs-number">3.52778</span>), (<span class="hljs-number">9.77052</span>, <span class="hljs-number">9.16828</span>)<br>VOC: (<span class="hljs-number">1.3221</span>, <span class="hljs-number">1.73145</span>), (<span class="hljs-number">3.19275</span>, <span class="hljs-number">4.00944</span>), (<span class="hljs-number">5.05587</span>, <span class="hljs-number">8.09892</span>), (<span class="hljs-number">9.47112</span>, <span class="hljs-number">4.84053</span>), (<span class="hljs-number">11.2364</span>, <span class="hljs-number">10.0071</span>)<br></code></pre></td></tr></table></figure><p><img src="/img/4.29/3.png" alt="数据集VOC和COCO上的边界框聚类分析结果"></p><p>但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（13*13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。</p><ol><li>New Network：Darknet-19</li></ol><p>YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3<em>3卷积，采用2</em>2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3<em>3卷积之间使用1</em>1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p><p><img src="/img/4.29/4.jpg" alt="Darknet-19模型结构"></p><ol><li>Direct location prediction</li></ol><p>前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置<script type="math/tex">(x,y)</script>，需要根据预测的坐标偏移值<script type="math/tex">(t_{x},t_{y})</script>，先验框的尺度<script type="math/tex">\left(w_{a}, h_{a}\right)</script>以及中心坐标<script type="math/tex">\left(x_{a}, y_{a}\right)</script>（特征图每个位置的中心点）来计算：</p><script type="math/tex; mode=display">x=\left(t_{x} \times w_{a}\right)-x_{a}, y=\left(t_{y} \times h_{a}\right)-y_{a}</script><p>但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当<script type="math/tex">t_{x}=1</script>时边界框将向右偏移先验框的一个宽度大小，而当<script type="math/tex">t_{x}=-1</script>时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets<script type="math/tex">t_{x}, t_{y}, t_{w}, t_{h}</script>，可以按如下公式计算出边界框实际位置和大小：</p><script type="math/tex; mode=display">\begin{array}{c}b_{x}=\sigma\left(t_{x}\right)+c_{x}, b_{y}=\sigma\left(t_{y}\right)+c_{y} \\b_{w}=p_{w} e^{t_{w}}, b_{h}=p_{h} e^{t_{h}}\end{array}</script><p>其中<script type="math/tex">\left(c_{x}, x_{y}\right)</script>为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为<script type="math/tex">(1,1)</script>。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而<script type="math/tex">p_{w}</script>和<script type="math/tex">p_{h}</script>是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为<script type="math/tex">(W, H)</script>（在文中是<script type="math/tex">(13,13)</script>），这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：</p><script type="math/tex; mode=display">\begin{array}{c}b_{x}=\left(\sigma\left(t_{x}\right)+c_{x}\right) / W, b_{y}=\left(\sigma\left(t_{y}\right)+c_{y}\right) / H \\b_{w}=p_{w} e^{t_{w}} / W, b_{h}=p_{h} e^{t_{h}} / H\end{array}</script><p>如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。</p><p><img src="/img/4.29/5.png" alt="边界框位置与大小的计算示例图"></p><ol><li>Fine-Grained Features</li></ol><p>YOLOv2的输入图片大小为416<em>416，经过5次maxpooling之后得到13</em>13大小的特征图，并以此特征图采用卷积做预测。13<em>13大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26</em>26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26<em>26</em>512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2<em>2的局部区域，然后将其转化为channel维度，对于26</em>26<em>512的特征图，经passthrough层处理之后就变成了13</em>13<em>2048的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的13</em>13<em>1024特征图连接在一起形成13</em>13*3072的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">out = tf.extract_image_patches(<span class="hljs-keyword">in</span>, [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], padding=<span class="hljs-string">&quot;VALID&quot;</span>)<br>// <span class="hljs-keyword">or</span> use tf.space_to_depth<br>out = tf.space_to_depth(<span class="hljs-keyword">in</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="/img/4.29/6.jpg" alt="passthrough层实例"></p><p>另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个1<em>1卷积核进行卷积，然后再进行passthrough处理，这样26</em>26<em>512的特征图得到13</em>13*256的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。</p><ol><li>Multi-Scale Training</li></ol><p>由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416<em>416大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：<script type="math/tex">\{320,352, \ldots, 608\}</script>输入图片最小为320</em>320，此时对应的特征图大小为10<em>10（不是奇数了，确实有点尴尬），而输入图片最大为608</em>608,对应的特征图大小为19*19,在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。 </p><p><img src="/img/4.29/7.jpg" alt="Multi-Scale Training"></p><p>采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544,mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。</p><p><img src="/img/4.29/8.jpg" alt="YOLOv2在VOC 2007数据集上的性能对比"></p><p>总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。</p><h1 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h1><p>YOLOv2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为224<em>224,共训练160个epochs。然后第二阶段将网络的输入调整为448</em>448,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个3<em>3</em>2014卷积层，同时增加了一个passthrough层，最后使用1*1卷积层输出预测结果，输出的channels数为：<script type="math/tex">\text { num_anchors } \times(5+\text { num_classes })</script>，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为T（shape为<script type="math/tex">(\text { batch_size }, 13,13,125)</script>），可以先将其reshape为<script type="math/tex">(\text { batch_size }, 13,13,5,25)</script>，其中<script type="math/tex">T[:,:,:,:, 0: 4]</script>为边界框的位置和大小，<script type="math/tex">\left(t_{x}, t_{y}, t_{w}, t_{h}\right)$$$$T[:,:,:,:, 4]</script>为边界框的置信度，而<script type="math/tex">T[:,:,:,:, 5:]</script>为类别预测值。</p><p><img src="/img/4.29/9.jpg" alt="YOLOv2训练的三个阶段"></p><p><img src="/img/4.29/10.jpg" alt="YOLOv2结构示意图"></p><p>YOLOv2的网络结构以及训练参数我们都知道了，但是貌似少了点东西。仔细一想，原来作者并没有给出YOLOv2的训练过程的两个最重要方面，即先验框匹配（样本选择）以及训练的损失函数，难怪Ng说YOLO论文很难懂，没有这两方面的说明我们确实不知道YOLOv2到底是怎么训练起来的。不过默认按照YOLOv1的处理方式也是可以处理，我看了YOLO在TensorFlow上的实现darkflow（见yolov2/train.py），发现它就是如此处理的：和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的5个先验框所对应的边界框负责预测它，具体是哪个边界框预测它，需要在训练中确定，即由那个与ground truth的IOU最大的边界框预测它，而剩余的4个边界框不与该ground truth匹配。YOLOv2同样需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的先验框计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的边界框只计算置信度误差（此时target为0）。YOLOv2和YOLOv1的损失函数一样，为均方差函数。但是我看了YOLOv2的源码（训练样本处理与loss计算都包含在文件region_layer.c中，YOLO源码没有任何注释，反正我看了是直摇头），并且参考国外的blog以及allanzelener/YAD2K（Ng深度学习教程所参考的那个Keras实现）上的实现，发现YOLOv2的处理比原来的v1版本更加复杂。先给出loss计算公式：</p><script type="math/tex; mode=display">\begin{array}{r}\operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} \quad 1_{\operatorname{Max} I O U<\text { Thresh }} \lambda_{\text {noobj }} *\left(-b_{i j k}^{o}\right)^{2} \\+1_{t<12800} \lambda_{\text {prior }} * \sum_{r \in(x, y, w, h)}\left(\text { prior }_{k}^{r}-b_{i j k}^{r}\right)^{2} \\+1_{k}^{\text {truth }}\left(\lambda_{\text {coord }} * \sum_{r \in(x, y, w, h)}\left(\text { truth }^{r}-b_{i j k}^{r}\right)^{2}\right. \\+\lambda_{o b j} *\left(I O U_{\text {truth }}^{k}-b_{i j k}^{o}\right)^{2} \\\left.+\lambda_{\text {class }} *\left(\sum_{c=1}^{C}\left(\text { truth }^{c}-b_{i j k}^{c}\right)^{2}\right)\right)\end{array}</script><p>首先<script type="math/tex">W</script>,<script type="math/tex">H</script>分别指的是特征图<script type="math/tex">(13 \times 13)</script>的宽与高，而A指的是先验框数目（这里是5），各个<script type="math/tex">lambda</script>值是各个loss部分的权重系数。第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差。第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，在YOLOv1中target=1，而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的和误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale <em> (2 - truth.w</em>truth.h)，这样对于尺度较小的boxes其权重系数会更大一些，起到和YOLOv1计算平方根相似的效果（参考YOLO v2 损失函数源码分析）。</p><p>最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。</p><h1 id="YOLOv2在TensorFlow上实现"><a href="#YOLOv2在TensorFlow上实现" class="headerlink" title="YOLOv2在TensorFlow上实现"></a>YOLOv2在TensorFlow上实现</h1><p>这里参考YOLOv2在Keras上的复现（见yhcc/yolo2）,使用TensorFlow实现YOLOv2在COCO数据集上的test过程。首先是定义YOLOv2的主体网络结构Darknet-19：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">darknet</span>(<span class="hljs-params">images, n_last_channels=<span class="hljs-number">425</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Darknet19 for YOLOv2&quot;&quot;&quot;</span><br>    net = conv2d(images, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv1&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv2&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv3_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv4_3&quot;</span>)<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv5_5&quot;</span>)<br>    shortcut = net<br>    net = maxpool(net, name=<span class="hljs-string">&quot;pool5&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_2&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_3&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_4&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv6_5&quot;</span>)<br>    <span class="hljs-comment"># ---------</span><br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_1&quot;</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv7_2&quot;</span>)<br>    <span class="hljs-comment"># shortcut</span><br>    shortcut = conv2d(shortcut, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv_shortcut&quot;</span>)<br>    shortcut = reorg(shortcut, <span class="hljs-number">2</span>)<br>    net = tf.concat([shortcut, net], axis=-<span class="hljs-number">1</span>)<br>    net = conv2d(net, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, name=<span class="hljs-string">&quot;conv8&quot;</span>)<br>    <span class="hljs-comment"># detection layer</span><br>    net = conv2d(net, n_last_channels, <span class="hljs-number">1</span>, batch_normalize=<span class="hljs-number">0</span>,<br>                 activation=<span class="hljs-literal">None</span>, use_bias=<span class="hljs-literal">True</span>, name=<span class="hljs-string">&quot;conv_dec&quot;</span>)<br>    <span class="hljs-keyword">return</span> net<br></code></pre></td></tr></table></figure><p>然后实现对Darknet-19模型输出的解码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decode</span>(<span class="hljs-params">detection_feat, feat_sizes=(<span class="hljs-params"><span class="hljs-number">13</span>, <span class="hljs-number">13</span></span>), num_classes=<span class="hljs-number">80</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">           anchors=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;decode from the detection feature&quot;&quot;&quot;</span><br>    H, W = feat_sizes<br>    num_anchors = <span class="hljs-built_in">len</span>(anchors)<br>    detetion_results = tf.reshape(detection_feat, [-<span class="hljs-number">1</span>, H * W, num_anchors,<br>                                        num_classes + <span class="hljs-number">5</span>])<br><br>    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>])<br>    bbox_wh = tf.exp(detetion_results[:, :, :, <span class="hljs-number">2</span>:<span class="hljs-number">4</span>])<br>    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, <span class="hljs-number">4</span>])<br>    class_probs = tf.nn.softmax(detetion_results[:, :, :, <span class="hljs-number">5</span>:])<br><br>    anchors = tf.constant(anchors, dtype=tf.float32)<br><br>    height_ind = tf.<span class="hljs-built_in">range</span>(H, dtype=tf.float32)<br>    width_ind = tf.<span class="hljs-built_in">range</span>(W, dtype=tf.float32)<br>    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)<br>    x_offset = tf.reshape(x_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    y_offset = tf.reshape(y_offset, [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br>    <span class="hljs-comment"># decode</span><br>    bbox_x = (bbox_xy[:, :, :, <span class="hljs-number">0</span>] + x_offset) / W<br>    bbox_y = (bbox_xy[:, :, :, <span class="hljs-number">1</span>] + y_offset) / H<br>    bbox_w = bbox_wh[:, :, :, <span class="hljs-number">0</span>] * anchors[:, <span class="hljs-number">0</span>] / W * <span class="hljs-number">0.5</span><br>    bbox_h = bbox_wh[:, :, :, <span class="hljs-number">1</span>] * anchors[:, <span class="hljs-number">1</span>] / H * <span class="hljs-number">0.5</span><br><br>    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,<br>                       bbox_x + bbox_w, bbox_y + bbox_h], axis=<span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">return</span> bboxes, obj_probs, class_probs<br></code></pre></td></tr></table></figure><h1 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h1><p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p><p>作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：</p><p><img src="/img/4.29/11.jpg" alt="基于COCO和ImageNet数据集建立的WordTree"></p><p>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。</p><p><img src="/img/4.29/12.jpg" alt="ImageNet与WordTree预测的对比">在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是<script type="math/tex">Pr(physicalobject)</script>，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p><p>通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。我觉得这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。</p>]]></content>
      
      
      <categories>
          
          <category> Computer Version </category>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>You Only Look Once: Unified,Real-Time Object Detection</title>
      <link href="2021/04/27/You-Only-Look-Once-Unified-Real-Time-Object-Detection/"/>
      <url>2021/04/27/You-Only-Look-Once-Unified-Real-Time-Object-Detection/</url>
      
        <content type="html"><![CDATA[<p>YOLOv1模型论文及原理详解</p><span id="more"></span><div class="row">    <embed src="./YOLOv1.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="what’s-YOLO"><a href="#what’s-YOLO" class="headerlink" title="what’s YOLO"></a>what’s YOLO</h1><p>根据<a href="https://pjreddie.com/darknet/yolo/">YOLO官网</a>对它的解释，YOLO：Real-Time Object Detection. You Only Look Once(YOLO)是一个最先进的实时的目标检测系统。在Pascal Titan X上面处理图像能够达到30FPS，在COCO test-dev上具有57.9%的<a href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/">mAP</a>。</p><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p><img src="/img/4.28/1.png" alt="目标检测"></p><p>2015年之前目标检测方法在Pascal VOC 2007数据集上测试的FPS和mAP结果对比,传统的目标检测方法大致分为三个步骤，先使用不同的方法(滑动窗口，区域候选)提取区域的特征图，然后再使用分类器进行识别，最后回归预测。大多数方法都较为复杂，速度较慢，训练耗时。</p><p>传统的方法可以按照检测系统分为两种：</p><ol><li>DPM，Deformatable Parts Models，采用sliding window检测</li><li>R-CNN、Fast R-CNN。采用region proposal的方法，生成一些可能包含待检测物体的potential bounding box，再通过一个classifier(SVM)判断每个bbox里是否真的包含物体，以及物体的class probability。</li></ol><p>目前深度学习相关的目标检测方法大致可以分为两派：</p><ol><li>基于区域提名的(regin proposal)的，比如R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、R-FCN。</li><li>基于端到端(end to end)的，无需候选区域，如YOLO、SSD。<br>二者发展都很迅速，区域提名准确率较好、端到端的方法速度较快。</li></ol><h2 id="YOLO-核心思想"><a href="#YOLO-核心思想" class="headerlink" title="YOLO 核心思想"></a>YOLO 核心思想</h2><ul><li>将整张图片作为网络的输入，直接在输出层对bounding box的位置和所属类别进行回归。与Faster R-CNN网络相比，虽然后者也是使用整张图片作为输入，但是它采用了RCNN那种区域预测+分类的思想，把提取proposal的步骤放在了CNN中实现，而YOLO则采用直接回归的思路，将目标定位和目标类别预测整合于在单个神经网络模型中。</li></ul><p><img src="/img/4.28/2.png" alt="YOLO检测系统"></p><ul><li>直接在输出层回归bbox的位置和所属类别。</li></ul><p>YOLO检测系统简单直接，可以看做只有三步：</p><ol><li>YOLO检测系统先将输入图像调整到448×448；</li><li>在图像上运行卷积网络；</li><li>通过模型的置信度对结果进行阈值。</li></ol><h3 id="YOLO实现细节"><a href="#YOLO实现细节" class="headerlink" title="YOLO实现细节"></a>YOLO实现细节</h3><ol><li>将一幅图像分成<script type="math/tex">S \times S</script>个网格(Grid Cell)，如果某个object的中心落在某个网格中(通过ground-truth框确定)，则这个网格就负责预测这个object。</li><li>每个网格要预测B个bounding box，每个box除了要回归自身的位置之外，还要附带预测一个confidence值。这个值代表了所预测的bounding box中是否含有object和若有object，这个object预测得有多准的两重信息，计算方式：</li></ol><script type="math/tex; mode=display">\operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><p>如果有object的中心落在一个网格里面，*的前第一项取1，否则取0。第二项是预测的边界框和ground-truth之间的<a href="https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/">IoU</a>值。</p><ol><li>每个网格单元针对20种类别预测bboxes属于单个类别的条件概率<script type="math/tex">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right)</script>,属于同一个网格的B个bboxes共享一个条件概率。在测试时，将条件概率分别和单个的bbox的confidence预测相乘：</li></ol><script type="math/tex; mode=display">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><ol><li>在Pascal VOC中，YOLO检测系统的图像输入为448×448，S=7，B=2，一共有20个class(C=20)，输出就是7×7×30的一个tensor。这个是怎么算出来的呢？看下面详解。</li></ol><h2 id="YOLO-网络设计"><a href="#YOLO-网络设计" class="headerlink" title="YOLO 网络设计"></a>YOLO 网络设计</h2><p><img src="/img/4.28/3.png" alt="YOLO网络结构"></p><p>YOLO使用了24个级联卷积层和最后2个全连接层，交替的1×1卷积层降低了前面层的特征空间。在ImageNet分类任务上使用分辨率的一半(224×224输入图像)对卷积层进行预训练，然后将分辨率加倍进行目标检测。</p><p><img src="/img/4.28/4.png" alt="YOLO网络概览图"></p><p>YOLO网络借鉴了GoogleNet的思想，但与之不同的是，为了更好的性能，它增加额外的4层卷积层(conv)。YOLO一共使用了24个级联的卷积层和2个全连接层(fc)，其中conv层中包含了1×1和3×3两种kernel，最后一个fc全连接层后经过reshape之后就是YOLO网络的输出，是长度为<script type="math/tex">S \times S \times (B \times 5 + C ) = 7 \times 7 \times 30</script>的tensor，最后经过识别过程得到最终的检测结果。</p><p><img src="/img/4.28/5.png" alt="1x1x30 tensor解释"></p><p>上文说到每个bounding box要预测(x,y,w,h,confidence)五个值，一张图片共分为<script type="math/tex">S \times S</script>个网格，每个网格要预测出B个bounding box和一个网格负责的object的类别信息，记为<script type="math/tex">C</script>。</p><p>则输出为<script type="math/tex">S \times S \times ( 5 \times B + C )</script>的tensor张量，<script type="math/tex">(x,y)</script>表示bounding box相对于网格单元的边界的offset，归一化到<script type="math/tex">(0,1)</script>范围之内，而<script type="math/tex">w</script>,<script type="math/tex">h</script>表示相对于整个图片的预测宽和高，也被归一化到<script type="math/tex">(0,1)</script>范围内，<script type="math/tex">c</script>代表的是object在某个bounding box的confidence。</p><p>使用下图更形象的说明，7×7×30的Tensor中的一个1×1×30的前10维的所代表的含义。</p><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>下面解释如何将预测坐标的x,y用相对于对应网格的offset归一化到0-1和w,h是如何利用图像的宽高归一化到0-1之间。</p><p>每个单元格预测的B个<script type="math/tex">(x,y,w,h,confidence)</script>向量，假设图片为<script type="math/tex">S \times S</script>个网格，S=7，图片宽为<script type="math/tex">w_{i}</script>高为<script type="math/tex">h_{i}</script>：</p><ol><li><script type="math/tex">(x,y)</script>是bbox的中心相对于单元格的offset对于下图中蓝色单元格，坐标为<script type="math/tex">\left(x_{col}=1, y_{row} \text =4\right)</script>，假设它的预测输出是红色框bbox，设bbox的中心坐标为那么最终预测出来的<script type="math/tex">(x,y)</script>是经过归一化处理的，表示的是相对于单元格的offset，公式：</li></ol><script type="math/tex; mode=display">x=\frac{x_{c}}{w_{i}} * S-x_{\text {col }}, \quad y=\frac{y_{c}}{h_{i}} * S-y_{\text {row }}</script><p><img src="/img/4.28/6.png" alt="归一化"></p><ol><li><script type="math/tex">(w,h)</script>是bbox相对于整个图片的比例预测的bbox的宽高为<script type="math/tex">w_{b}</script>, <script type="math/tex">h_{b}</script>，<script type="math/tex">(w,h)</script>表示的是bbox相对于整张图片的占比，公式：</li></ol><script type="math/tex; mode=display">w=\frac{w_{b}}{w_{i}}, h=\frac{h_{b}}{h_{i}}</script><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p><img src="/img/4.28/7.png" alt="1x1x30 前10维解释"></p><p>每个grid cell对应的两个bounding box的5个值在tensor中体现出来，组成前十维。每个单元格负责预测一个object，但是每个单元格预测出2个bounding box，如图。</p><p><img src="/img/4.28/8.png" alt="1x1x30 Tensor的30维解释"></p><p>每个grid cell有30维，其中8维是两个预测回归bboxes的坐标信息，2维是bboxes的confidences，还有20维是与类别相关的信息。</p><p><img src="/img/4.28/9.png" alt="20维中每个值表示此object在此bbox中的时候属于class_i的概率分数"></p><p><img src="/img/4.28/10.png" alt="类别预测分数的计算"></p><p><img src="/img/4.28/11.png" alt="类别预测分数的计算"></p><p>每个单元格预测一个属于类别<script type="math/tex">class_{i}</script>的条件概率$\operatorname{Pr}\left(\right.$ Class $_{i} \mid$ Object $)$。要注意的是，属于一个网格的2个bboxs共享一套条件概率值，因为这两个box都是为了一个grid cell服务，最终预测出一个object类别。在测试时，将条件概率和单个bbox的confidence预测相乘：</p><script type="math/tex; mode=display">\operatorname{Pr}\left(\text { Class }_{i} \mid \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}=\operatorname{Pr}\left(\text { Class }_{i}\right) * \operatorname{Io} U \frac{\text { truth }}{\text { pred }}</script><p>如上式所述，每个网格预测的类别概率乘以每个bbox的预测confidence，得到每个bbox的class-specific confidence score分数。对每个格子的每一个bounding box进行此运算，最后会得到7×7×2=98个scores，设置一个阈值，滤掉得分低的bboxes，对保留的bboxes进行NMS(Non Maximum Suppression)处理，最终得到目标检测结果。</p><p><img src="/img/4.28/12.png" alt="图像中第一个网格中两个bboxes经过计算后的结果"></p><p><img src="/img/4.28/13.png" alt="依次计算最终得到98个附带有score的tensors"></p><p><img src="/img/4.28/14.png" alt="依次计算最终得到98个附带有score的tensors"></p><h2 id="预测框的定位"><a href="#预测框的定位" class="headerlink" title="预测框的定位"></a>预测框的定位</h2><p><img src="/img/4.28/15.png" alt="得到7×7×30的张量之后，经过检测过程处理的到定位框"></p><p><img src="/img/4.28/16.png" alt="假设得到的每个20x1的tensor的第一维为预测为dog的score"></p><p><img src="/img/4.28/17.png" alt="通过设定阈值过滤掉score低的冗余bboxes"></p><p><img src="/img/4.28/18.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/19.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/20.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/21.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/22.png" alt="Non-Maximum Suppression: intuition"><br><img src="/img/4.28/23.png" alt="根据得到的98个bboxes中对dog的预测分数经过阈值后进行从大到小排序"></p><p>最后得到第一个为最大的score值，找出针对dog这个种类预测出的对应框，记为bbox_max。然后将它与其他分数较低的但不是0的框作对比，这种框记为bbox_cur。将bbox_max和bbox_cur分别做IoU计算，如果 IoU(bbox_max, bbox_cur) &gt; 0.5，那么将bbox_cur对应的score设为0。例如：</p><p><img src="/img/4.28/24.png" alt="IoU &gt; 0.5的分数较小的框的score归零"></p><p>然后接着遍历下一个score，如果它不是最大的且不为0，就和最大的score对应的框座IoU运算，若结果大于0.5则，同上。否则它的score不变，继续处理下一个bbox_cur……直到最后一个score，如图：</p><p><img src="/img/4.28/25.png" alt="IoU &lt; 0.5， 继续下一个score与max score对应框做IOU计算"></p><p>计算完一轮之后，假如得到score序列：0.5、0、0.2、0.1 … 0、0、0、0<br>那么进行下一轮循环，从0.2开始，将0.2对应的框作为bbox_max，继续循环计算后面的bbox_cur与新的bbox_max的IoU值，大于0.5的设为0，小于0.5的score不变。再这样一直计算比较到最后一个score。</p><p>得到新的的score序列为：0.5、0、0.2、0 … 0、0、0、0<br>即最后只得到两个score不为0的框，如图：</p><p><img src="/img/4.28/26.png" alt="多次循环完毕的结果"></p><p><img src="/img/4.28/27.png" alt="98个框对下一类别cat的score的NMS处理"></p><p><img src="/img/4.28/28.png" alt="对98个框的针对其他类别预测的score依次进行NMS处理"></p><h2 id="再筛选bounding-box"><a href="#再筛选bounding-box" class="headerlink" title="再筛选bounding box"></a>再筛选bounding box</h2><p>经过这些NMS算法的处理，会出现很多框针对某个class的预测的score为0的情况。<br>最后，针对每个bbox的20×1的张量，对20种class的预测score进行判断。例如，</p><ol><li>先取出针对bbox3的所有20个scores，按照类别的默认顺序找出score最大的那个score的index索引号(根据此index可以找出所属的类别)记为class；</li><li>然后找出bbox3的最大score分数，记为score。</li><li>判断score是否大于0，如果是，就在图像中画出标有class的框。否则，丢弃此bbox。<br>如图：</li></ol><p><img src="/img/4.28/29.png" alt="筛选bbox的过程示例"></p><p><img src="/img/4.28/30.png" alt="判断每个bbox是否能在图像中最后保留"></p><p><img src="/img/4.28/31.png" alt="遍历完毕，得到带有预测框的图像"></p><h2 id="YOLO-Key-Points"><a href="#YOLO-Key-Points" class="headerlink" title="YOLO Key Points"></a>YOLO Key Points</h2><p><img src="/img/4.28/32.png" alt="YOLO Key Points"></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="损失函数解析"><a href="#损失函数解析" class="headerlink" title="损失函数解析"></a>损失函数解析</h2><p>论文中损失函数的形式，一共分为四个部分:</p><p><img src="/img/4.28/33.png" alt="Loss Function"></p><p>在此损失函数中，只有某个网格中有object的中心落入的时候才对classification loss进行惩罚。只有当某个网格i中的bbox对某的ground-truth box负责的时候，才会对box的coordinate error进行惩罚。哪个bbox对ground-truth box负责就看ground-truth box和bbox的IoU最大。</p><p>Loss相关解释：</p><ol><li>8维的localization error和20维的classification error同样重要显然是不合理的，应该更重视8维的坐标预测，所以作者给这个损失前加了个权重，记为：<script type="math/tex">\lambda_{\text {coord }}</script>,在Pascal VOC训练中取5。</li><li>对于不存在object的bbox的confidence loss，赋予了更小的损失权重，记为<script type="math/tex">\lambda_{\text {noobj }}</script>，在Pascal VOC中取0.5。若没有任何物体中心落入边界框中，则<script type="math/tex">\widehat{C}_{i}</script>为0，此时我们希望预测含有物体的置信度<script type="math/tex">C_{i}</script>越小越好。然而，大部分bbox中都没有object，积少成多，造成loss的第2部分与第3部分的不平衡，因此，在loss的三部分增加权重<script type="math/tex">\lambda_{\text {noobj}}=0.5</script>。</li><li>对于存在object的bbox的confidence loss和类别的loss，权重取常数1。对于每个格子而言，作者设计只能包含同种物体。若格子中包含物体，我们希望希望预测正确的类别的概率越接近于1越好，而错误类别的概率越接近于0越好。loss第4部分中，若<script type="math/tex">\hat{p}_{i}(c)</script>为0中<script type="math/tex">c</script>为正确类别，则值为1，若非正确类别，则值为0。</li><li>对于第二行中为什么对<script type="math/tex">w_{i}</script>和<script type="math/tex">h_{i}</script>取平方根处理，原因是：对于一个图像中存在的不同大小的object，小物体的预测框偏离一点和大的相比肯定更严重，而sum-square error loss中对大小object的偏移loss是一同处理的。为了缓和，作者使用开方的方法，将bbox的width和height去平方根代替width和height。我们可以这样理解，假如预测出的小物体<script type="math/tex">(width=10, height=10)</script>的宽比实际大了10pixel，大物体<script type="math/tex">(width=100,height=100)</script>的宽度比实际预测大了10pixel。显然对于小物体，预测大了一倍，我们接受不了，而对大物体来说就影响不大。所以我们就要加大惩罚措施，让小物体预测大了10pixel的后果对于损失函数的值贡献更大，我们通过取平方根<script type="math/tex">\sqrt{20}-\sqrt{10} \approx 3.4>\sqrt{110}-\sqrt{100}\approx 0.5</script>，这样也算达到了目的吧。</li></ol><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="与其他检测方法对比"><a href="#与其他检测方法对比" class="headerlink" title="与其他检测方法对比"></a>与其他检测方法对比</h2><p>详见论文。</p><h3 id="在Pascal-VOC-2007上的测试结果"><a href="#在Pascal-VOC-2007上的测试结果" class="headerlink" title="在Pascal VOC 2007上的测试结果"></a>在Pascal VOC 2007上的测试结果</h3><p><img src="/img/4.28/34.png" alt="在Pascal VOC 2007 上与其他检测方法的对比"></p><p>Fast YOLO在Pascal数据集上速度最快，达到155FPS，而UOLO的mAP是最高的。</p><h3 id="VOC-2007错误分析"><a href="#VOC-2007错误分析" class="headerlink" title="VOC 2007错误分析"></a>VOC 2007错误分析</h3><p>比较了YOLO和Faster R-CNN的错误情况，结果如图所示。</p><p>预测结果包括以下几类：<br>正确：类别正确，IOU&gt;0.5<br>定位：类别正确，0.1<IOU<0.5类似：类别相似，IOU>0.1<br>其它：类别错误，IOU&gt;0.1<br>背景：任何物体，IOU&lt;0.1</p><p><img src="/img/4.28/35.png" alt="YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO"></p><h2 id="组合Fast-R-CNN-和-YOLO"><a href="#组合Fast-R-CNN-和-YOLO" class="headerlink" title="组合Fast R-CNN 和 YOLO"></a>组合Fast R-CNN 和 YOLO</h2><p><img src="/img/4.28/36.png" alt="模型组合在VOC 2007上的实验结果对比"></p><h2 id="在Pascal-VOC-2012上测试结果"><a href="#在Pascal-VOC-2012上测试结果" class="headerlink" title="在Pascal VOC 2012上测试结果"></a>在Pascal VOC 2012上测试结果</h2><p><img src="/img/4.28/37.png" alt="mAP排序"></p><h2 id="通用性"><a href="#通用性" class="headerlink" title="通用性"></a>通用性</h2><p><img src="/img/4.28/38.png" alt="通用性(Picasso 数据集和 People-Art数据集)YOLO都具有很好的检测结果"></p><h1 id="merits-and-drawbacks"><a href="#merits-and-drawbacks" class="headerlink" title="merits and drawbacks"></a>merits and drawbacks</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>假阳性(FP)错误率低。YOLO网络将整个图像的全部信息作为上下文，在训练的过程中使用到了全图信息，能够更好的区分目标个背景区域。</li><li>端到端(end-to-end)的方法，速度快，如官网所说，具有实时性，45fps，在YOLO-tiny上可以达到155fps。</li><li>通用性强，可以学到物体的generalizable-representation。对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于DPM和RCNN系列检测方法。</li></ol><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ol><li>性能表现比当前最先进的目标检测技术低</li><li>定位准确性差，对于小目标或者密集型群体object的定位效果不好。虽然每个格子可以预测出B个bounding box，但是最终只选择一个score最高的作为输出，即每个格子最多只得出一个box，如果多个物体的中心落在同一个格子，那么最终可能只预测出一个object。</li><li>YOLO的loss函数中，大物体的IoU误差和小物体的IoU的误差对训练中的loss贡献值接近，造成定位不准确。作者采用了一个去平方根的方式，但还不是最好的解决方法。</li><li>输入尺寸固定。由于输出层为全连接层，因此在检测时，YOLO训练模型只支持与训练图像相同的输入分辨率。其他分辨率需要缩放成此固定分辨率大小。</li><li>采用了多个下采样层，网络学到的物体特征并不精细，因此对检测效果会有影响。</li></ol><h1 id="代码-yolo-small即dark-net19"><a href="#代码-yolo-small即dark-net19" class="headerlink" title="代码(yolo-small即dark-net19)"></a>代码(yolo-small即dark-net19)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br> <br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> cv2<br> <br> <br><span class="hljs-comment"># leaky_relu激活函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">leaky_relu</span>(<span class="hljs-params">x, alpha=<span class="hljs-number">0.1</span></span>):</span><br>    <span class="hljs-keyword">return</span> tf.maximum(alpha * x, x)<br> <br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Yolo</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, weights_file, input_image, verbose=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-comment"># 后面程序打印描述功能的标志位</span><br>        self.verbose = verbose<br> <br>        <span class="hljs-comment"># 检测超参数</span><br>        self.S = <span class="hljs-number">7</span>  <span class="hljs-comment"># cell数量</span><br>        self.B = <span class="hljs-number">2</span>  <span class="hljs-comment"># 每个网格的边界框数</span><br>        self.classes = [<span class="hljs-string">&quot;aeroplane&quot;</span>, <span class="hljs-string">&quot;bicycle&quot;</span>, <span class="hljs-string">&quot;bird&quot;</span>, <span class="hljs-string">&quot;boat&quot;</span>, <span class="hljs-string">&quot;bottle&quot;</span>,<br>                        <span class="hljs-string">&quot;bus&quot;</span>, <span class="hljs-string">&quot;car&quot;</span>, <span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;chair&quot;</span>, <span class="hljs-string">&quot;cow&quot;</span>, <span class="hljs-string">&quot;diningtable&quot;</span>,<br>                        <span class="hljs-string">&quot;dog&quot;</span>, <span class="hljs-string">&quot;horse&quot;</span>, <span class="hljs-string">&quot;motorbike&quot;</span>, <span class="hljs-string">&quot;person&quot;</span>, <span class="hljs-string">&quot;pottedplant&quot;</span>,<br>                        <span class="hljs-string">&quot;sheep&quot;</span>, <span class="hljs-string">&quot;sofa&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;tvmonitor&quot;</span>]<br>        self.C = <span class="hljs-built_in">len</span>(self.classes)  <span class="hljs-comment"># 类别数</span><br> <br>        self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)] * self.S * self.B),<br>                                                [self.B, self.S, self.S]), [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>])<br>        self.y_offset = np.transpose(self.x_offset, [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>])  <span class="hljs-comment"># 改变数组的shape</span><br> <br>        self.threshold = <span class="hljs-number">0.2</span>  <span class="hljs-comment"># 类别置信度分数阈值</span><br>        self.iou_threshold = <span class="hljs-number">0.4</span>  <span class="hljs-comment"># IOU阈值，小于0.4的会过滤掉</span><br> <br>        self.max_output_size = <span class="hljs-number">10</span>  <span class="hljs-comment"># NMS选择的边界框的最大数量</span><br> <br>        self.sess = tf.Session()<br>        self._build_net()  <span class="hljs-comment"># 【1】搭建网络模型(预测):模型的主体网络部分，这个网络将输出[batch,7*7*30]的张量</span><br>        self._build_detector()  <span class="hljs-comment"># 【2】解析网络的预测结果：先判断预测框类别，再NMS</span><br>        self._load_weights(weights_file)  <span class="hljs-comment"># 【3】导入权重文件</span><br>        self.detect_from_file(image_file=input_image)  <span class="hljs-comment"># 【4】从预测输入图片，并可视化检测边界框、将obj的分类结果和坐标保存成txt。</span><br> <br>    <span class="hljs-comment"># 【1】搭建网络模型(预测):模型的主体网络部分，这个网络将输出[batch,7*7*30]的张量</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_build_net</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 打印状态信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start to build the network ...&quot;</span>)<br> <br>        <span class="hljs-comment"># 输入、输出用占位符，因为尺寸一般不会改变</span><br>        self.images = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment"># None表示不确定，为了自适应batchsize</span><br> <br>        <span class="hljs-comment"># 搭建网络模型</span><br>        net = self._conv_layer(self.images, <span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">2</span>, <span class="hljs-number">192</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">3</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">4</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">5</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">6</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">7</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">8</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">9</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">10</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">11</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">12</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">13</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">14</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">15</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">16</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._maxpool_layer(net, <span class="hljs-number">16</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">17</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">18</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">19</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">20</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">21</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">22</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">23</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._conv_layer(net, <span class="hljs-number">24</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        net = self._flatten(net)<br>        net = self._fc_layer(net, <span class="hljs-number">25</span>, <span class="hljs-number">512</span>, activation=leaky_relu)<br>        net = self._fc_layer(net, <span class="hljs-number">26</span>, <span class="hljs-number">4096</span>, activation=leaky_relu)<br>        net = self._fc_layer(net, <span class="hljs-number">27</span>, self.S * self.S * (self.B * <span class="hljs-number">5</span> + self.C))<br> <br>        <span class="hljs-comment"># 网络输出，[batch,7*7*30]的张量</span><br>        self.predicts = net<br> <br>    <span class="hljs-comment"># 【2】解析网络的预测结果：先判断预测框类别，再NMS</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_build_detector</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 原始图片的宽和高</span><br>        self.width = tf.placeholder(tf.float32, name=<span class="hljs-string">&#x27;img_w&#x27;</span>)<br>        self.height = tf.placeholder(tf.float32, name=<span class="hljs-string">&#x27;img_h&#x27;</span>)<br> <br>        <span class="hljs-comment"># 网络回归[batch,7*7*30]：</span><br>        idx1 = self.S * self.S * self.C<br>        idx2 = idx1 + self.S * self.S * self.B<br>        <span class="hljs-comment"># 1.类别概率[:,:7*7*20]  20维</span><br>        class_probs = tf.reshape(self.predicts[<span class="hljs-number">0</span>, :idx1], [self.S, self.S, self.C])<br>        <span class="hljs-comment"># 2.置信度[:,7*7*20:7*7*(20+2)]  2维</span><br>        confs = tf.reshape(self.predicts[<span class="hljs-number">0</span>, idx1:idx2], [self.S, self.S, self.B])<br>        <span class="hljs-comment"># 3.边界框[:,7*7*(20+2):]  8维 -&gt; (x,y,w,h)</span><br>        boxes = tf.reshape(self.predicts[<span class="hljs-number">0</span>, idx2:], [self.S, self.S, self.B, <span class="hljs-number">4</span>])<br> <br>        <span class="hljs-comment"># 将x，y转换为相对于图像左上角的坐标</span><br>        <span class="hljs-comment"># w，h的预测是平方根乘以图像的宽度和高度</span><br>        boxes = tf.stack([(boxes[:, :, :, <span class="hljs-number">0</span>] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,<br>                          (boxes[:, :, :, <span class="hljs-number">1</span>] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height,<br>                          tf.square(boxes[:, :, :, <span class="hljs-number">2</span>]) * self.width,<br>                          tf.square(boxes[:, :, :, <span class="hljs-number">3</span>]) * self.height], axis=<span class="hljs-number">3</span>)<br> <br>        <span class="hljs-comment"># 类别置信度分数：[S,S,B,1]*[S,S,1,C]=[S,S,B,类别置信度C]</span><br>        scores = tf.expand_dims(confs, -<span class="hljs-number">1</span>) * tf.expand_dims(class_probs, <span class="hljs-number">2</span>)<br> <br>        scores = tf.reshape(scores, [-<span class="hljs-number">1</span>, self.C])  <span class="hljs-comment"># [S*S*B, C]</span><br>        boxes = tf.reshape(boxes, [-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])  <span class="hljs-comment"># [S*S*B, 4]</span><br> <br>        <span class="hljs-comment"># 只选择类别置信度最大的值作为box的类别、分数</span><br>        box_classes = tf.argmax(scores, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 边界框box的类别</span><br>        box_class_scores = tf.reduce_max(scores, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 边界框box的分数</span><br> <br>        <span class="hljs-comment"># 利用类别置信度阈值self.threshold，过滤掉类别置信度低的</span><br>        filter_mask = box_class_scores &gt;= self.threshold<br>        scores = tf.boolean_mask(box_class_scores, filter_mask)<br>        boxes = tf.boolean_mask(boxes, filter_mask)<br>        box_classes = tf.boolean_mask(box_classes, filter_mask)<br> <br>        <span class="hljs-comment"># NMS (不区分不同的类别)</span><br>        <span class="hljs-comment"># 中心坐标+宽高box (x, y, w, h) -&gt; xmin=x-w/2 -&gt; 左上+右下box (xmin, ymin, xmax, ymax)，因为NMS函数是这种计算方式</span><br>        _boxes = tf.stack([boxes[:, <span class="hljs-number">0</span>] - <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">1</span>] - <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">3</span>],<br>                           boxes[:, <span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">2</span>], boxes[:, <span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span> * boxes[:, <span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>)<br>        nms_indices = tf.image.non_max_suppression(_boxes, scores,<br>                                                   self.max_output_size, self.iou_threshold)<br>        self.scores = tf.gather(scores, nms_indices)<br>        self.boxes = tf.gather(boxes, nms_indices)<br>        self.box_classes = tf.gather(box_classes, nms_indices)<br> <br>    <span class="hljs-comment"># 【3】导入权重文件</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_load_weights</span>(<span class="hljs-params">self, weights_file</span>):</span><br>        <span class="hljs-comment"># 打印状态信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start to load weights from file:%s&quot;</span> % (weights_file))<br> <br>        <span class="hljs-comment"># 导入权重</span><br>        saver = tf.train.Saver()  <span class="hljs-comment"># 初始化</span><br>        saver.restore(self.sess, weights_file)  <span class="hljs-comment"># saver.restore导入/saver.save保存</span><br> <br>    <span class="hljs-comment"># 【4】从预测输入图片，并可视化检测边界框、将obj的分类结果和坐标保存成txt。</span><br>    <span class="hljs-comment"># image_file是输入图片文件路径；</span><br>    <span class="hljs-comment"># deteted_boxes_file=&quot;boxes.txt&quot;是最后坐标txt；detected_image_file=&quot;detected_image.jpg&quot;是检测结果可视化图片</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">detect_from_file</span>(<span class="hljs-params">self, image_file, imshow=<span class="hljs-literal">True</span>, deteted_boxes_file=<span class="hljs-string">&quot;boxes.txt&quot;</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                         detected_image_file=<span class="hljs-string">&quot;detected_image.jpg&quot;</span></span>):</span><br>        <span class="hljs-comment"># read image</span><br>        image = cv2.imread(image_file)<br>        img_h, img_w, _ = image.shape<br>        scores, boxes, box_classes = self._detect_from_image(image)<br>        predict_boxes = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(scores)):<br>            <span class="hljs-comment"># 预测框数据为：[概率,x,y,w,h,类别置信度]</span><br>            predict_boxes.append((self.classes[box_classes[i]], boxes[i, <span class="hljs-number">0</span>],<br>                                  boxes[i, <span class="hljs-number">1</span>], boxes[i, <span class="hljs-number">2</span>], boxes[i, <span class="hljs-number">3</span>], scores[i]))<br>        self.show_results(image, predict_boxes, imshow, deteted_boxes_file, detected_image_file)<br> <br>    <span class="hljs-comment">################# 对应【1】:定义conv/maxpool/flatten/fc层#############################################################</span><br>    <span class="hljs-comment"># 卷积层：x输入；id：层数索引；num_filters：卷积核个数；filter_size：卷积核尺寸；stride：步长</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_conv_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, num_filters, filter_size, stride</span>):</span><br> <br>        <span class="hljs-comment"># 通道数</span><br>        in_channels = x.get_shape().as_list()[-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># 均值为0标准差为0.1的正态分布，初始化权重w；shape=行*列*通道数*卷积核个数</span><br>        weight = tf.Variable(<br>            tf.truncated_normal([filter_size, filter_size, in_channels, num_filters], mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">0.1</span>))<br>        bias = tf.Variable(tf.zeros([num_filters, ]))  <span class="hljs-comment"># 列向量</span><br> <br>        <span class="hljs-comment"># padding, 注意: 不用padding=&quot;SAME&quot;,否则可能会导致坐标计算错误</span><br>        pad_size = filter_size // <span class="hljs-number">2</span>  <span class="hljs-comment"># 除法运算，保留商的整数部分</span><br>        pad_mat = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [pad_size, pad_size], [pad_size, pad_size], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>        x_pad = tf.pad(x, pad_mat)<br>        conv = tf.nn.conv2d(x_pad, weight, strides=[<span class="hljs-number">1</span>, stride, stride, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&quot;VALID&quot;</span>)<br>        output = leaky_relu(tf.nn.bias_add(conv, bias))<br> <br>        <span class="hljs-comment"># 打印该层信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=conv,num_filter=%d,filter_size=%d,stride=%d,output_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, num_filters, filter_size, stride, <span class="hljs-built_in">str</span>(output.get_shape())))<br> <br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment"># 池化层：x输入；id：层数索引；pool_size：池化尺寸；stride：步长</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_maxpool_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, pool_size, stride</span>):</span><br>        output = tf.layers.max_pooling2d(inputs=x,<br>                                         pool_size=pool_size,<br>                                         strides=stride,<br>                                         padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)<br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=MaxPool,pool_size=%d,stride=%d,out_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, pool_size, stride, <span class="hljs-built_in">str</span>(output.get_shape())))<br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment"># 扁平层：因为接下来会连接全连接层，例如[n_samples, 7, 7, 32] -&gt; [n_samples, 7*7*32]</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_flatten</span>(<span class="hljs-params">self, x</span>):</span><br>        tran_x = tf.transpose(x, [<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])  <span class="hljs-comment"># [batch,行,列,通道数channels] -&gt; [batch,通道数channels,列,行]</span><br>        nums = np.product(x.get_shape().as_list()[<span class="hljs-number">1</span>:])  <span class="hljs-comment"># 计算的是总共的神经元数量，第一个表示batch数量所以去掉</span><br>        <span class="hljs-keyword">return</span> tf.reshape(tran_x, [-<span class="hljs-number">1</span>, nums])  <span class="hljs-comment"># [batch,通道数channels,列,行] -&gt; [batch,通道数channels*列*行],-1代表自适应batch数量</span><br> <br>    <span class="hljs-comment"># 全连接层：x输入；id：层数索引；num_out：输出尺寸；activation：激活函数</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_fc_layer</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">id</span>, num_out, activation=<span class="hljs-literal">None</span></span>):</span><br>        num_in = x.get_shape().as_list()[-<span class="hljs-number">1</span>]  <span class="hljs-comment"># 通道数/维度</span><br>        <span class="hljs-comment"># 均值为0标准差为0.1的正态分布，初始化权重w；shape=行*列*通道数*卷积核个数</span><br>        weight = tf.Variable(tf.truncated_normal(shape=[num_in, num_out], mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">0.1</span>))<br>        bias = tf.Variable(tf.zeros(shape=[num_out, ]))  <span class="hljs-comment"># 列向量</span><br>        output = tf.nn.xw_plus_b(x, weight, bias)<br> <br>        <span class="hljs-comment"># 正常全连接层是leak_relu激活函数；但是最后一层是liner函数</span><br>        <span class="hljs-keyword">if</span> activation:<br>            output = activation(output)<br> <br>        <span class="hljs-comment"># 打印该层信息</span><br>        <span class="hljs-keyword">if</span> self.verbose:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Layer%d:type=Fc,num_out=%d,output_shape=%s&#x27;</span><br>                  % (<span class="hljs-built_in">id</span>, num_out, <span class="hljs-built_in">str</span>(output.get_shape())))<br>        <span class="hljs-keyword">return</span> output<br> <br>    <span class="hljs-comment">######################## 对应【4】:可视化检测边界框、将obj的分类结果和坐标保存成txt#########################################</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_detect_from_image</span>(<span class="hljs-params">self, image</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Do detection given a cv image&quot;&quot;&quot;</span><br>        img_h, img_w, _ = image.shape<br>        img_resized = cv2.resize(image, (<span class="hljs-number">448</span>, <span class="hljs-number">448</span>))<br>        img_RGB = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)<br>        img_resized_np = np.asarray(img_RGB)<br>        _images = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>, <span class="hljs-number">3</span>), dtype=np.float32)<br>        _images[<span class="hljs-number">0</span>] = (img_resized_np / <span class="hljs-number">255.0</span>) * <span class="hljs-number">2.0</span> - <span class="hljs-number">1.0</span><br>        scores, boxes, box_classes = self.sess.run([self.scores, self.boxes, self.box_classes],<br>                                                   feed_dict=&#123;self.images: _images, self.width: img_w,<br>                                                              self.height: img_h&#125;)<br>        <span class="hljs-keyword">return</span> scores, boxes, box_classes<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_results</span>(<span class="hljs-params">self, image, results, imshow=<span class="hljs-literal">True</span>, deteted_boxes_file=<span class="hljs-literal">None</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                     detected_image_file=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Show the detection boxes&quot;&quot;&quot;</span><br>        img_cp = image.copy()<br>        <span class="hljs-keyword">if</span> deteted_boxes_file:<br>            f = <span class="hljs-built_in">open</span>(deteted_boxes_file, <span class="hljs-string">&quot;w&quot;</span>)<br>        <span class="hljs-comment"># draw boxes</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(results)):<br>            x = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">1</span>])<br>            y = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">2</span>])<br>            w = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">3</span>]) // <span class="hljs-number">2</span><br>            h = <span class="hljs-built_in">int</span>(results[i][<span class="hljs-number">4</span>]) // <span class="hljs-number">2</span><br>            <span class="hljs-keyword">if</span> self.verbose:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class: %s, [x, y, w, h]=[%d, %d, %d, %d], confidence=%f&quot;</span><br>                      % (results[i][<span class="hljs-number">0</span>], x, y, w, h, results[i][-<span class="hljs-number">1</span>]))<br> <br>                <span class="hljs-comment"># 中心坐标 + 宽高box(x, y, w, h) -&gt; xmin = x - w / 2 -&gt; 左上 + 右下box(xmin, ymin, xmax, ymax)</span><br>                cv2.rectangle(img_cp, (x - w, y - h), (x + w, y + h), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)<br> <br>                <span class="hljs-comment"># 在边界框上显示类别、分数(类别置信度)</span><br>                cv2.rectangle(img_cp, (x - w, y - h - <span class="hljs-number">20</span>), (x + w, y - h), (<span class="hljs-number">125</span>, <span class="hljs-number">125</span>, <span class="hljs-number">125</span>), -<span class="hljs-number">1</span>)  <span class="hljs-comment"># puttext函数的背景</span><br>                cv2.putText(img_cp, results[i][<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27; : %.2f&#x27;</span> % results[i][<span class="hljs-number">5</span>], (x - w + <span class="hljs-number">5</span>, y - h - <span class="hljs-number">7</span>),<br>                            cv2.FONT_HERSHEY_SIMPLEX, <span class="hljs-number">0.5</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)<br> <br>            <span class="hljs-keyword">if</span> deteted_boxes_file:<br>                <span class="hljs-comment"># 保存obj检测结果为txt文件</span><br>                f.write(results[i][<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(x) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(y) + <span class="hljs-string">&#x27;,&#x27;</span> +<br>                        <span class="hljs-built_in">str</span>(w) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(h) + <span class="hljs-string">&#x27;,&#x27;</span> + <span class="hljs-built_in">str</span>(results[i][<span class="hljs-number">5</span>]) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>        <span class="hljs-keyword">if</span> imshow:<br>            cv2.imshow(<span class="hljs-string">&#x27;YOLO_small detection&#x27;</span>, img_cp)<br>            cv2.waitKey(<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> detected_image_file:<br>            cv2.imwrite(detected_image_file, img_cp)<br>        <span class="hljs-keyword">if</span> deteted_boxes_file:<br>            f.close()<br> <br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    yolo_net = Yolo(weights_file=<span class="hljs-string">&#x27;D:/Python/YOLOv1-Tensorflow-master/YOLO_small.ckpt&#x27;</span>,<br>                    input_image=<span class="hljs-string">&#x27;D:/Python/YOLOv1-Tensorflow-master/car.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Computer Version </category>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-7</title>
      <link href="2021/04/22/DL-with-python-7/"/>
      <url>2021/04/22/DL-with-python-7/</url>
      
        <content type="html"><![CDATA[<p>5.2-using-convnets-with-small-datasets</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="5-2-Using-convnets-with-small-datasets"><a href="#5-2-Using-convnets-with-small-datasets" class="headerlink" title="5.2 - Using convnets with small datasets"></a>5.2 - Using convnets with small datasets</h1><p>This notebook contains the code sample found in Chapter 5, Section 2 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><h2 id="Training-a-convnet-from-scratch-on-a-small-dataset"><a href="#Training-a-convnet-from-scratch-on-a-small-dataset" class="headerlink" title="Training a convnet from scratch on a small dataset"></a>Training a convnet from scratch on a small dataset</h2><p>Having to train an image classification model using only very little data is a common situation, which you likely encounter yourself in<br>practice if you ever do computer vision in a professional context.</p><p>Having “few” samples can mean anywhere from a few hundreds to a few tens of thousands of images. As a practical example, we will focus on<br>classifying images as “dogs” or “cats”, in a dataset containing 4000 pictures of cats and dogs (2000 cats, 2000 dogs). We will use 2000<br>pictures for training, 1000 for validation, and finally 1000 for testing.</p><p>In this section, we will review one basic strategy to tackle this problem: training a new model from scratch on what little data we have. We<br>will start by naively training a small convnet on our 2000 training samples, without any regularization, to set a baseline for what can be<br>achieved. This will get us to a classification accuracy of 71%. At that point, our main issue will be overfitting. Then we will introduce<br><em>data augmentation</em>, a powerful technique for mitigating overfitting in computer vision. By leveraging data augmentation, we will improve<br>our network to reach an accuracy of 82%.</p><p>In the next section, we will review two more essential techniques for applying deep learning to small datasets: <em>doing feature extraction<br>with a pre-trained network</em> (this will get us to an accuracy of 90% to 93%), and <em>fine-tuning a pre-trained network</em> (this will get us to<br>our final accuracy of 95%). Together, these three strategies — training a small model from scratch, doing feature extracting using a<br>pre-trained model, and fine-tuning a pre-trained model — will constitute your future toolbox for tackling the problem of doing computer<br>vision with small datasets.</p><h2 id="The-relevance-of-deep-learning-for-small-data-problems"><a href="#The-relevance-of-deep-learning-for-small-data-problems" class="headerlink" title="The relevance of deep learning for small-data problems"></a>The relevance of deep learning for small-data problems</h2><p>You will sometimes hear that deep learning only works when lots of data is available. This is in part a valid point: one fundamental<br>characteristic of deep learning is that it is able to find interesting features in the training data on its own, without any need for manual<br>feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where<br>the input samples are very high-dimensional, like images.</p><p>However, what constitutes “lots” of samples is relative — relative to the size and depth of the network you are trying to train, for<br>starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundreds can<br>potentially suffice if the model is small and well-regularized and if the task is simple.<br>Because convnets learn local, translation-invariant features, they are very<br>data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results<br>despite a relative lack of data, without the need for any custom feature engineering. You will see this in action in this section.</p><p>But what’s more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model<br>trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes. Specifically, in the case of<br>computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used<br>to bootstrap powerful vision models out of very little data. That’s what we will do in the next section.</p><p>For now, let’s get started by getting our hands on the data.</p><h2 id="Downloading-the-data"><a href="#Downloading-the-data" class="headerlink" title="Downloading the data"></a>Downloading the data</h2><p>The cats vs. dogs dataset that we will use isn’t packaged with Keras. It was made available by Kaggle.com as part of a computer vision<br>competition in late 2013, back when convnets weren’t quite mainstream. You can download the original dataset at:<br><code>https://www.kaggle.com/c/dogs-vs-cats/data</code> (you will need to create a Kaggle account if you don’t already have one — don’t worry, the<br>process is painless).</p><p>The pictures are medium-resolution color JPEGs. They look like this:</p><p><img src="https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg" alt="cats_vs_dogs_samples"></p><p>Unsurprisingly, the cats vs. dogs Kaggle competition in 2013 was won by entrants who used convnets. The best entries could achieve up to<br>95% accuracy. In our own example, we will get fairly close to this accuracy (in the next section), even though we will be training our<br>models on less than 10% of the data that was available to the competitors.<br>This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). After downloading<br>and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, a validation<br>set with 500 samples of each class, and finally a test set with 500 samples of each class.</p><p>Here are a few lines of code to do this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os, shutil<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># The path to the directory where the original</span><br><span class="hljs-comment"># dataset was uncompressed</span><br>original_dataset_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/kaggle_original_data&#x27;</span><br><br><span class="hljs-comment"># The directory where we will</span><br><span class="hljs-comment"># store our smaller dataset</span><br>base_dir = <span class="hljs-string">&#x27;/Users/fchollet/Downloads/cats_and_dogs_small&#x27;</span><br>os.mkdir(base_dir)<br><br><span class="hljs-comment"># Directories for our training,</span><br><span class="hljs-comment"># validation and test splits</span><br>train_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;train&#x27;</span>)<br>os.mkdir(train_dir)<br>validation_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;validation&#x27;</span>)<br>os.mkdir(validation_dir)<br>test_dir = os.path.join(base_dir, <span class="hljs-string">&#x27;test&#x27;</span>)<br>os.mkdir(test_dir)<br><br><span class="hljs-comment"># Directory with our training cat pictures</span><br>train_cats_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(train_cats_dir)<br><br><span class="hljs-comment"># Directory with our training dog pictures</span><br>train_dogs_dir = os.path.join(train_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(train_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(validation_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(validation_dogs_dir)<br><br><span class="hljs-comment"># Directory with our validation cat pictures</span><br>test_cats_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;cats&#x27;</span>)<br>os.mkdir(test_cats_dir)<br><br><span class="hljs-comment"># Directory with our validation dog pictures</span><br>test_dogs_dir = os.path.join(test_dir, <span class="hljs-string">&#x27;dogs&#x27;</span>)<br>os.mkdir(test_dogs_dir)<br><br><span class="hljs-comment"># Copy first 1000 cat images to train_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br><br><span class="hljs-comment"># Copy next 500 cat images to validation_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 cat images to test_cats_dir</span><br>fnames = [<span class="hljs-string">&#x27;cat.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_cats_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy first 1000 dog images to train_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(train_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to validation_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>, <span class="hljs-number">1500</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(validation_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br>    <br><span class="hljs-comment"># Copy next 500 dog images to test_dogs_dir</span><br>fnames = [<span class="hljs-string">&#x27;dog.&#123;&#125;.jpg&#x27;</span>.<span class="hljs-built_in">format</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1500</span>, <span class="hljs-number">2000</span>)]<br><span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> fnames:<br>    src = os.path.join(original_dataset_dir, fname)<br>    dst = os.path.join(test_dogs_dir, fname)<br>    shutil.copyfile(src, dst)<br></code></pre></td></tr></table></figure><p>As a sanity check, let’s count how many pictures we have in each training split (train/validation/test):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total training cat images: 1000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total training dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(train_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total training dog images: 1000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total validation cat images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total validation dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(validation_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total validation dog images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test cat images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_cats_dir)))<br></code></pre></td></tr></table></figure><pre><code>total test cat images: 500</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;total test dog images:&#x27;</span>, <span class="hljs-built_in">len</span>(os.listdir(test_dogs_dir)))<br></code></pre></td></tr></table></figure><pre><code>total test dog images: 500</code></pre><p>So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of<br>samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate<br>measure of success.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>We’ve already built a small convnet for MNIST in the previous example, so you should be familiar with them. We will reuse the same<br>general structure: our convnet will be a stack of alternated <code>Conv2D</code> (with <code>relu</code> activation) and <code>MaxPooling2D</code> layers.</p><p>However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger: it will have one<br>more <code>Conv2D</code> + <code>MaxPooling2D</code> stage. This serves both to augment the capacity of the network, and to further reduce the size of the<br>feature maps, so that they aren’t overly large when we reach the <code>Flatten</code> layer. Here, since we start from inputs of size 150x150 (a<br>somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the <code>Flatten</code> layer.</p><p>Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is<br>decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.</p><p>Since we are attacking a binary classification problem, we are ending the network with a single unit (a <code>Dense</code> layer of size 1) and a<br><code>sigmoid</code> activation. This unit will encode the probability that the network is looking at one class or the other.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><br>model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Let’s take a look at how the dimensions of the feature maps change with every successive layer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         _________________________________________________________________flatten_1 (Flatten)          (None, 6272)              0         _________________________________________________________________dense_1 (Dense)              (None, 512)               3211776   _________________________________________________________________dense_2 (Dense)              (None, 1)                 513       =================================================================Total params: 3,453,121Trainable params: 3,453,121Non-trainable params: 0_________________________________________________________________</code></pre><p>For our compilation step, we’ll go with the <code>RMSprop</code> optimizer as usual. Since we ended our network with a single sigmoid unit, we will<br>use binary crossentropy as our loss (as a reminder, check out the table in Chapter 4, section 5 for a cheatsheet on what loss function to<br>use in various situations).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p>As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our<br>network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:</p><ul><li>Read the picture files.</li><li>Decode the JPEG content to RBG grids of pixels.</li><li>Convert these into floating point tensors.</li><li>Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).</li></ul><p>It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image<br>processing helper tools, located at <code>keras.preprocessing.image</code>. In particular, it contains the class <code>ImageDataGenerator</code> which allows to<br>quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we<br>will use here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator<br><br><span class="hljs-comment"># All images will be rescaled by 1./255</span><br>train_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">20</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br></code></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.</code></pre><p>Let’s take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape <code>(20, 150, 150, 3)</code>) and binary<br>labels (shape <code>(20,)</code>). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches<br>indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to <code>break</code> the iteration loop<br>at some point.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> data_batch, labels_batch <span class="hljs-keyword">in</span> train_generator:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data batch shape:&#x27;</span>, data_batch.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;labels batch shape:&#x27;</span>, labels_batch.shape)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><pre><code>data batch shape: (20, 150, 150, 3)labels batch shape: (20,)</code></pre><p>Let’s fit our model to the data using the generator. We do it using the <code>fit_generator</code> method, the equivalent of <code>fit</code> for data generators<br>like ours. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does.<br>Because the data is being generated endlessly, the generator needs to know example how many samples to draw from the generator before<br>declaring an epoch over. This is the role of the <code>steps_per_epoch</code> argument: after having drawn <code>steps_per_epoch</code> batches from the<br>generator, i.e. after having run for <code>steps_per_epoch</code> gradient descent steps, the fitting process will go to the next epoch. In our case,<br>batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.</p><p>When using <code>fit_generator</code>, one may pass a <code>validation_data</code> argument, much like with the <code>fit</code> method. Importantly, this argument is<br>allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as <code>validation_data</code>, then<br>this generator is expected to yield batches of validation data endlessly, and thus you should also specify the <code>validation_steps</code> argument,<br>which tells the process how many batches to draw from the validation generator for evaluation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">30</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/30100/100 [==============================] - 9s - loss: 0.6898 - acc: 0.5285 - val_loss: 0.6724 - val_acc: 0.5950Epoch 2/30100/100 [==============================] - 8s - loss: 0.6543 - acc: 0.6340 - val_loss: 0.6565 - val_acc: 0.5950Epoch 3/30100/100 [==============================] - 8s - loss: 0.6143 - acc: 0.6690 - val_loss: 0.6116 - val_acc: 0.6650Epoch 4/30100/100 [==============================] - 8s - loss: 0.5626 - acc: 0.7125 - val_loss: 0.5774 - val_acc: 0.6970Epoch 5/30100/100 [==============================] - 8s - loss: 0.5266 - acc: 0.7335 - val_loss: 0.5726 - val_acc: 0.6960Epoch 6/30100/100 [==============================] - 8s - loss: 0.5007 - acc: 0.7550 - val_loss: 0.6075 - val_acc: 0.6580Epoch 7/30100/100 [==============================] - 8s - loss: 0.4723 - acc: 0.7840 - val_loss: 0.5516 - val_acc: 0.7060Epoch 8/30100/100 [==============================] - 8s - loss: 0.4521 - acc: 0.7875 - val_loss: 0.5724 - val_acc: 0.6980Epoch 9/30100/100 [==============================] - 8s - loss: 0.4163 - acc: 0.8095 - val_loss: 0.5653 - val_acc: 0.7140Epoch 10/30100/100 [==============================] - 8s - loss: 0.3988 - acc: 0.8185 - val_loss: 0.5508 - val_acc: 0.7180Epoch 11/30100/100 [==============================] - 8s - loss: 0.3694 - acc: 0.8385 - val_loss: 0.5712 - val_acc: 0.7300Epoch 12/30100/100 [==============================] - 8s - loss: 0.3385 - acc: 0.8465 - val_loss: 0.6097 - val_acc: 0.7110Epoch 13/30100/100 [==============================] - 8s - loss: 0.3229 - acc: 0.8565 - val_loss: 0.5827 - val_acc: 0.7150Epoch 14/30100/100 [==============================] - 8s - loss: 0.2962 - acc: 0.8720 - val_loss: 0.5928 - val_acc: 0.7190Epoch 15/30100/100 [==============================] - 8s - loss: 0.2684 - acc: 0.9005 - val_loss: 0.5921 - val_acc: 0.7190Epoch 16/30100/100 [==============================] - 8s - loss: 0.2509 - acc: 0.8980 - val_loss: 0.6148 - val_acc: 0.7250Epoch 17/30100/100 [==============================] - 8s - loss: 0.2221 - acc: 0.9110 - val_loss: 0.6487 - val_acc: 0.7010Epoch 18/30100/100 [==============================] - 8s - loss: 0.2021 - acc: 0.9250 - val_loss: 0.6185 - val_acc: 0.7300Epoch 19/30100/100 [==============================] - 8s - loss: 0.1824 - acc: 0.9310 - val_loss: 0.7713 - val_acc: 0.7020Epoch 20/30100/100 [==============================] - 8s - loss: 0.1579 - acc: 0.9425 - val_loss: 0.6657 - val_acc: 0.7260Epoch 21/30100/100 [==============================] - 8s - loss: 0.1355 - acc: 0.9550 - val_loss: 0.8077 - val_acc: 0.7040Epoch 22/30100/100 [==============================] - 8s - loss: 0.1247 - acc: 0.9545 - val_loss: 0.7726 - val_acc: 0.7080Epoch 23/30100/100 [==============================] - 8s - loss: 0.1111 - acc: 0.9585 - val_loss: 0.7387 - val_acc: 0.7220Epoch 24/30100/100 [==============================] - 8s - loss: 0.0932 - acc: 0.9710 - val_loss: 0.8196 - val_acc: 0.7050Epoch 25/30100/100 [==============================] - 8s - loss: 0.0707 - acc: 0.9790 - val_loss: 0.9012 - val_acc: 0.7190Epoch 26/30100/100 [==============================] - 8s - loss: 0.0625 - acc: 0.9855 - val_loss: 1.0437 - val_acc: 0.6970Epoch 27/30100/100 [==============================] - 8s - loss: 0.0611 - acc: 0.9820 - val_loss: 0.9831 - val_acc: 0.7060Epoch 28/30100/100 [==============================] - 8s - loss: 0.0488 - acc: 0.9865 - val_loss: 0.9721 - val_acc: 0.7310Epoch 29/30100/100 [==============================] - 8s - loss: 0.0375 - acc: 0.9915 - val_loss: 0.9987 - val_acc: 0.7100Epoch 30/30100/100 [==============================] - 8s - loss: 0.0387 - acc: 0.9895 - val_loss: 1.0139 - val_acc: 0.7240</code></pre><p>It is good practice to always save your models after training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_1.h5&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Let’s plot the loss and accuracy of the model over the training and validation data during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_30_0p.png" alt="output"></p><p><img src="/img/output_30_1.png" alt="output"></p><p>These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our<br>validation accuracy stalls at 70-72%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss<br>keeps decreasing linearly until it reaches nearly 0.</p><p>Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. You already know about a<br>number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to<br>introduce a new one, specific to computer vision, and used almost universally when processing images with deep learning models: <em>data<br>augmentation</em>.</p><h2 id="Using-data-augmentation"><a href="#Using-data-augmentation" class="headerlink" title="Using data augmentation"></a>Using data augmentation</h2><p>Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data.<br>Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data<br>augmentation takes the approach of generating more training data from existing training samples, by “augmenting” the samples via a number<br>of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same<br>picture twice. This helps the model get exposed to more aspects of the data and generalize better.</p><p>In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our <code>ImageDataGenerator</code><br>instance. Let’s get started with an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">datagen = ImageDataGenerator(<br>      rotation_range=<span class="hljs-number">40</span>,<br>      width_shift_range=<span class="hljs-number">0.2</span>,<br>      height_shift_range=<span class="hljs-number">0.2</span>,<br>      shear_range=<span class="hljs-number">0.2</span>,<br>      zoom_range=<span class="hljs-number">0.2</span>,<br>      horizontal_flip=<span class="hljs-literal">True</span>,<br>      fill_mode=<span class="hljs-string">&#x27;nearest&#x27;</span>)<br></code></pre></td></tr></table></figure><p>These are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over what we just wrote:</p><ul><li><code>rotation_range</code> is a value in degrees (0-180), a range within which to randomly rotate pictures.</li><li><code>width_shift</code> and <code>height_shift</code> are ranges (as a fraction of total width or height) within which to randomly translate pictures<br>vertically or horizontally.</li><li><code>shear_range</code> is for randomly applying shearing transformations.</li><li><code>zoom_range</code> is for randomly zooming inside pictures.</li><li><code>horizontal_flip</code> is for randomly flipping half of the images horizontally — relevant when there are no assumptions of horizontal<br>asymmetry (e.g. real-world pictures).</li><li><code>fill_mode</code> is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.</li></ul><p>Let’s take a look at our augmented images:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This is module with image preprocessing utilities</span><br><span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> image<br><br>fnames = [os.path.join(train_cats_dir, fname) <span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> os.listdir(train_cats_dir)]<br><br><span class="hljs-comment"># We pick one image to &quot;augment&quot;</span><br>img_path = fnames[<span class="hljs-number">3</span>]<br><br><span class="hljs-comment"># Read the image and resize it</span><br>img = image.load_img(img_path, target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>))<br><br><span class="hljs-comment"># Convert it to a Numpy array with shape (150, 150, 3)</span><br>x = image.img_to_array(img)<br><br><span class="hljs-comment"># Reshape it to (1, 150, 150, 3)</span><br>x = x.reshape((<span class="hljs-number">1</span>,) + x.shape)<br><br><span class="hljs-comment"># The .flow() command below generates batches of randomly transformed images.</span><br><span class="hljs-comment"># It will loop indefinitely, so we need to `break` the loop at some point!</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> datagen.flow(x, batch_size=<span class="hljs-number">1</span>):<br>    plt.figure(i)<br>    imgplot = plt.imshow(image.array_to_img(batch[<span class="hljs-number">0</span>]))<br>    i += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">4</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">break</span><br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_35_0.png" alt="output"></p><p><img src="/img/output_35_1.png" alt="output"></p><p><img src="/img/output_35_2.png" alt="output"></p><p><img src="/img/output_35_3.png" alt="output"></p><p>If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs<br>that it sees are still heavily intercorrelated, since they come from a small number of original images — we cannot produce new information,<br>we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight<br>overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                        input_shape=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>, <span class="hljs-number">3</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Flatten())<br>model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              optimizer=optimizers.RMSprop(lr=<span class="hljs-number">1e-4</span>),<br>              metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Let’s train our network using data augmentation and dropout:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">train_datagen = ImageDataGenerator(<br>    rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>,<br>    rotation_range=<span class="hljs-number">40</span>,<br>    width_shift_range=<span class="hljs-number">0.2</span>,<br>    height_shift_range=<span class="hljs-number">0.2</span>,<br>    shear_range=<span class="hljs-number">0.2</span>,<br>    zoom_range=<span class="hljs-number">0.2</span>,<br>    horizontal_flip=<span class="hljs-literal">True</span>,)<br><br><span class="hljs-comment"># Note that the validation data should not be augmented!</span><br>test_datagen = ImageDataGenerator(rescale=<span class="hljs-number">1.</span>/<span class="hljs-number">255</span>)<br><br>train_generator = train_datagen.flow_from_directory(<br>        <span class="hljs-comment"># This is the target directory</span><br>        train_dir,<br>        <span class="hljs-comment"># All images will be resized to 150x150</span><br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        <span class="hljs-comment"># Since we use binary_crossentropy loss, we need binary labels</span><br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>validation_generator = test_datagen.flow_from_directory(<br>        validation_dir,<br>        target_size=(<span class="hljs-number">150</span>, <span class="hljs-number">150</span>),<br>        batch_size=<span class="hljs-number">32</span>,<br>        class_mode=<span class="hljs-string">&#x27;binary&#x27;</span>)<br><br>history = model.fit_generator(<br>      train_generator,<br>      steps_per_epoch=<span class="hljs-number">100</span>,<br>      epochs=<span class="hljs-number">100</span>,<br>      validation_data=validation_generator,<br>      validation_steps=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><pre><code>Found 2000 images belonging to 2 classes.Found 1000 images belonging to 2 classes.Epoch 1/100100/100 [==============================] - 24s - loss: 0.6857 - acc: 0.5447 - val_loss: 0.6620 - val_acc: 0.5888Epoch 2/100100/100 [==============================] - 23s - loss: 0.6710 - acc: 0.5675 - val_loss: 0.6606 - val_acc: 0.5825Epoch 3/100100/100 [==============================] - 22s - loss: 0.6609 - acc: 0.5913 - val_loss: 0.6663 - val_acc: 0.5711.594 - ETA: 7s - loss: 0.6655 - ETA: 5s - los - ETA: 1s - loss: 0.6620 - acc: Epoch 4/100100/100 [==============================] - 22s - loss: 0.6446 - acc: 0.6178 - val_loss: 0.6200 - val_acc: 0.6379Epoch 5/100100/100 [==============================] - 22s - loss: 0.6267 - acc: 0.6325 - val_loss: 0.6280 - val_acc: 0.5996Epoch 6/100100/100 [==============================] - 22s - loss: 0.6080 - acc: 0.6631 - val_loss: 0.6841 - val_acc: 0.5490Epoch 7/100100/100 [==============================] - 22s - loss: 0.5992 - acc: 0.6700 - val_loss: 0.5717 - val_acc: 0.6946Epoch 8/100100/100 [==============================] - 22s - loss: 0.5908 - acc: 0.6819 - val_loss: 0.5858 - val_acc: 0.6764Epoch 9/100100/100 [==============================] - 22s - loss: 0.5869 - acc: 0.6856 - val_loss: 0.5658 - val_acc: 0.6785Epoch 10/100100/100 [==============================] - 23s - loss: 0.5692 - acc: 0.6934 - val_loss: 0.5409 - val_acc: 0.7170Epoch 11/100100/100 [==============================] - 22s - loss: 0.5708 - acc: 0.6897 - val_loss: 0.5325 - val_acc: 0.7274Epoch 12/100100/100 [==============================] - 23s - loss: 0.5583 - acc: 0.7047 - val_loss: 0.5683 - val_acc: 0.7126Epoch 13/100100/100 [==============================] - 22s - loss: 0.5602 - acc: 0.7069 - val_loss: 0.6010 - val_acc: 0.6593Epoch 14/100100/100 [==============================] - 22s - loss: 0.5510 - acc: 0.7231 - val_loss: 0.5387 - val_acc: 0.7229Epoch 15/100100/100 [==============================] - 23s - loss: 0.5527 - acc: 0.7175 - val_loss: 0.5204 - val_acc: 0.7322Epoch 16/100100/100 [==============================] - 23s - loss: 0.5426 - acc: 0.7181 - val_loss: 0.5083 - val_acc: 0.7410Epoch 17/100100/100 [==============================] - 23s - loss: 0.5399 - acc: 0.7344 - val_loss: 0.5103 - val_acc: 0.7468Epoch 18/100100/100 [==============================] - 23s - loss: 0.5375 - acc: 0.7312 - val_loss: 0.5133 - val_acc: 0.7430Epoch 19/100100/100 [==============================] - 22s - loss: 0.5308 - acc: 0.7338 - val_loss: 0.4936 - val_acc: 0.7610Epoch 20/100100/100 [==============================] - 22s - loss: 0.5225 - acc: 0.7387 - val_loss: 0.4952 - val_acc: 0.7563Epoch 21/100100/100 [==============================] - 22s - loss: 0.5180 - acc: 0.7491 - val_loss: 0.4999 - val_acc: 0.7481Epoch 22/100100/100 [==============================] - 23s - loss: 0.5118 - acc: 0.7538 - val_loss: 0.4770 - val_acc: 0.7764Epoch 23/100100/100 [==============================] - 22s - loss: 0.5245 - acc: 0.7378 - val_loss: 0.4929 - val_acc: 0.7671Epoch 24/100100/100 [==============================] - 22s - loss: 0.5136 - acc: 0.7503 - val_loss: 0.4709 - val_acc: 0.7732Epoch 25/100100/100 [==============================] - 22s - loss: 0.4980 - acc: 0.7512 - val_loss: 0.4775 - val_acc: 0.7684Epoch 26/100100/100 [==============================] - 22s - loss: 0.4875 - acc: 0.7622 - val_loss: 0.4745 - val_acc: 0.7790Epoch 27/100100/100 [==============================] - 22s - loss: 0.5044 - acc: 0.7578 - val_loss: 0.5000 - val_acc: 0.7403Epoch 28/100100/100 [==============================] - 22s - loss: 0.4948 - acc: 0.7603 - val_loss: 0.4619 - val_acc: 0.7754Epoch 29/100100/100 [==============================] - 22s - loss: 0.4898 - acc: 0.7578 - val_loss: 0.4730 - val_acc: 0.7726Epoch 30/100100/100 [==============================] - 22s - loss: 0.4808 - acc: 0.7691 - val_loss: 0.4599 - val_acc: 0.7716Epoch 31/100100/100 [==============================] - 22s - loss: 0.4792 - acc: 0.7678 - val_loss: 0.4671 - val_acc: 0.7790Epoch 32/100100/100 [==============================] - 22s - loss: 0.4723 - acc: 0.7716 - val_loss: 0.4451 - val_acc: 0.7849Epoch 33/100100/100 [==============================] - 22s - loss: 0.4750 - acc: 0.7694 - val_loss: 0.4827 - val_acc: 0.7665Epoch 34/100100/100 [==============================] - 22s - loss: 0.4816 - acc: 0.7647 - val_loss: 0.4953 - val_acc: 0.7513Epoch 35/100100/100 [==============================] - 22s - loss: 0.4598 - acc: 0.7813 - val_loss: 0.4426 - val_acc: 0.7843Epoch 36/100100/100 [==============================] - 23s - loss: 0.4643 - acc: 0.7781 - val_loss: 0.4692 - val_acc: 0.7680Epoch 37/100100/100 [==============================] - 22s - loss: 0.4675 - acc: 0.7778 - val_loss: 0.4849 - val_acc: 0.7633Epoch 38/100100/100 [==============================] - 22s - loss: 0.4658 - acc: 0.7737 - val_loss: 0.4632 - val_acc: 0.7760Epoch 39/100100/100 [==============================] - 22s - loss: 0.4581 - acc: 0.7866 - val_loss: 0.4489 - val_acc: 0.7880Epoch 40/100100/100 [==============================] - 23s - loss: 0.4485 - acc: 0.7856 - val_loss: 0.4479 - val_acc: 0.7931Epoch 41/100100/100 [==============================] - 22s - loss: 0.4637 - acc: 0.7759 - val_loss: 0.4453 - val_acc: 0.7990Epoch 42/100100/100 [==============================] - 22s - loss: 0.4528 - acc: 0.7841 - val_loss: 0.4758 - val_acc: 0.7868Epoch 43/100100/100 [==============================] - 22s - loss: 0.4481 - acc: 0.7856 - val_loss: 0.4472 - val_acc: 0.7893Epoch 44/100100/100 [==============================] - 22s - loss: 0.4540 - acc: 0.7953 - val_loss: 0.4366 - val_acc: 0.7867A: 6s - loss: 0.4523 - acc: - ETA: Epoch 45/100100/100 [==============================] - 22s - loss: 0.4411 - acc: 0.7919 - val_loss: 0.4708 - val_acc: 0.7697Epoch 46/100100/100 [==============================] - 22s - loss: 0.4493 - acc: 0.7869 - val_loss: 0.4366 - val_acc: 0.7829Epoch 47/100100/100 [==============================] - 22s - loss: 0.4436 - acc: 0.7916 - val_loss: 0.4307 - val_acc: 0.8090Epoch 48/100100/100 [==============================] - 22s - loss: 0.4391 - acc: 0.7928 - val_loss: 0.4203 - val_acc: 0.8065Epoch 49/100100/100 [==============================] - 23s - loss: 0.4284 - acc: 0.8053 - val_loss: 0.4422 - val_acc: 0.8041Epoch 50/100100/100 [==============================] - 22s - loss: 0.4492 - acc: 0.7906 - val_loss: 0.5422 - val_acc: 0.7437Epoch 51/100100/100 [==============================] - 22s - loss: 0.4292 - acc: 0.7953 - val_loss: 0.4446 - val_acc: 0.7932Epoch 52/100100/100 [==============================] - 22s - loss: 0.4275 - acc: 0.8037 - val_loss: 0.4287 - val_acc: 0.7989Epoch 53/100100/100 [==============================] - 22s - loss: 0.4297 - acc: 0.7975 - val_loss: 0.4091 - val_acc: 0.8046Epoch 54/100100/100 [==============================] - 23s - loss: 0.4198 - acc: 0.7978 - val_loss: 0.4413 - val_acc: 0.7964Epoch 55/100100/100 [==============================] - 23s - loss: 0.4195 - acc: 0.8019 - val_loss: 0.4265 - val_acc: 0.8001Epoch 56/100100/100 [==============================] - 22s - loss: 0.4081 - acc: 0.8056 - val_loss: 0.4374 - val_acc: 0.7957Epoch 57/100100/100 [==============================] - 22s - loss: 0.4214 - acc: 0.8006 - val_loss: 0.4228 - val_acc: 0.8020Epoch 58/100100/100 [==============================] - 22s - loss: 0.4050 - acc: 0.8097 - val_loss: 0.4332 - val_acc: 0.7900Epoch 59/100100/100 [==============================] - 22s - loss: 0.4162 - acc: 0.8134 - val_loss: 0.4088 - val_acc: 0.8099Epoch 60/100100/100 [==============================] - 22s - loss: 0.4042 - acc: 0.8141 - val_loss: 0.4436 - val_acc: 0.7957Epoch 61/100100/100 [==============================] - 23s - loss: 0.4016 - acc: 0.8212 - val_loss: 0.4082 - val_acc: 0.8189Epoch 62/100100/100 [==============================] - 22s - loss: 0.4167 - acc: 0.8097 - val_loss: 0.3935 - val_acc: 0.8236Epoch 63/100100/100 [==============================] - 23s - loss: 0.4052 - acc: 0.8138 - val_loss: 0.4509 - val_acc: 0.7824Epoch 64/100100/100 [==============================] - 22s - loss: 0.4011 - acc: 0.8209 - val_loss: 0.3874 - val_acc: 0.8299Epoch 65/100100/100 [==============================] - 22s - loss: 0.3966 - acc: 0.8131 - val_loss: 0.4328 - val_acc: 0.7970Epoch 66/100100/100 [==============================] - 23s - loss: 0.3889 - acc: 0.8163 - val_loss: 0.4766 - val_acc: 0.7719Epoch 67/100100/100 [==============================] - 22s - loss: 0.3960 - acc: 0.8163 - val_loss: 0.3859 - val_acc: 0.8325Epoch 68/100100/100 [==============================] - 22s - loss: 0.3893 - acc: 0.8231 - val_loss: 0.4172 - val_acc: 0.8128Epoch 69/100100/100 [==============================] - 23s - loss: 0.3828 - acc: 0.8219 - val_loss: 0.4023 - val_acc: 0.8215 loss: 0.3881 - acc:Epoch 70/100100/100 [==============================] - 22s - loss: 0.3909 - acc: 0.8275 - val_loss: 0.4275 - val_acc: 0.8008Epoch 71/100100/100 [==============================] - 22s - loss: 0.3826 - acc: 0.8244 - val_loss: 0.3815 - val_acc: 0.8177Epoch 72/100100/100 [==============================] - 22s - loss: 0.3837 - acc: 0.8272 - val_loss: 0.4040 - val_acc: 0.8287Epoch 73/100100/100 [==============================] - 23s - loss: 0.3812 - acc: 0.8222 - val_loss: 0.4039 - val_acc: 0.8058Epoch 74/100100/100 [==============================] - 22s - loss: 0.3829 - acc: 0.8281 - val_loss: 0.4204 - val_acc: 0.8015Epoch 75/100100/100 [==============================] - 22s - loss: 0.3708 - acc: 0.8350 - val_loss: 0.4083 - val_acc: 0.8204Epoch 76/100100/100 [==============================] - 22s - loss: 0.3831 - acc: 0.8216 - val_loss: 0.3899 - val_acc: 0.8215Epoch 77/100100/100 [==============================] - 22s - loss: 0.3695 - acc: 0.8375 - val_loss: 0.3963 - val_acc: 0.8293Epoch 78/100100/100 [==============================] - 22s - loss: 0.3809 - acc: 0.8234 - val_loss: 0.4046 - val_acc: 0.8236Epoch 79/100100/100 [==============================] - 22s - loss: 0.3637 - acc: 0.8362 - val_loss: 0.3990 - val_acc: 0.8325Epoch 80/100100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8400 - val_loss: 0.3925 - val_acc: 0.8350Epoch 81/100100/100 [==============================] - 22s - loss: 0.3762 - acc: 0.8303 - val_loss: 0.3813 - val_acc: 0.8331Epoch 82/100100/100 [==============================] - 23s - loss: 0.3672 - acc: 0.8347 - val_loss: 0.4539 - val_acc: 0.7931Epoch 83/100100/100 [==============================] - 22s - loss: 0.3636 - acc: 0.8353 - val_loss: 0.3988 - val_acc: 0.8261Epoch 84/100100/100 [==============================] - 22s - loss: 0.3503 - acc: 0.8453 - val_loss: 0.3987 - val_acc: 0.8325Epoch 85/100100/100 [==============================] - 22s - loss: 0.3586 - acc: 0.8437 - val_loss: 0.3842 - val_acc: 0.8306Epoch 86/100100/100 [==============================] - 22s - loss: 0.3624 - acc: 0.8353 - val_loss: 0.4100 - val_acc: 0.8196.834Epoch 87/100100/100 [==============================] - 22s - loss: 0.3596 - acc: 0.8422 - val_loss: 0.3814 - val_acc: 0.8331Epoch 88/100100/100 [==============================] - 22s - loss: 0.3487 - acc: 0.8494 - val_loss: 0.4266 - val_acc: 0.8109Epoch 89/100100/100 [==============================] - 22s - loss: 0.3598 - acc: 0.8400 - val_loss: 0.4076 - val_acc: 0.8325Epoch 90/100100/100 [==============================] - 22s - loss: 0.3510 - acc: 0.8450 - val_loss: 0.3762 - val_acc: 0.8388Epoch 91/100100/100 [==============================] - 22s - loss: 0.3458 - acc: 0.8450 - val_loss: 0.4684 - val_acc: 0.8015Epoch 92/100100/100 [==============================] - 22s - loss: 0.3454 - acc: 0.8441 - val_loss: 0.4017 - val_acc: 0.8204Epoch 93/100100/100 [==============================] - 22s - loss: 0.3402 - acc: 0.8487 - val_loss: 0.3928 - val_acc: 0.8204Epoch 94/100100/100 [==============================] - 22s - loss: 0.3569 - acc: 0.8394 - val_loss: 0.4005 - val_acc: 0.8338Epoch 95/100100/100 [==============================] - 22s - loss: 0.3425 - acc: 0.8494 - val_loss: 0.3641 - val_acc: 0.8439Epoch 96/100100/100 [==============================] - 22s - loss: 0.3335 - acc: 0.8531 - val_loss: 0.3811 - val_acc: 0.8363Epoch 97/100100/100 [==============================] - 22s - loss: 0.3204 - acc: 0.8581 - val_loss: 0.3786 - val_acc: 0.8331Epoch 98/100100/100 [==============================] - 22s - loss: 0.3250 - acc: 0.8606 - val_loss: 0.4205 - val_acc: 0.8236Epoch 99/100100/100 [==============================] - 22s - loss: 0.3255 - acc: 0.8581 - val_loss: 0.3518 - val_acc: 0.8460Epoch 100/100100/100 [==============================] - 22s - loss: 0.3280 - acc: 0.8491 - val_loss: 0.3776 - val_acc: 0.8439</code></pre><p>Let’s save our model — we will be using it in the section on convnet visualization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save(<span class="hljs-string">&#x27;cats_and_dogs_small_2.h5&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Let’s plot our results again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(acc))<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.legend()<br><br>plt.figure()<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_43_0.png" alt="output"></p><p><img src="/img/output_43_1.png" alt="output"></p><p>Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation<br>curves. We are now able to reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.</p><p>By leveraging regularization techniques even further and by tuning the network’s parameters (such as the number of filters per convolution<br>layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86-87%. However, it would prove<br>very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a<br>next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two<br>sections.</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-6</title>
      <link href="2021/04/22/DL-with-python-6/"/>
      <url>2021/04/22/DL-with-python-6/</url>
      
        <content type="html"><![CDATA[<p>5.1-introduction-to-convnets</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="5-1-Introduction-to-convnets"><a href="#5-1-Introduction-to-convnets" class="headerlink" title="5.1 - Introduction to convnets"></a>5.1 - Introduction to convnets</h1><p>This notebook contains the code sample found in Chapter 5, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>First, let’s take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you’ve already been<br>through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its<br>accuracy will still blow out of the water that of the densely-connected model from Chapter 2.</p><p>The 6 lines of code below show you what a basic convnet looks like. It’s a stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers. We’ll see in a<br>minute what they do concretely.<br>Importantly, a convnet takes as input tensors of shape <code>(image_height, image_width, image_channels)</code> (not including the batch dimension).<br>In our case, we will configure our convnet to process inputs of size <code>(28, 28, 1)</code>, which is the format of MNIST images. We do this via<br>passing the argument <code>input_shape=(28, 28, 1)</code> to our first layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><br>model = models.Sequential()<br>model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))<br>model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Let’s display the architecture of our convnet so far:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     =================================================================Total params: 55,744Trainable params: 55,744Non-trainable params: 0_________________________________________________________________</code></pre><p>You can see above that the output of every <code>Conv2D</code> and <code>MaxPooling2D</code> layer is a 3D tensor of shape <code>(height, width, channels)</code>. The width<br>and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to<br>the <code>Conv2D</code> layers (e.g. 32 or 64).</p><p>The next step would be to feed our last output tensor (of shape <code>(3, 3, 64)</code>) into a densely-connected classifier network like those you are<br>already familiar with: a stack of <code>Dense</code> layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor.<br>So first, we will have to flatten our 3D outputs to 1D, and then add a few <code>Dense</code> layers on top:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(layers.Flatten())<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here’s what our network<br>looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.summary()<br></code></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     _________________________________________________________________flatten_1 (Flatten)          (None, 576)               0         _________________________________________________________________dense_1 (Dense)              (None, 64)                36928     _________________________________________________________________dense_2 (Dense)              (None, 10)                650       =================================================================Total params: 93,322Trainable params: 93,322Non-trainable params: 0_________________________________________________________________</code></pre><p>As you can see, our <code>(3, 3, 64)</code> outputs were flattened into vectors of shape <code>(576,)</code>, before going through two <code>Dense</code> layers.</p><p>Now, let’s train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter<br>2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br><br>train_images = train_images.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>test_images = test_images.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(train_images, train_labels, epochs=<span class="hljs-number">5</span>, batch_size=<span class="hljs-number">64</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/560000/60000 [==============================] - 8s - loss: 0.1766 - acc: 0.9440     Epoch 2/560000/60000 [==============================] - 7s - loss: 0.0462 - acc: 0.9855     Epoch 3/560000/60000 [==============================] - 7s - loss: 0.0322 - acc: 0.9902     Epoch 4/560000/60000 [==============================] - 7s - loss: 0.0241 - acc: 0.9926     Epoch 5/560000/60000 [==============================] - 7s - loss: 0.0187 - acc: 0.9943     &lt;keras.callbacks.History at 0x7fbd9c4cd828&gt;</code></pre><p>Let’s evaluate the model on the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loss, test_acc = model.evaluate(test_images, test_labels)<br></code></pre></td></tr></table></figure><pre><code> 9536/10000 [===========================&gt;..] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_acc<br></code></pre></td></tr></table></figure><pre><code>0.99129999999999996</code></pre><p>While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we<br>decreased our error rate by 68% (relative). Not bad! </p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-5</title>
      <link href="2021/04/22/DL-with-python-5/"/>
      <url>2021/04/22/DL-with-python-5/</url>
      
        <content type="html"><![CDATA[<p>4.4-overfitting-and-underfitting</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Overfitting-and-underfitting"><a href="#Overfitting-and-underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In all the examples we saw in the previous chapter — movie review sentiment prediction, topic classification, and house price regression —<br>we could notice that the performance of our model on the held-out validation data would always peak after a few epochs and would then start<br>degrading, i.e. our model would quickly start to <em>overfit</em> to the training data. Overfitting happens in every single machine learning<br>problem. Learning how to deal with overfitting is essential to mastering machine learning.</p><p>The fundamental issue in machine learning is the tension between optimization and generalization. “Optimization” refers to the process of<br>adjusting a model to get the best performance possible on the training data (the “learning” in “machine learning”), while “generalization”<br>refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of<br>course, but you do not control generalization; you can only adjust the model based on its training data.</p><p>At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on<br>test data. While this is happening, your model is said to be <em>under-fit</em>: there is still progress to be made; the network hasn’t yet<br>modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops<br>improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns<br>that are specific to the training data but that are misleading or irrelevant when it comes to new data.</p><p>To prevent a model from learning misleading or irrelevant patterns found in the training data, <em>the best solution is of course to get<br>more training data</em>. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution<br>is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to<br>store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most<br>prominent patterns, which have a better chance of generalizing well.</p><p>The processing of fighting overfitting in this way is called <em>regularization</em>. Let’s review some of the most common regularization<br>techniques, and let’s apply them in practice to improve our movie classification model from  the previous chapter.</p><p>Note: in this notebook we will be using the IMDB test set as our validation set. It doesn’t matter in this context.</p><p>Let’s prepare the data using the code from Chapter 3, Section 5:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="Fighting-overfitting"><a href="#Fighting-overfitting" class="headerlink" title="Fighting overfitting"></a>Fighting overfitting</h1><h2 id="Reducing-the-network’s-size"><a href="#Reducing-the-network’s-size" class="headerlink" title="Reducing the network’s size"></a>Reducing the network’s size</h2><p>The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is<br>determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is<br>often referred to as the model’s “capacity”. Intuitively, a model with more parameters will have more “memorization capacity” and therefore<br>will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any<br>generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the<br>MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying<br>new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge<br>is generalization, not fitting.</p><p>On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in<br>order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets<br>— precisely the type of representations that we are interested in. At the same time, keep in mind that you should be using models that have<br>enough parameters that they won’t be underfitting: your model shouldn’t be starved for memorization resources. There is a compromise to be<br>found between “too much capacity” and “not enough capacity”.</p><p>Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You<br>will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the<br>right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and<br>parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the<br>validation loss.</p><p>Let’s try this on our movie review classification network. Our original network was as such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>original_model = models.Sequential()<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>original_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>original_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>original_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                       loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                       metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Now let’s try to replace it with this smaller network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model = models.Sequential()<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>smaller_model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>smaller_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>smaller_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                      loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                      metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Here’s a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of<br>the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">original_hist = original_model.fit(x_train, y_train,<br>                                   epochs=<span class="hljs-number">20</span>,<br>                                   batch_size=<span class="hljs-number">512</span>,<br>                                   validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4594 - acc: 0.8188 - val_loss: 0.3429 - val_acc: 0.8812Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.2659 - acc: 0.9075 - val_loss: 0.2873 - val_acc: 0.8905Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2060 - acc: 0.9276 - val_loss: 0.2827 - val_acc: 0.8879Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.1697 - acc: 0.9401 - val_loss: 0.2921 - val_acc: 0.8851Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.1495 - acc: 0.9470 - val_loss: 0.3100 - val_acc: 0.8812Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1283 - acc: 0.9572 - val_loss: 0.3336 - val_acc: 0.8748Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1121 - acc: 0.9624 - val_loss: 0.3987 - val_acc: 0.8593Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.0994 - acc: 0.9670 - val_loss: 0.3788 - val_acc: 0.8702Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.0889 - acc: 0.9716 - val_loss: 0.4242 - val_acc: 0.8603Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.0782 - acc: 0.9757 - val_loss: 0.4256 - val_acc: 0.8653Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.0691 - acc: 0.9792 - val_loss: 0.4515 - val_acc: 0.8638Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.0603 - acc: 0.9820 - val_loss: 0.5102 - val_acc: 0.8610Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.0518 - acc: 0.9851 - val_loss: 0.5281 - val_acc: 0.8587Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0446 - acc: 0.9873 - val_loss: 0.5441 - val_acc: 0.8589Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0367 - acc: 0.9903 - val_loss: 0.5777 - val_acc: 0.8574Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0313 - acc: 0.9922 - val_loss: 0.6377 - val_acc: 0.8555Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0247 - acc: 0.9941 - val_loss: 0.7269 - val_acc: 0.8501Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0203 - acc: 0.9956 - val_loss: 0.6920 - val_acc: 0.8516Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0156 - acc: 0.9970 - val_loss: 0.7689 - val_acc: 0.8425Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0144 - acc: 0.9966 - val_loss: 0.7694 - val_acc: 0.8487</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">smaller_model_hist = smaller_model.fit(x_train, y_train,<br>                                       epochs=<span class="hljs-number">20</span>,<br>                                       batch_size=<span class="hljs-number">512</span>,<br>                                       validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 2s - loss: 0.5737 - acc: 0.8049 - val_loss: 0.4826 - val_acc: 0.8616Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3973 - acc: 0.8866 - val_loss: 0.3699 - val_acc: 0.8776Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2985 - acc: 0.9054 - val_loss: 0.3140 - val_acc: 0.8860Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2428 - acc: 0.9189 - val_loss: 0.2913 - val_acc: 0.8870Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2085 - acc: 0.9290 - val_loss: 0.2809 - val_acc: 0.8897Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.1849 - acc: 0.9360 - val_loss: 0.2772 - val_acc: 0.8899Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.1666 - acc: 0.9430 - val_loss: 0.2835 - val_acc: 0.8863Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1515 - acc: 0.9487 - val_loss: 0.2909 - val_acc: 0.8850Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1388 - acc: 0.9526 - val_loss: 0.2984 - val_acc: 0.8842Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1285 - acc: 0.9569 - val_loss: 0.3102 - val_acc: 0.8818Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1194 - acc: 0.9599 - val_loss: 0.3219 - val_acc: 0.8794Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1105 - acc: 0.9648 - val_loss: 0.3379 - val_acc: 0.8774Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1035 - acc: 0.9674 - val_loss: 0.3532 - val_acc: 0.8730Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.0963 - acc: 0.9688 - val_loss: 0.3651 - val_acc: 0.8731Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.0895 - acc: 0.9724 - val_loss: 0.3858 - val_acc: 0.8703Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.0838 - acc: 0.9734 - val_loss: 0.4157 - val_acc: 0.8654Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.0785 - acc: 0.9764 - val_loss: 0.4214 - val_acc: 0.8677Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.0733 - acc: 0.9784 - val_loss: 0.4390 - val_acc: 0.8644Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.0685 - acc: 0.9796 - val_loss: 0.4539 - val_acc: 0.8638Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.0638 - acc: 0.9822 - val_loss: 0.4744 - val_acc: 0.8617</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>)<br>original_val_loss = original_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br>smaller_model_val_loss = smaller_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># b+ is for &quot;blue cross&quot;</span><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, smaller_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Smaller model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_12_0.png" alt="output"></p><p>As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance<br>degrades much more slowly once it starts overfitting.</p><p>Now, for kicks, let’s add to this benchmark a network that has much more capacity, far more than the problem would warrant:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model = models.Sequential()<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>bigger_model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>bigger_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>bigger_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                     loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                     metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_hist = bigger_model.fit(x_train, y_train,<br>                                     epochs=<span class="hljs-number">20</span>,<br>                                     batch_size=<span class="hljs-number">512</span>,<br>                                     validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4539 - acc: 0.8011 - val_loss: 0.4150 - val_acc: 0.8229Epoch 2/2025000/25000 [==============================] - 3s - loss: 0.2148 - acc: 0.9151 - val_loss: 0.2742 - val_acc: 0.8901Epoch 3/2025000/25000 [==============================] - 3s - loss: 0.1217 - acc: 0.9544 - val_loss: 0.5442 - val_acc: 0.7975Epoch 4/2025000/25000 [==============================] - 3s - loss: 0.0552 - acc: 0.9835 - val_loss: 0.4316 - val_acc: 0.8842Epoch 5/2025000/25000 [==============================] - 3s - loss: 0.0662 - acc: 0.9888 - val_loss: 0.5098 - val_acc: 0.8822Epoch 6/2025000/25000 [==============================] - 3s - loss: 0.0017 - acc: 0.9998 - val_loss: 0.6867 - val_acc: 0.8811Epoch 7/2025000/25000 [==============================] - 3s - loss: 0.1019 - acc: 0.9882 - val_loss: 0.6737 - val_acc: 0.8800Epoch 8/2025000/25000 [==============================] - 3s - loss: 0.0735 - acc: 0.9896 - val_loss: 0.6185 - val_acc: 0.8772Epoch 9/2025000/25000 [==============================] - 3s - loss: 3.4759e-04 - acc: 1.0000 - val_loss: 0.7328 - val_acc: 0.8818Epoch 10/2025000/25000 [==============================] - 3s - loss: 0.0504 - acc: 0.9912 - val_loss: 0.7092 - val_acc: 0.8791Epoch 11/2025000/25000 [==============================] - 3s - loss: 0.0589 - acc: 0.9919 - val_loss: 0.6831 - val_acc: 0.8785Epoch 12/2025000/25000 [==============================] - 3s - loss: 1.9067e-04 - acc: 1.0000 - val_loss: 0.8005 - val_acc: 0.8784Epoch 13/2025000/25000 [==============================] - 3s - loss: 0.0623 - acc: 0.9916 - val_loss: 0.7540 - val_acc: 0.8740Epoch 14/2025000/25000 [==============================] - 3s - loss: 0.0274 - acc: 0.9954 - val_loss: 0.7806 - val_acc: 0.8670Epoch 15/2025000/25000 [==============================] - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.8107 - val_acc: 0.8783Epoch 16/2025000/25000 [==============================] - 3s - loss: 0.0445 - acc: 0.9943 - val_loss: 0.8394 - val_acc: 0.8702Epoch 17/2025000/25000 [==============================] - 3s - loss: 0.0268 - acc: 0.9959 - val_loss: 0.7708 - val_acc: 0.8745Epoch 18/2025000/25000 [==============================] - 3s - loss: 7.7057e-04 - acc: 1.0000 - val_loss: 0.8885 - val_acc: 0.8738Epoch 19/2025000/25000 [==============================] - 3s - loss: 0.0297 - acc: 0.9962 - val_loss: 0.8419 - val_acc: 0.8728Epoch 20/2025000/25000 [==============================] - 3s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.8896 - val_acc: 0.8682</code></pre><p>Here’s how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the<br>crosses are the initial network.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">bigger_model_val_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_17_0.png" alt="output"></p><p>The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also<br>more noisy.</p><p>Meanwhile, here are the training losses for our two networks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">original_train_loss = original_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>bigger_model_train_loss = bigger_model_hist.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br><br>plt.plot(epochs, original_train_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, bigger_model_train_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Bigger model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_19_0.png" alt="output"></p><p>As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be<br>able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large<br>difference between the training and validation loss).</p><h2 id="Adding-weight-regularization"><a href="#Adding-weight-regularization" class="headerlink" title="Adding weight regularization"></a>Adding weight regularization</h2><p>You may be familiar with <em>Occam’s Razor</em> principle: given two explanations for something, the explanation most likely to be correct is the<br>“simplest” one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some<br>training data and a network architecture, there are multiple sets of weights values (multiple <em>models</em>) that could explain the data, and<br>simpler models are less likely to overfit than complex ones.</p><p>A “simple model” in this context is a model where the distribution of parameter values has less entropy (or a model with fewer<br>parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity<br>of a network by forcing its weights to only take small values, which makes the distribution of weight values more “regular”. This is called<br>“weight regularization”, and it is done by adding to the loss function of the network a <em>cost</em> associated with having large weights. This<br>cost comes in two flavors:</p><ul><li>L1 regularization, where the cost added is proportional to the <em>absolute value of the weights coefficients</em> (i.e. to what is called the<br>“L1 norm” of the weights).</li><li>L2 regularization, where the cost added is proportional to the <em>square of the value of the weights coefficients</em> (i.e. to what is called<br>the “L2 norm” of the weights). L2 regularization is also called <em>weight decay</em> in the context of neural networks. Don’t let the different<br>name confuse you: weight decay is mathematically the exact same as L2 regularization.</li></ul><p>In Keras, weight regularization is added by passing <em>weight regularizer instances</em> to layers as keyword arguments. Let’s add L2 weight<br>regularization to our movie review classification network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br>l2_model = models.Sequential()<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>l2_model.add(layers.Dense(<span class="hljs-number">16</span>, kernel_regularizer=regularizers.l2(<span class="hljs-number">0.001</span>),<br>                          activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>l2_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                 loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                 metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p><code>l2(0.001)</code> means that every coefficient in the weight matrix of the layer will add <code>0.001 * weight_coefficient_value</code> to the total loss of<br>the network. Note that because this penalty is <em>only added at training time</em>, the loss for this network will be much higher at training<br>than at test time.</p><p>Here’s the impact of our L2 regularization penalty:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_hist = l2_model.fit(x_train, y_train,<br>                             epochs=<span class="hljs-number">20</span>,<br>                             batch_size=<span class="hljs-number">512</span>,<br>                             validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.4880 - acc: 0.8218 - val_loss: 0.3820 - val_acc: 0.8798Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.9068 - val_loss: 0.3353 - val_acc: 0.8896Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.2742 - acc: 0.9185 - val_loss: 0.3306 - val_acc: 0.8898Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.2489 - acc: 0.9288 - val_loss: 0.3363 - val_acc: 0.8866Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2420 - acc: 0.9318 - val_loss: 0.3492 - val_acc: 0.8820Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2322 - acc: 0.9359 - val_loss: 0.3567 - val_acc: 0.8788Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2254 - acc: 0.9385 - val_loss: 0.3632 - val_acc: 0.8787Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.2219 - acc: 0.9380 - val_loss: 0.3630 - val_acc: 0.8794Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.2162 - acc: 0.9430 - val_loss: 0.3704 - val_acc: 0.8763Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.2144 - acc: 0.9428 - val_loss: 0.3876 - val_acc: 0.8727Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.2091 - acc: 0.9439 - val_loss: 0.3883 - val_acc: 0.8724Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.2061 - acc: 0.9455 - val_loss: 0.3870 - val_acc: 0.8740Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.2069 - acc: 0.9445 - val_loss: 0.4073 - val_acc: 0.8714Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.2028 - acc: 0.9475 - val_loss: 0.3976 - val_acc: 0.8714Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1998 - acc: 0.9472 - val_loss: 0.4362 - val_acc: 0.8670Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.2019 - acc: 0.9462 - val_loss: 0.4088 - val_acc: 0.8711Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1953 - acc: 0.9495 - val_loss: 0.4185 - val_acc: 0.8698Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1945 - acc: 0.9508 - val_loss: 0.4371 - val_acc: 0.8674Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1934 - acc: 0.9486 - val_loss: 0.4136 - val_acc: 0.8699Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1924 - acc: 0.9504 - val_loss: 0.4200 - val_acc: 0.8704</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">l2_model_val_loss = l2_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, l2_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;L2-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0p.png" alt="output"></p><p>As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses),<br>even though both models have the same number of parameters.</p><p>As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> regularizers<br><br><span class="hljs-comment"># L1 regularization</span><br>regularizers.l1(<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># L1 and L2 regularization at the same time</span><br>regularizers.l1_l2(l1=<span class="hljs-number">0.001</span>, l2=<span class="hljs-number">0.001</span>)<br></code></pre></td></tr></table></figure><h2 id="Adding-dropout"><a href="#Adding-dropout" class="headerlink" title="Adding dropout"></a>Adding dropout</h2><p>Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his<br>students at the University of Toronto. Dropout, applied to a layer, consists of randomly “dropping out” (i.e. setting to zero) a number of<br>output features of the layer during training. Let’s say a given layer would normally have returned a vector <code>[0.2, 0.5, 1.3, 0.8, 1.1]</code> for a<br>given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. <code>[0, 0.5, 1.3, 0, 1.1]</code>. The “dropout rate” is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test<br>time, no units are dropped out, and instead the layer’s output values are scaled down by a factor equal to the dropout rate, so as to<br>balance for the fact that more units are active than at training time.</p><p>Consider a Numpy matrix containing the output of a layer, <code>layer_output</code>, of shape <code>(batch_size, features)</code>. At training time, we would be<br>zero-ing out at random a fraction of the values in the matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time: we drop out 50% of the units in the output</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br></code></pre></td></tr></table></figure><p>At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the<br>units):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At test time:</span><br>layer_output *= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is<br>often the way it is implemented in practice:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># At training time:</span><br>layer_output *= np.randint(<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=layer_output.shape)<br><span class="hljs-comment"># Note that we are scaling *up* rather scaling *down* in this case</span><br>layer_output /= <span class="hljs-number">0.5</span><br></code></pre></td></tr></table></figure><p>This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among<br>other things, by a fraud prevention mechanism used by banks — in his own words: <em>“I went to my bank. The tellers kept changing and I asked<br>one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation<br>between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each<br>example would prevent conspiracies and thus reduce overfitting”</em>.</p><p>The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what<br>Hinton refers to as “conspiracies”), which the network would start memorizing if no noise was present. </p><p>In Keras you can introduce dropout in a network via the <code>Dropout</code> layer, which gets applied to the output of layer right before it, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br></code></pre></td></tr></table></figure><p>Let’s add two <code>Dropout</code> layers in our IMDB network to see how well they do at reducing overfitting:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model = models.Sequential()<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>dpt_model.add(layers.Dropout(<span class="hljs-number">0.5</span>))<br>dpt_model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>dpt_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                  loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>                  metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_hist = dpt_model.fit(x_train, y_train,<br>                               epochs=<span class="hljs-number">20</span>,<br>                               batch_size=<span class="hljs-number">512</span>,<br>                               validation_data=(x_test, y_test))<br></code></pre></td></tr></table></figure><pre><code>Train on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 3s - loss: 0.6035 - acc: 0.6678 - val_loss: 0.4704 - val_acc: 0.8651Epoch 2/2025000/25000 [==============================] - 2s - loss: 0.4622 - acc: 0.8002 - val_loss: 0.3612 - val_acc: 0.8724Epoch 3/2025000/25000 [==============================] - 2s - loss: 0.3731 - acc: 0.8553 - val_loss: 0.2960 - val_acc: 0.8904Epoch 4/2025000/25000 [==============================] - 2s - loss: 0.3162 - acc: 0.8855 - val_loss: 0.2772 - val_acc: 0.8917Epoch 5/2025000/25000 [==============================] - 2s - loss: 0.2762 - acc: 0.9033 - val_loss: 0.2803 - val_acc: 0.8889Epoch 6/2025000/25000 [==============================] - 2s - loss: 0.2454 - acc: 0.9172 - val_loss: 0.2823 - val_acc: 0.8892Epoch 7/2025000/25000 [==============================] - 2s - loss: 0.2178 - acc: 0.9281 - val_loss: 0.2982 - val_acc: 0.8877Epoch 8/2025000/25000 [==============================] - 2s - loss: 0.1994 - acc: 0.9351 - val_loss: 0.3101 - val_acc: 0.8875Epoch 9/2025000/25000 [==============================] - 2s - loss: 0.1832 - acc: 0.9400 - val_loss: 0.3318 - val_acc: 0.8860Epoch 10/2025000/25000 [==============================] - 2s - loss: 0.1692 - acc: 0.9434 - val_loss: 0.3534 - val_acc: 0.8841Epoch 11/2025000/25000 [==============================] - 2s - loss: 0.1590 - acc: 0.9483 - val_loss: 0.3689 - val_acc: 0.8830Epoch 12/2025000/25000 [==============================] - 2s - loss: 0.1499 - acc: 0.9496 - val_loss: 0.4107 - val_acc: 0.8776Epoch 13/2025000/25000 [==============================] - 2s - loss: 0.1405 - acc: 0.9539 - val_loss: 0.4114 - val_acc: 0.8782Epoch 14/2025000/25000 [==============================] - 2s - loss: 0.1333 - acc: 0.9562 - val_loss: 0.4549 - val_acc: 0.8771Epoch 15/2025000/25000 [==============================] - 2s - loss: 0.1267 - acc: 0.9572 - val_loss: 0.4579 - val_acc: 0.8800Epoch 16/2025000/25000 [==============================] - 2s - loss: 0.1225 - acc: 0.9580 - val_loss: 0.4843 - val_acc: 0.8772Epoch 17/2025000/25000 [==============================] - 2s - loss: 0.1233 - acc: 0.9590 - val_loss: 0.4783 - val_acc: 0.8761Epoch 18/2025000/25000 [==============================] - 2s - loss: 0.1212 - acc: 0.9601 - val_loss: 0.5051 - val_acc: 0.8740Epoch 19/2025000/25000 [==============================] - 2s - loss: 0.1153 - acc: 0.9618 - val_loss: 0.5451 - val_acc: 0.8747Epoch 20/2025000/25000 [==============================] - 2s - loss: 0.1155 - acc: 0.9621 - val_loss: 0.5358 - val_acc: 0.8738</code></pre><p>Let’s plot the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">dpt_model_val_loss = dpt_model_hist.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>plt.plot(epochs, original_val_loss, <span class="hljs-string">&#x27;b+&#x27;</span>, label=<span class="hljs-string">&#x27;Original model&#x27;</span>)<br>plt.plot(epochs, dpt_model_val_loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Dropout-regularized model&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_41_0.png" alt="output"></p><p>Again, a clear improvement over the reference network.</p><p>To recap: here the most common ways to prevent overfitting in neural networks:</p><ul><li>Getting more training data.</li><li>Reducing the capacity of the network.</li><li>Adding weight regularization.</li><li>Adding dropout.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-4</title>
      <link href="2021/04/22/DL-with-python-4/"/>
      <url>2021/04/22/DL-with-python-4/</url>
      
        <content type="html"><![CDATA[<p>3.7-predicting-house-prices</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Predicting-house-prices-a-regression-example"><a href="#Predicting-house-prices-a-regression-example" class="headerlink" title="Predicting house prices: a regression example"></a>Predicting house prices: a regression example</h1><p>This notebook contains the code samples found in Chapter 3, Section 6 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In our two previous examples, we were considering classification problems, where the goal was to predict a single discrete label of an<br>input data point. Another common type of machine learning problem is “regression”, which consists of predicting a continuous value instead<br>of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a<br>software project will take to complete, given its specifications.</p><p>Do not mix up “regression” with the algorithm “logistic regression”: confusingly, “logistic regression” is not a regression algorithm,<br>it is a classification algorithm.</p><h2 id="The-Boston-Housing-Price-dataset"><a href="#The-Boston-Housing-Price-dataset" class="headerlink" title="The Boston Housing Price dataset"></a>The Boston Housing Price dataset</h2><p>We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the<br>suburb at the time, such as the crime rate, the local property tax rate, etc.</p><p>The dataset we will be using has another interesting difference from our two previous examples: it has very few data points, only 506 in<br>total, split between 404 training samples and 102 test samples, and each “feature” in the input data (e.g. the crime rate is a feature) has<br>a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12,<br>others between 0 and 100…</p><p>Let’s take a look at the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> boston_housing<br><br>(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data.shape<br></code></pre></td></tr></table></figure><pre><code>(404, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data.shape<br></code></pre></td></tr></table></figure><pre><code>(102, 13)</code></pre><p>As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as<br>follow:</p><ol><li>Per capita crime rate.</li><li>Proportion of residential land zoned for lots over 25,000 square feet.</li><li>Proportion of non-retail business acres per town.</li><li>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</li><li>Nitric oxides concentration (parts per 10 million).</li><li>Average number of rooms per dwelling.</li><li>Proportion of owner-occupied units built prior to 1940.</li><li>Weighted distances to five Boston employment centres.</li><li>Index of accessibility to radial highways.</li><li>Full-value property-tax rate per $10,000.</li><li>Pupil-teacher ratio by town.</li><li>1000 <em> (Bk - 0.63) *</em> 2 where Bk is the proportion of Black people by town.</li><li>% lower status of the population.</li></ol><p>The targets are the median values of owner-occupied homes, in thousands of dollars:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_targets<br></code></pre></td></tr></table></figure><pre><code>array([ 15.2,  42.3,  50. ,  21.1,  17.7,  18.5,  11.3,  15.6,  15.6,        14.4,  12.1,  17.9,  23.1,  19.9,  15.7,   8.8,  50. ,  22.5,        24.1,  27.5,  10.9,  30.8,  32.9,  24. ,  18.5,  13.3,  22.9,        34.7,  16.6,  17.5,  22.3,  16.1,  14.9,  23.1,  34.9,  25. ,        13.9,  13.1,  20.4,  20. ,  15.2,  24.7,  22.2,  16.7,  12.7,        15.6,  18.4,  21. ,  30.1,  15.1,  18.7,   9.6,  31.5,  24.8,        19.1,  22. ,  14.5,  11. ,  32. ,  29.4,  20.3,  24.4,  14.6,        19.5,  14.1,  14.3,  15.6,  10.5,   6.3,  19.3,  19.3,  13.4,        36.4,  17.8,  13.5,  16.5,   8.3,  14.3,  16. ,  13.4,  28.6,        43.5,  20.2,  22. ,  23. ,  20.7,  12.5,  48.5,  14.6,  13.4,        23.7,  50. ,  21.7,  39.8,  38.7,  22.2,  34.9,  22.5,  31.1,        28.7,  46. ,  41.7,  21. ,  26.6,  15. ,  24.4,  13.3,  21.2,        11.7,  21.7,  19.4,  50. ,  22.8,  19.7,  24.7,  36.2,  14.2,        18.9,  18.3,  20.6,  24.6,  18.2,   8.7,  44. ,  10.4,  13.2,        21.2,  37. ,  30.7,  22.9,  20. ,  19.3,  31.7,  32. ,  23.1,        18.8,  10.9,  50. ,  19.6,   5. ,  14.4,  19.8,  13.8,  19.6,        23.9,  24.5,  25. ,  19.9,  17.2,  24.6,  13.5,  26.6,  21.4,        11.9,  22.6,  19.6,   8.5,  23.7,  23.1,  22.4,  20.5,  23.6,        18.4,  35.2,  23.1,  27.9,  20.6,  23.7,  28. ,  13.6,  27.1,        23.6,  20.6,  18.2,  21.7,  17.1,   8.4,  25.3,  13.8,  22.2,        18.4,  20.7,  31.6,  30.5,  20.3,   8.8,  19.2,  19.4,  23.1,        23. ,  14.8,  48.8,  22.6,  33.4,  21.1,  13.6,  32.2,  13.1,        23.4,  18.9,  23.9,  11.8,  23.3,  22.8,  19.6,  16.7,  13.4,        22.2,  20.4,  21.8,  26.4,  14.9,  24.1,  23.8,  12.3,  29.1,        21. ,  19.5,  23.3,  23.8,  17.8,  11.5,  21.7,  19.9,  25. ,        33.4,  28.5,  21.4,  24.3,  27.5,  33.1,  16.2,  23.3,  48.3,        22.9,  22.8,  13.1,  12.7,  22.6,  15. ,  15.3,  10.5,  24. ,        18.5,  21.7,  19.5,  33.2,  23.2,   5. ,  19.1,  12.7,  22.3,        10.2,  13.9,  16.3,  17. ,  20.1,  29.9,  17.2,  37.3,  45.4,        17.8,  23.2,  29. ,  22. ,  18. ,  17.4,  34.6,  20.1,  25. ,        15.6,  24.8,  28.2,  21.2,  21.4,  23.8,  31. ,  26.2,  17.4,        37.9,  17.5,  20. ,   8.3,  23.9,   8.4,  13.8,   7.2,  11.7,        17.1,  21.6,  50. ,  16.1,  20.4,  20.6,  21.4,  20.6,  36.5,         8.5,  24.8,  10.8,  21.9,  17.3,  18.9,  36.2,  14.9,  18.2,        33.3,  21.8,  19.7,  31.6,  24.8,  19.4,  22.8,   7.5,  44.8,        16.8,  18.7,  50. ,  50. ,  19.5,  20.1,  50. ,  17.2,  20.8,        19.3,  41.3,  20.4,  20.5,  13.8,  16.5,  23.9,  20.6,  31.5,        23.3,  16.8,  14. ,  33.8,  36.1,  12.8,  18.3,  18.7,  19.1,        29. ,  30.1,  50. ,  50. ,  22. ,  11.9,  37.6,  50. ,  22.7,        20.8,  23.5,  27.9,  50. ,  19.3,  23.9,  22.6,  15.2,  21.7,        19.2,  43.8,  20.3,  33.2,  19.9,  22.5,  32.7,  22. ,  17.1,        19. ,  15. ,  16.1,  25.1,  23.7,  28.7,  37.2,  22.6,  16.4,        25. ,  29.8,  22.1,  17.4,  18.1,  30.3,  17.5,  24.7,  12.6,        26.5,  28.7,  13.3,  10.4,  24.4,  23. ,  20. ,  17.8,   7. ,        11.8,  24.4,  13.8,  19.4,  25.2,  19.4,  19.4,  29.1])</code></pre><p>The prices are typically between $10,000 and $50,000. If that sounds cheap, remember this was the mid-1970s, and these prices are not<br>inflation-adjusted.</p><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to<br>automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal<br>with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we<br>will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a<br>unit standard deviation. This is easily done in Numpy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">mean = train_data.mean(axis=<span class="hljs-number">0</span>)<br>train_data -= mean<br>std = train_data.std(axis=<span class="hljs-number">0</span>)<br>train_data /= std<br><br>test_data -= mean<br>test_data /= std<br></code></pre></td></tr></table></figure><p>Note that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our<br>workflow any quantity computed on the test data, even for something as simple as data normalization.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Because so few samples are available, we will be using a very small network with two<br>hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using<br>a small network is one way to mitigate overfitting.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>():</span><br>    <span class="hljs-comment"># Because we will need to instantiate</span><br>    <span class="hljs-comment"># the same model multiple times,</span><br>    <span class="hljs-comment"># we use a function to construct it.</span><br>    model = models.Sequential()<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>,<br>                           input_shape=(train_data.shape[<span class="hljs-number">1</span>],)))<br>    model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>    model.add(layers.Dense(<span class="hljs-number">1</span>))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;mse&#x27;</span>, metrics=[<span class="hljs-string">&#x27;mae&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>Our network ends with a single unit, and no activation (i.e. it will be linear layer).<br>This is a typical setup for scalar regression (i.e. regression where we are trying to predict a single continuous value).<br>Applying an activation function would constrain the range that the output can take; for instance if<br>we applied a <code>sigmoid</code> activation function to our last layer, the network could only learn to predict values between 0 and 1. Here, because<br>the last layer is purely linear, the network is free to learn to predict values in any range.</p><p>Note that we are compiling the network with the <code>mse</code> loss function — Mean Squared Error, the square of the difference between the<br>predictions and the targets, a widely used loss function for regression problems.</p><p>We are also monitoring a new metric during training: <code>mae</code>. This stands for Mean Absolute Error. It is simply the absolute value of the<br>difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by<br>$500 on average.</p><h2 id="Validating-our-approach-using-K-fold-validation"><a href="#Validating-our-approach-using-K-fold-validation" class="headerlink" title="Validating our approach using K-fold validation"></a>Validating our approach using K-fold validation</h2><p>To evaluate our network while we keep adjusting its parameters (such as the number of epochs used for training), we could simply split the<br>data into a training set and a validation set, as we were doing in our previous examples. However, because we have so few data points, the<br>validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot<br>depending on <em>which</em> data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a<br>high <em>variance</em> with regard to the validation split. This would prevent us from reliably evaluating our model.</p><p>The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions<br>(typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining<br>partition. The validation score for the model used would then be the average of the K validation scores obtained.</p><p>In terms of code, this is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>k = <span class="hljs-number">4</span><br>num_val_samples = <span class="hljs-built_in">len</span>(train_data) // k<br>num_epochs = <span class="hljs-number">100</span><br>all_scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    model.fit(partial_train_data, partial_train_targets,<br>              epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Evaluate the model on the validation data</span><br>    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="hljs-number">0</span>)<br>    all_scores.append(val_mae)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">all_scores<br></code></pre></td></tr></table></figure><pre><code>[2.0750808349930412, 2.117215852926273, 2.9140411863232605, 2.4288365227161068]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.mean(all_scores)<br></code></pre></td></tr></table></figure><pre><code>2.3837935992396706</code></pre><p>As you can notice, the different runs do indeed show rather different validation scores, from 2.1 to 2.9. Their average (2.4) is a much more<br>reliable metric than any single of these scores — that’s the entire point of K-fold cross-validation. In this case, we are off by $2,400 on<br>average, which is still significant considering that the prices range from $10,000 to $50,000. </p><p>Let’s try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop<br>to save the per-epoch validation score log:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K<br><br><span class="hljs-comment"># Some memory clean-up</span><br>K.clear_session()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">500</span><br>all_mae_histories = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;processing fold #&#x27;</span>, i)<br>    <span class="hljs-comment"># Prepare the validation data: data from partition # k</span><br>    val_data = train_data[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br>    val_targets = train_targets[i * num_val_samples: (i + <span class="hljs-number">1</span>) * num_val_samples]<br><br>    <span class="hljs-comment"># Prepare the training data: data from all other partitions</span><br>    partial_train_data = np.concatenate(<br>        [train_data[:i * num_val_samples],<br>         train_data[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br>    partial_train_targets = np.concatenate(<br>        [train_targets[:i * num_val_samples],<br>         train_targets[(i + <span class="hljs-number">1</span>) * num_val_samples:]],<br>        axis=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Build the Keras model (already compiled)</span><br>    model = build_model()<br>    <span class="hljs-comment"># Train the model (in silent mode, verbose=0)</span><br>    history = model.fit(partial_train_data, partial_train_targets,<br>                        validation_data=(val_data, val_targets),<br>                        epochs=num_epochs, batch_size=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">0</span>)<br>    mae_history = history.history[<span class="hljs-string">&#x27;val_mean_absolute_error&#x27;</span>]<br>    all_mae_histories.append(mae_history)<br></code></pre></td></tr></table></figure><pre><code>processing fold # 0processing fold # 1processing fold # 2processing fold # 3</code></pre><p>We can then compute the average of the per-epoch MAE scores for all folds:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">average_mae_history = [<br>    np.mean([x[i] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_mae_histories]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs)]<br></code></pre></td></tr></table></figure><p>Let’s plot this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(average_mae_history) + <span class="hljs-number">1</span>), average_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_26_0.png" alt="output"></p><p>It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let’s:</p><ul><li>Omit the first 10 data points, which are on a different scale from the rest of the curve.</li><li>Replace each point with an exponential moving average of the previous points, to obtain a smooth curve.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smooth_curve</span>(<span class="hljs-params">points, factor=<span class="hljs-number">0.9</span></span>):</span><br>  smoothed_points = []<br>  <span class="hljs-keyword">for</span> point <span class="hljs-keyword">in</span> points:<br>    <span class="hljs-keyword">if</span> smoothed_points:<br>      previous = smoothed_points[-<span class="hljs-number">1</span>]<br>      smoothed_points.append(previous * factor + point * (<span class="hljs-number">1</span> - factor))<br>    <span class="hljs-keyword">else</span>:<br>      smoothed_points.append(point)<br>  <span class="hljs-keyword">return</span> smoothed_points<br><br>smooth_mae_history = smooth_curve(average_mae_history[<span class="hljs-number">10</span>:])<br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(smooth_mae_history) + <span class="hljs-number">1</span>), smooth_mae_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Validation MAE&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_28_0.png" alt="output"></p><p>According to this plot, it seems that validation MAE stops improving significantly after 80 epochs. Past that point, we start overfitting.</p><p>Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we<br>can train a final “production” model on all of the training data, with the best parameters, then look at its performance on the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Get a fresh, compiled model.</span><br>model = build_model()<br><span class="hljs-comment"># Train it on the entirety of the data.</span><br>model.fit(train_data, train_targets,<br>          epochs=<span class="hljs-number">80</span>, batch_size=<span class="hljs-number">16</span>, verbose=<span class="hljs-number">0</span>)<br>test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)<br></code></pre></td></tr></table></figure><pre><code> 32/102 [========&gt;.....................] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_mae_score<br></code></pre></td></tr></table></figure><pre><code>2.5532484335057877</code></pre><p>We are still off by about $2,550.</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>Regression is done using different loss functions from classification; Mean Squared Error (MSE) is a commonly used loss function for<br>regression.</li><li>Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally the concept of “accuracy”<br>does not apply for regression. A common regression metric is Mean Absolute Error (MAE).</li><li>When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.</li><li>When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.</li><li>When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two),<br>in order to avoid severe overfitting.</li></ul><p>This example concludes our series of three introductory practical examples. You are now able to handle common types of problems with vector data input:</p><ul><li>Binary (2-class) classification.</li><li>Multi-class, single-label classification.</li><li>Scalar regression.</li></ul><p>In the next chapter, you will acquire a more formal understanding of some of the concepts you have encountered in these first examples,<br>such as data preprocessing, model evaluation, and overfitting.</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-3</title>
      <link href="2021/04/22/DL-with-python-3/"/>
      <url>2021/04/22/DL-with-python-3/</url>
      
        <content type="html"><![CDATA[<p>3.6-classifying-newswires</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>Using TensorFlow backend.</p><p>‘2.0.8’</p><h1 id="Classifying-newswires-a-multi-class-classification-example"><a href="#Classifying-newswires-a-multi-class-classification-example" class="headerlink" title="Classifying newswires: a multi-class classification example"></a>Classifying newswires: a multi-class classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network.<br>But what happens when you have more than two classes? </p><p>In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many<br>classes, this problem is an instance of “multi-class classification”, and since each data point should be classified into only one<br>category, the problem is more specifically an instance of “single-label, multi-class classification”. If each data point could have<br>belonged to multiple categories (in our case, topics) then we would be facing a “multi-label, multi-class classification” problem.</p><h2 id="The-Reuters-dataset"><a href="#The-Reuters-dataset" class="headerlink" title="The Reuters dataset"></a>The Reuters dataset</h2><p>We will be working with the <em>Reuters dataset</em>, a set of short newswires and their topics, published by Reuters in 1986. It’s a very simple,<br>widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each<br>topic has at least 10 examples in the training set.</p><p>Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s take a look right away:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> reuters<br><br>(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>Like with the IMDB dataset, the argument <code>num_words=10000</code> restricts the data to the 10,000 most frequently occurring words found in the<br>data.</p><p>We have 8,982 training examples and 2,246 test examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_data)<br></code></pre></td></tr></table></figure><pre><code>8982</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_data)<br></code></pre></td></tr></table></figure><pre><code>2246</code></pre><p>As with the IMDB reviews, each example is a list of integers (word indices):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]</code></pre><p>Here’s how you can decode it back to words, in case you are curious:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">word_index = reuters.get_word_index()<br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># Note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_newswire = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_newswire<br></code></pre></td></tr></table></figure><pre><code>&#39;? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3&#39;</code></pre><p>The label associated with an example is an integer between 0 and 45: a topic index.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We can vectorize the data with the exact same code as in our previous example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a “one-hot”<br>encoding. One-hot encoding is a widely used format for categorical data, also called “categorical encoding”.<br>For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1.<br>In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_one_hot</span>(<span class="hljs-params">labels, dimension=<span class="hljs-number">46</span></span>):</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(labels), dimension))<br>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        results[i, label] = <span class="hljs-number">1.</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training labels</span><br>one_hot_train_labels = to_one_hot(train_labels)<br><span class="hljs-comment"># Our vectorized test labels</span><br>one_hot_test_labels = to_one_hot(test_labels)<br></code></pre></td></tr></table></figure><p>Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils.np_utils <span class="hljs-keyword">import</span> to_categorical<br><br>one_hot_train_labels = to_categorical(train_labels)<br>one_hot_test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to<br>classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the<br>dimensionality of the output space is much larger. </p><p>In a stack of <code>Dense</code> layers like what we were using, each layer can only access information present in the output of the previous layer.<br>If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each<br>layer can potentially become an “information bottleneck”. In our previous example, we were using 16-dimensional intermediate layers, but a<br>16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks,<br>permanently dropping relevant information.</p><p>For this reason we will use larger layers. Let’s go with 64 units:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>There are two other things you should note about this architecture:</p><ul><li>We are ending the network with a <code>Dense</code> layer of size 46. This means that for each input sample, our network will output a<br>46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.</li><li>The last layer uses a <code>softmax</code> activation. You have already seen this pattern in the MNIST example. It means that the network will<br>output a <em>probability distribution</em> over the 46 different output classes, i.e. for every input sample, the network will produce a<br>46-dimensional output vector where <code>output[i]</code> is the probability that the sample belongs to class <code>i</code>. The 46 scores will sum to 1.</li></ul><p>The best loss function to use in this case is <code>categorical_crossentropy</code>. It measures the distance between two probability distributions:<br>in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the<br>distance between these two distributions, we train our network to output something as close as possible to the true labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>Let’s set apart 1,000 samples in our training data to use as a validation set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">1000</span>]<br>partial_x_train = x_train[<span class="hljs-number">1000</span>:]<br><br>y_val = one_hot_train_labels[:<span class="hljs-number">1000</span>]<br>partial_y_train = one_hot_train_labels[<span class="hljs-number">1000</span>:]<br></code></pre></td></tr></table></figure><p>Now let’s train our network for 20 epochs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 1s - loss: 2.5241 - acc: 0.4952 - val_loss: 1.7263 - val_acc: 0.6100Epoch 2/207982/7982 [==============================] - 0s - loss: 1.4500 - acc: 0.6854 - val_loss: 1.3478 - val_acc: 0.7070Epoch 3/207982/7982 [==============================] - 0s - loss: 1.0979 - acc: 0.7643 - val_loss: 1.1736 - val_acc: 0.7460Epoch 4/207982/7982 [==============================] - 0s - loss: 0.8723 - acc: 0.8178 - val_loss: 1.0880 - val_acc: 0.7490Epoch 5/207982/7982 [==============================] - 0s - loss: 0.7045 - acc: 0.8477 - val_loss: 0.9822 - val_acc: 0.7760Epoch 6/207982/7982 [==============================] - 0s - loss: 0.5660 - acc: 0.8792 - val_loss: 0.9379 - val_acc: 0.8030Epoch 7/207982/7982 [==============================] - 0s - loss: 0.4569 - acc: 0.9037 - val_loss: 0.9039 - val_acc: 0.8050Epoch 8/207982/7982 [==============================] - 0s - loss: 0.3668 - acc: 0.9238 - val_loss: 0.9279 - val_acc: 0.7890Epoch 9/207982/7982 [==============================] - 0s - loss: 0.3000 - acc: 0.9326 - val_loss: 0.8835 - val_acc: 0.8070Epoch 10/207982/7982 [==============================] - 0s - loss: 0.2505 - acc: 0.9434 - val_loss: 0.8967 - val_acc: 0.8150Epoch 11/207982/7982 [==============================] - 0s - loss: 0.2155 - acc: 0.9473 - val_loss: 0.9080 - val_acc: 0.8110Epoch 12/207982/7982 [==============================] - 0s - loss: 0.1853 - acc: 0.9506 - val_loss: 0.9025 - val_acc: 0.8140Epoch 13/207982/7982 [==============================] - 0s - loss: 0.1680 - acc: 0.9524 - val_loss: 0.9268 - val_acc: 0.8100Epoch 14/207982/7982 [==============================] - 0s - loss: 0.1512 - acc: 0.9562 - val_loss: 0.9500 - val_acc: 0.8130Epoch 15/207982/7982 [==============================] - 0s - loss: 0.1371 - acc: 0.9559 - val_loss: 0.9621 - val_acc: 0.8090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.1306 - acc: 0.9553 - val_loss: 1.0152 - val_acc: 0.8050Epoch 17/207982/7982 [==============================] - 0s - loss: 0.1210 - acc: 0.9575 - val_loss: 1.0262 - val_acc: 0.8010Epoch 18/207982/7982 [==============================] - 0s - loss: 0.1185 - acc: 0.9570 - val_loss: 1.0354 - val_acc: 0.8040Epoch 19/207982/7982 [==============================] - 0s - loss: 0.1128 - acc: 0.9598 - val_loss: 1.0841 - val_acc: 0.8010Epoch 20/207982/7982 [==============================] - 0s - loss: 0.1097 - acc: 0.9594 - val_loss: 1.0707 - val_acc: 0.8040</code></pre><p>Let’s display its loss and accuracy curves:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(loss) + <span class="hljs-number">1</span>)<br><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_29_0.png" alt="output"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/output_30_0.png" alt="output"></p><p>It seems that the network starts overfitting after 8 epochs. Let’s train a new network from scratch for 8 epochs, then let’s evaluate it on<br>the test set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">8</span>,<br>          batch_size=<span class="hljs-number">512</span>,<br>          validation_data=(x_val, y_val))<br>results = model.evaluate(x_test, one_hot_test_labels)<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/87982/7982 [==============================] - 0s - loss: 2.6118 - acc: 0.4667 - val_loss: 1.7207 - val_acc: 0.6360Epoch 2/87982/7982 [==============================] - 0s - loss: 1.3998 - acc: 0.7107 - val_loss: 1.2645 - val_acc: 0.7360Epoch 3/87982/7982 [==============================] - 0s - loss: 1.0343 - acc: 0.7839 - val_loss: 1.0994 - val_acc: 0.7700Epoch 4/87982/7982 [==============================] - 0s - loss: 0.8114 - acc: 0.8329 - val_loss: 1.0252 - val_acc: 0.7820Epoch 5/87982/7982 [==============================] - 0s - loss: 0.6466 - acc: 0.8628 - val_loss: 0.9536 - val_acc: 0.8070Epoch 6/87982/7982 [==============================] - 0s - loss: 0.5271 - acc: 0.8894 - val_loss: 0.9187 - val_acc: 0.8110Epoch 7/87982/7982 [==============================] - 0s - loss: 0.4193 - acc: 0.9126 - val_loss: 0.9051 - val_acc: 0.8120Epoch 8/87982/7982 [==============================] - 0s - loss: 0.3478 - acc: 0.9258 - val_loss: 0.8891 - val_acc: 0.81601952/2246 [=========================&gt;....] - ETA: 0s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure><pre><code>[0.98764628548762257, 0.77693677651807869]</code></pre><p>Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier<br>would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><br>test_labels_copy = copy.copy(test_labels)<br>np.random.shuffle(test_labels_copy)<br><span class="hljs-built_in">float</span>(np.<span class="hljs-built_in">sum</span>(np.array(test_labels) == np.array(test_labels_copy))) / <span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>0.18477292965271594</code></pre><h2 id="Generating-predictions-on-new-data"><a href="#Generating-predictions-on-new-data" class="headerlink" title="Generating predictions on new data"></a>Generating predictions on new data</h2><p>We can verify that the <code>predict</code> method of our model instance returns a probability distribution over all 46 topics. Let’s generate topic<br>predictions for all of the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions = model.predict(x_test)<br></code></pre></td></tr></table></figure><p>Each entry in <code>predictions</code> is a vector of length 46:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions[<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></table></figure><pre><code>(46,)</code></pre><p>The coefficients in this vector sum to 1:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.<span class="hljs-built_in">sum</span>(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>0.99999994</code></pre><p>The largest entry is the predicted class, i.e. the class with the highest probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.argmax(predictions[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><pre><code>3</code></pre><h2 id="A-different-way-to-handle-the-labels-and-the-loss"><a href="#A-different-way-to-handle-the-labels-and-the-loss" class="headerlink" title="A different way to handle the labels and the loss"></a>A different way to handle the labels and the loss</h2><p>We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y_train = np.array(train_labels)<br>y_test = np.array(test_labels)<br></code></pre></td></tr></table></figure><p>The only thing it would change is the choice of the loss function. Our previous loss, <code>categorical_crossentropy</code>, expects the labels to<br>follow a categorical encoding. With integer labels, we should use <code>sparse_categorical_crossentropy</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>, loss=<span class="hljs-string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;acc&#x27;</span>])<br></code></pre></td></tr></table></figure><p>This new loss function is still mathematically the same as <code>categorical_crossentropy</code>; it just has a different interface.</p><h2 id="On-the-importance-of-having-sufficiently-large-intermediate-layers"><a href="#On-the-importance-of-having-sufficiently-large-intermediate-layers" class="headerlink" title="On the importance of having sufficiently large intermediate layers"></a>On the importance of having sufficiently large intermediate layers</h2><p>We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden<br>units. Now let’s try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than<br>46-dimensional, e.g. 4-dimensional.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">4</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">46</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>model.fit(partial_x_train,<br>          partial_y_train,<br>          epochs=<span class="hljs-number">20</span>,<br>          batch_size=<span class="hljs-number">128</span>,<br>          validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure><pre><code>Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740Epoch 2/207982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990Epoch 3/207982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390Epoch 4/207982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570Epoch 5/207982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700Epoch 6/207982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800Epoch 7/207982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810Epoch 8/207982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960Epoch 9/207982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050Epoch 10/207982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020Epoch 11/207982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020Epoch 12/207982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080Epoch 13/207982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110Epoch 14/207982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080Epoch 15/207982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090Epoch 16/207982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080Epoch 17/207982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060Epoch 18/207982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950Epoch 19/207982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930Epoch 20/207982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060&lt;keras.callbacks.History at 0x7f8ce7cdb9b0&gt;</code></pre><p>Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to<br>compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is<br>too low-dimensional. The network is able to cram <em>most</em> of the necessary information into these 8-dimensional representations, but not all<br>of it.</p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>Try using larger or smaller layers: 32 units, 128 units…</li><li>We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers.</li></ul><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Here’s what you should take away from this example:</p><ul><li>If you are trying to classify data points between N classes, your network should end with a <code>Dense</code> layer of size N.</li><li>In a single-label, multi-class classification problem, your network should end with a <code>softmax</code> activation, so that it will output a<br>probability distribution over the N output classes.</li><li><em>Categorical crossentropy</em> is almost always the loss function you should use for such problems. It minimizes the distance between the<br>probability distributions output by the network, and the true distribution of the targets.</li><li>There are two ways to handle labels in multi-class classification:<br>  <strong> Encoding the labels via “categorical encoding” (also known as “one-hot encoding”) and using <code>categorical_crossentropy</code> as your loss<br>function.  </strong> Encoding the labels as integers and using the <code>sparse_categorical_crossentropy</code> loss function.</li><li>If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having<br>intermediate layers that are too small.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-2</title>
      <link href="2021/04/22/DL-with-python-2/"/>
      <url>2021/04/22/DL-with-python-2/</url>
      
        <content type="html"><![CDATA[<p>3.5-classifying-movie-reviews</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="Classifying-movie-reviews-a-binary-classification-example"><a href="#Classifying-movie-reviews-a-binary-classification-example" class="headerlink" title="Classifying movie reviews: a binary classification example"></a>Classifying movie reviews: a binary classification example</h1><p>This notebook contains the code samples found in Chapter 3, Section 5 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we<br>will learn to classify movie reviews into “positive” reviews and “negative” reviews, just based on the text content of the reviews.</p><h2 id="The-IMDB-dataset"><a href="#The-IMDB-dataset" class="headerlink" title="The IMDB dataset"></a>The IMDB dataset</h2><p>We’ll be working with “IMDB dataset”, a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000<br>reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.</p><p>Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to<br>train it! Just because a model performs well on its training data doesn’t mean that it will perform well on data it has never seen, and<br>what you actually care about is your model’s performance on new data (since you already know the labels of your training data — obviously<br>you don’t need your model to predict those). For instance, it is possible that your model could end up merely <em>memorizing</em> a mapping between<br>your training samples and their targets — which would be completely useless for the task of predicting targets for data never seen before.<br>We will go over this point in much more detail in the next chapter.</p><p>Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words)<br>have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.</p><p>The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> imdb<br><br>(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>The argument <code>num_words=10000</code> means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words<br>will be discarded. This allows us to work with vector data of manageable size.</p><p>The variables <code>train_data</code> and <code>test_data</code> are lists of reviews, each review being a list of word indices (encoding a sequence of words).<br><code>train_labels</code> and <code>test_labels</code> are lists of 0s and 1s, where 0 stands for “negative” and 1 stands for “positive”:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>1</code></pre><p>Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">max</span>([<span class="hljs-built_in">max</span>(sequence) <span class="hljs-keyword">for</span> sequence <span class="hljs-keyword">in</span> train_data])<br></code></pre></td></tr></table></figure><pre><code>9999</code></pre><p>For kicks, here’s how you can quickly decode one of these reviews back to English words:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># word_index is a dictionary mapping words to an integer index</span><br>word_index = imdb.get_word_index()<br><span class="hljs-comment"># We reverse it, mapping integer indices to words</span><br>reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])<br><span class="hljs-comment"># We decode the review; note that our indices were offset by 3</span><br><span class="hljs-comment"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span><br>decoded_review = <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;?&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train_data[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_review<br></code></pre></td></tr></table></figure><pre><code>&quot;? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all&quot;</code></pre><h2 id="Preparing-the-data"><a href="#Preparing-the-data" class="headerlink" title="Preparing the data"></a>Preparing the data</h2><p>We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:</p><ul><li>We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape <code>(samples, word_indices)</code>,<br>then use as first layer in our network a layer capable of handling such integer tensors (the <code>Embedding</code> layer, which we will cover in<br>detail later in the book).</li><li>We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence<br><code>[3, 5]</code> into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as<br>first layer in our network a <code>Dense</code> layer, capable of handling floating point vector data.</li></ul><p>We will go with the latter solution. Let’s vectorize our data, which we will do manually for maximum clarity:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorize_sequences</span>(<span class="hljs-params">sequences, dimension=<span class="hljs-number">10000</span></span>):</span><br>    <span class="hljs-comment"># Create an all-zero matrix of shape (len(sequences), dimension)</span><br>    results = np.zeros((<span class="hljs-built_in">len</span>(sequences), dimension))<br>    <span class="hljs-keyword">for</span> i, sequence <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sequences):<br>        results[i, sequence] = <span class="hljs-number">1.</span>  <span class="hljs-comment"># set specific indices of results[i] to 1s</span><br>    <span class="hljs-keyword">return</span> results<br><br><span class="hljs-comment"># Our vectorized training data</span><br>x_train = vectorize_sequences(train_data)<br><span class="hljs-comment"># Our vectorized test data</span><br>x_test = vectorize_sequences(test_data)<br></code></pre></td></tr></table></figure><p>Here’s what our samples look like now:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x_train[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><pre><code>array([0., 1., 1., ..., 0., 0., 0.])</code></pre><p>We should also vectorize our labels, which is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Our vectorized labels</span><br>y_train = np.asarray(train_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = np.asarray(test_labels).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Now our data is ready to be fed into a neural network.</p><h2 id="Building-our-network"><a href="#Building-our-network" class="headerlink" title="Building our network"></a>Building our network</h2><p>Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of<br>network that performs well on such a problem would be a simple stack of fully-connected (<code>Dense</code>) layers with <code>relu</code> activations: <code>Dense(16, activation=&#39;relu&#39;)</code></p><p>The argument being passed to each <code>Dense</code> layer (16) is the number of “hidden units” of the layer. What’s a hidden unit? It’s a dimension<br>in the representation space of the layer. You may remember from the previous chapter that each such <code>Dense</code> layer with a <code>relu</code> activation implements<br>the following chain of tensor operations:</p><p><code>output = relu(dot(W, input) + b)</code></p><p>Having 16 hidden units means that the weight matrix <code>W</code> will have shape <code>(input_dimension, 16)</code>, i.e. the dot product with <code>W</code> will project the<br>input data onto a 16-dimensional representation space (and then we would add the bias vector <code>b</code> and apply the <code>relu</code> operation). You can<br>intuitively understand the dimensionality of your representation space as “how much freedom you are allowing the network to have when<br>learning internal representations”. Having more hidden units (a higher-dimensional representation space) allows your network to learn more<br>complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that<br>will improve performance on the training data but not on the test data).</p><p>There are two key architecture decisions to be made about such stack of dense layers:</p><ul><li>How many layers to use.</li><li>How many “hidden units” to chose for each layer.</li></ul><p>In the next chapter, you will learn formal principles to guide you in making these choices.<br>For the time being, you will have to trust us with the following architecture choice:<br>two intermediate layers with 16 hidden units each,<br>and a third layer which will output the scalar prediction regarding the sentiment of the current review.<br>The intermediate layers will use <code>relu</code> as their “activation function”,<br>and the final layer will use a sigmoid activation so as to output a probability<br>(a score between 0 and 1, indicating how likely the sample is to have the target “1”, i.e. how likely the review is to be positive).<br>A <code>relu</code> (rectified linear unit) is a function meant to zero-out negative values,<br>while a sigmoid “squashes” arbitrary values into the <code>[0, 1]</code> interval, thus outputting something that can be interpreted as a probability.</p><p>Here’s what our network looks like:</p><p><img src="https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png" alt="3-layer network"></p><p>And here’s the Keras implementation, very similar to the MNIST example you saw previously:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br></code></pre></td></tr></table></figure><p>Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network<br>is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the <code>binary_crossentropy</code> loss.<br>It isn’t the only viable choice: you could use, for instance, <code>mean_squared_error</code>. But crossentropy is usually the best choice when you<br>are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the “distance”<br>between probability distributions, or in our case, between the ground-truth distribution and our predictions.</p><p>Here’s the step where we configure our model with the <code>rmsprop</code> optimizer and the <code>binary_crossentropy</code> loss function. Note that we will<br>also monitor accuracy during training.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>We are passing our optimizer, loss function and metrics as strings, which is possible because <code>rmsprop</code>, <code>binary_crossentropy</code> and<br><code>accuracy</code> are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss<br>function or metric function. This former can be done by passing an optimizer class instance as the <code>optimizer</code> argument:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>The latter can be done by passing function objects as the <code>loss</code> or <code>metrics</code> arguments:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> losses<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> metrics<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="hljs-number">0.001</span>),<br>              loss=losses.binary_crossentropy,<br>              metrics=[metrics.binary_accuracy])<br></code></pre></td></tr></table></figure><h2 id="Validating-our-approach"><a href="#Validating-our-approach" class="headerlink" title="Validating our approach"></a>Validating our approach</h2><p>In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a “validation set” by<br>setting apart 10,000 samples from the original training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_val = x_train[:<span class="hljs-number">10000</span>]<br>partial_x_train = x_train[<span class="hljs-number">10000</span>:]<br><br>y_val = y_train[:<span class="hljs-number">10000</span>]<br>partial_y_train = y_train[<span class="hljs-number">10000</span>:]<br></code></pre></td></tr></table></figure><p>We will now train our model for 20 epochs (20 iterations over all samples in the <code>x_train</code> and <code>y_train</code> tensors), in mini-batches of 512<br>samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the<br>validation data as the <code>validation_data</code> argument:</p><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">history = model.fit(partial_x_train,<br>                    partial_y_train,<br>                    epochs=<span class="hljs-number">20</span>,<br>                    batch_size=<span class="hljs-number">512</span>,<br>                    validation_data=(x_val, y_val))<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-20-90a50ee7d258&gt; in &lt;module&gt;----&gt; 1 history = model.fit(partial_x_train,      2                     partial_y_train,      3                     epochs=20,      4                     batch_size=512,      5                     validation_data=(x_val, y_val))D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 572. MiB for an array with shape (15000, 10000) and data type float32</code></pre><p>On CPU, this will take less than two seconds per epoch — training is over in 20 seconds. At the end of every epoch, there is a slight pause<br>as the model computes its loss and accuracy on the 10,000 samples of the validation data.</p><p>Note that the call to <code>model.fit()</code> returns a <code>History</code> object. This object has a member <code>history</code>, which is a dictionary containing data<br>about everything that happened during training. Let’s take a look at it:</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">history_dict = history.history<br>history_dict.keys()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-21-fc51a3f187a7&gt; in &lt;module&gt;----&gt; 1 history_dict = history.history      2 history_dict.keys()NameError: name &#39;history&#39; is not defined</code></pre><p>It contains 4 entries: one per metric that was being monitored, during training and during validation. Let’s use Matplotlib to plot the<br>training and validation loss side by side, as well as the training and validation accuracy:</p><h2 id="-2"><a href="#-2" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>acc = history.history[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc = history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br>loss = history.history[<span class="hljs-string">&#x27;loss&#x27;</span>]<br>val_loss = history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>]<br><br>epochs = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(acc) + <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># &quot;bo&quot; is for &quot;blue dot&quot;</span><br>plt.plot(epochs, loss, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training loss&#x27;</span>)<br><span class="hljs-comment"># b is for &quot;solid blue line&quot;</span><br>plt.plot(epochs, val_loss, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-22-fa6555deaae9&gt; in &lt;module&gt;      1 import matplotlib.pyplot as plt      2----&gt; 3 acc = history.history[&#39;acc&#39;]      4 val_acc = history.history[&#39;val_acc&#39;]      5 loss = history.history[&#39;loss&#39;]NameError: name &#39;history&#39; is not defined</code></pre><h2 id="-3"><a href="#-3" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.clf()   <span class="hljs-comment"># clear figure</span><br>acc_values = history_dict[<span class="hljs-string">&#x27;acc&#x27;</span>]<br>val_acc_values = history_dict[<span class="hljs-string">&#x27;val_acc&#x27;</span>]<br><br>plt.plot(epochs, acc, <span class="hljs-string">&#x27;bo&#x27;</span>, label=<span class="hljs-string">&#x27;Training acc&#x27;</span>)<br>plt.plot(epochs, val_acc, <span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;Validation acc&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training and validation accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epochs&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.legend()<br><br>plt.show()<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-23-cc3500a51e0d&gt; in &lt;module&gt;      1 plt.clf()   # clear figure----&gt; 2 acc_values = history_dict[&#39;acc&#39;]      3 val_acc_values = history_dict[&#39;val_acc&#39;]      4      5 plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)NameError: name &#39;history_dict&#39; is not defined&lt;Figure size 432x288 with 0 Axes&gt;</code></pre><p>The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary<br>slightly due to a different random initialization of your network.</p><p>As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That’s what you would<br>expect when running gradient descent optimization — the quantity you are trying to minimize should get lower with every iteration. But that<br>isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning<br>against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen<br>before. In precise terms, what you are seeing is “overfitting”: after the second epoch, we are over-optimizing on the training data, and we<br>ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.</p><p>In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can<br>leverage to mitigate overfitting, which we will cover in the next chapter.</p><p>Let’s train a new network from scratch for four epochs, then evaluate it on our test data:</p><h2 id="-4"><a href="#-4" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.Sequential()<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">10000</span>,)))<br>model.add(layers.Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>))<br>model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>))<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>              loss=<span class="hljs-string">&#x27;binary_crossentropy&#x27;</span>,<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>model.fit(x_train, y_train, epochs=<span class="hljs-number">4</span>, batch_size=<span class="hljs-number">512</span>)<br>results = model.evaluate(x_test, y_test)<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-24-82373714f0fe&gt; in &lt;module&gt;      8               metrics=[&#39;accuracy&#39;])      9---&gt; 10 model.fit(x_train, y_train, epochs=4, batch_size=512)     11 results = model.evaluate(x_test, y_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)   1048          training_utils.RespectCompiledTrainableState(self):   1049       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.-&gt; 1050       data_handler = data_adapter.DataHandler(   1051           x=x,   1052           y=y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><h2 id="-5"><a href="#-5" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results<br></code></pre></td></tr></table></figure></h2><pre><code>NameError                                 Traceback (most recent call last)&lt;ipython-input-25-100f62972f2f&gt; in &lt;module&gt;----&gt; 1 resultsNameError: name &#39;results&#39; is not defined</code></pre><p>Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.</p><h2 id="Using-a-trained-network-to-generate-predictions-on-new-data"><a href="#Using-a-trained-network-to-generate-predictions-on-new-data" class="headerlink" title="Using a trained network to generate predictions on new data"></a>Using a trained network to generate predictions on new data</h2><p>After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive<br>by using the <code>predict</code> method:</p><h2 id="-6"><a href="#-6" class="headerlink" title=""></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.predict(x_test)<br></code></pre></td></tr></table></figure></h2><pre><code>MemoryError                               Traceback (most recent call last)&lt;ipython-input-26-a556f0421074&gt; in &lt;module&gt;----&gt; 1 model.predict(x_test)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)   1596                         &#39;. Consider setting it to AutoShardPolicy.DATA.&#39;)   1597-&gt; 1598       data_handler = data_adapter.DataHandler(   1599           x=x,   1600           batch_size=batch_size,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)   1098   1099     adapter_cls = select_data_adapter(x, y)-&gt; 1100     self._adapter = adapter_cls(   1101         x,   1102         y,D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)    261                **kwargs):    262     super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)--&gt; 263     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))    264     sample_weight_modes = broadcast_sample_weight_modes(    265         sample_weights, sample_weight_modes)D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _process_tensorlike(inputs)   1014     return x   1015-&gt; 1016   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)   1017   return nest.list_to_tuple(inputs)   1018D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)    657    658   return pack_sequence_as(--&gt; 659       structure[0], [func(*x) for x in entries],    660       expand_composites=expand_composites)    661D:\anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _convert_numpy_and_scipy(x)   1009       if issubclass(x.dtype.type, np.floating):   1010         dtype = backend.floatx()-&gt; 1011       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)   1012     elif scipy_sparse and scipy_sparse.issparse(x):   1013       return _scipy_sparse_to_sparse_tensor(x)D:\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py in wrapper(*args, **kwargs)    199     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;    200     try:--&gt; 201       return target(*args, **kwargs)    202     except (TypeError, ValueError):    203       # Note: convert_to_eager_tensor currently raises a ValueError, not aD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)   1402     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.   1403   &quot;&quot;&quot;-&gt; 1404   return convert_to_tensor_v2(   1405       value, dtype=dtype, dtype_hint=dtype_hint, name=name)   1406D:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)   1408 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):   1409   &quot;&quot;&quot;Converts the given `value` to a `Tensor`.&quot;&quot;&quot;-&gt; 1410   return convert_to_tensor(   1411       value=value,   1412       dtype=dtype,D:\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py in wrapped(*args, **kwargs)    161         with Trace(trace_name, **trace_kwargs):    162           return func(*args, **kwargs)--&gt; 163       return func(*args, **kwargs)    164    165     return wrappedD:\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)   1538   1539     if ret is None:-&gt; 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)   1541   1542     if ret is NotImplemented:D:\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)     50 def _default_conversion_function(value, dtype, name, as_ref):     51   del as_ref  # Unused.---&gt; 52   return constant_op.constant(value, dtype, name=name)     53     54D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)    262     ValueError: if called on a symbolic tensor.    263   &quot;&quot;&quot;--&gt; 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,    265                         allow_broadcast=True)    266D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)    274       with trace.Trace(&quot;tf.constant&quot;):    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    277    278   g = ops.get_default_graph()D:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):    300   &quot;&quot;&quot;Implementation of eager constant.&quot;&quot;&quot;--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)    302   if shape is None:    303     return tD:\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum     97   ctx.ensure_initialized()---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)     99    100MemoryError: Unable to allocate 954. MiB for an array with shape (25000, 10000) and data type float32</code></pre><p>As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). </p><h2 id="Further-experiments"><a href="#Further-experiments" class="headerlink" title="Further experiments"></a>Further experiments</h2><ul><li>We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.</li><li>Try to use layers with more hidden units or less hidden units: 32 units, 64 units…</li><li>Try to use the <code>mse</code> loss function instead of <code>binary_crossentropy</code>.</li><li>Try to use the <code>tanh</code> activation (an activation that was popular in the early days of neural networks) instead of <code>relu</code>.</li></ul><p>These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be<br>improved!</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Here’s what you should take away from this example:</p><ul><li>There’s usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it — as tensors — into a neural<br>network. In the case of sequences of words, they can be encoded as binary vectors — but there are other encoding options too.</li><li>Stacks of <code>Dense</code> layers with <code>relu</code> activations can solve a wide range of problems (including sentiment classification), and you will<br>likely use them frequently.</li><li>In a binary classification problem (two output classes), your network should end with a <code>Dense</code> layer with 1 unit and a <code>sigmoid</code> activation,<br>i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.</li><li>With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is <code>binary_crossentropy</code>.</li><li>The <code>rmsprop</code> optimizer is generally a good enough choice of optimizer, whatever your problem. That’s one less thing for you to worry<br>about.</li><li>As they get better on their training data, neural networks eventually start <em>overfitting</em> and end up obtaining increasingly worse results on data<br>never-seen-before. Make sure to always monitor performance on data that is outside of the training set.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><p><strong>不知道这个破电脑怎么一回事，*的，流下贫穷的泪水</strong></p><p><img src="/img/4.22.1.png" alt="艹"></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL with python-1</title>
      <link href="2021/04/20/DL-with-python-1/"/>
      <url>2021/04/20/DL-with-python-1/</url>
      
        <content type="html"><![CDATA[<p>2.1-a-first-look-at-a-neural-network</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> keras<br>keras.__version__<br></code></pre></td></tr></table></figure><p>‘2.4.3’</p><h1 id="A-first-look-at-a-neural-network"><a href="#A-first-look-at-a-neural-network" class="headerlink" title="A first look at a neural network"></a>A first look at a neural network</h1><p>This notebook contains the code samples found in Chapter 2, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p><hr><p>We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify<br>hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this<br>first example right away. You probably haven’t even installed Keras yet. Don’t worry, that is perfectly fine. In the next chapter, we will<br>review each element in our example and explain them in detail. So don’t worry if some steps seem arbitrary or look like magic to you!<br>We’ve got to start somewhere.</p><p>The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10<br>categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been<br>around for almost as long as the field itself and has been very intensively studied. It’s a set of 60,000 training images, plus 10,000 test<br>images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST<br>as the “Hello World” of deep learning — it’s what you do to verify that your algorithms are working as expected. As you become a machine<br>learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.</p><p>The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.datasets <span class="hljs-keyword">import</span> mnist<br><br>(train_images, train_labels), (test_images, test_labels) = mnist.load_data()<br></code></pre></td></tr></table></figure><p><code>train_images</code> and <code>train_labels</code> form the “training set”, the data that the model will learn from. The model will then be tested on the<br>“test set”, <code>test_images</code> and <code>test_labels</code>. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging<br>from 0 to 9. There is a one-to-one correspondence between the images and the labels.</p><p>Let’s have a look at the training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images.shape<br></code></pre></td></tr></table></figure><pre><code>(60000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(train_labels)<br></code></pre></td></tr></table></figure><pre><code>60000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_labels<br></code></pre></td></tr></table></figure><pre><code>array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</code></pre><p>Let’s have a look at the test data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_images.shape<br></code></pre></td></tr></table></figure><pre><code>(10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(test_labels)<br></code></pre></td></tr></table></figure><pre><code>10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_labels<br></code></pre></td></tr></table></figure><pre><code>array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)</code></pre><p>Our workflow will be as follow: first we will present our neural network with the training data, <code>train_images</code> and <code>train_labels</code>. The<br>network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for <code>test_images</code>, and we<br>will verify if these predictions match the labels from <code>test_labels</code>.</p><p>Let’s build our network — again, remember that you aren’t supposed to understand everything about this example just yet.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> layers<br><br>network = models.Sequential()<br>network.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_shape=(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>,)))<br>network.add(layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br></code></pre></td></tr></table></figure><p>The core building block of neural networks is the “layer”, a data-processing module which you can conceive as a “filter” for data. Some<br>data comes in, and comes out in a more useful form. Precisely, layers extract <em>representations</em> out of the data fed into them — hopefully<br>representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers<br>which will implement a form of progressive “data distillation”. A deep learning model is like a sieve for data processing, made of a<br>succession of increasingly refined data filters — the “layers”.</p><p>Here our network consists of a sequence of two <code>Dense</code> layers, which are densely-connected (also called “fully-connected”) neural layers.<br>The second (and last) layer is a 10-way “softmax” layer, which means it will return an array of 10 probability scores (summing to 1). Each<br>score will be the probability that the current digit image belongs to one of our 10 digit classes.</p><p>To make our network ready for training, we need to pick three more things, as part of “compilation” step:</p><ul><li>A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be<br>able to steer itself in the right direction.</li><li>An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.</li><li>Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly<br>classified).</li></ul><p>The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">network.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;rmsprop&#x27;</span>,<br>                loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>                metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in<br>the <code>[0, 1]</code> interval. Previously, our training images for instance were stored in an array of shape <code>(60000, 28, 28)</code> of type <code>uint8</code> with<br>values in the <code>[0, 255]</code> interval. We transform it into a <code>float32</code> array of shape <code>(60000, 28 * 28)</code> with values between 0 and 1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">train_images = train_images.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>train_images = train_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br><br>test_images = test_images.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>))<br>test_images = test_images.astype(<span class="hljs-string">&#x27;float32&#x27;</span>) / <span class="hljs-number">255</span><br></code></pre></td></tr></table></figure><p>We also need to categorically encode the labels, a step which we explain in chapter 3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><br>train_labels = to_categorical(train_labels)<br>test_labels = to_categorical(test_labels)<br></code></pre></td></tr></table></figure><p>We are now ready to train our network, which in Keras is done via a call to the <code>fit</code> method of the network:<br>we “fit” the model to its training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">network.fit(train_images, train_labels, epochs=<span class="hljs-number">5</span>, batch_size=<span class="hljs-number">128</span>)<br></code></pre></td></tr></table></figure><pre><code>Epoch 1/5469/469 [==============================] - 2s 5ms/step - loss: 0.0272 - accuracy: 0.9920Epoch 2/5469/469 [==============================] - 2s 5ms/step - loss: 0.0212 - accuracy: 0.9939Epoch 3/5469/469 [==============================] - 2s 5ms/step - loss: 0.0159 - accuracy: 0.9955Epoch 4/5469/469 [==============================] - 2s 4ms/step - loss: 0.0123 - accuracy: 0.9964Epoch 5/5469/469 [==============================] - 2s 4ms/step - loss: 0.0101 - accuracy: 0.9973&lt;tensorflow.python.keras.callbacks.History at 0x2bb3ae958b0&gt;</code></pre><p>Two quantities are being displayed during training: the “loss” of the network over the training data, and the accuracy of the network over<br>the training data.</p><p>We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_loss, test_acc = network.evaluate(test_images, test_labels)<br></code></pre></td></tr></table></figure><pre><code>313/313 [==============================] - 1s 3ms/step - loss: 0.0721 - accuracy: 0.9808</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;test_acc:&#x27;</span>, test_acc)<br></code></pre></td></tr></table></figure><pre><code>test_acc: 0.9807999730110168</code></pre><p>Our test set accuracy turns out to be 97.8% — that’s quite a bit lower than the training set accuracy.<br>This gap between training accuracy and test accuracy is an example of “overfitting”,<br>the fact that machine learning models tend to perform worse on new data than on their training data.<br>Overfitting will be a central topic in chapter 3.</p><p>This concludes our very first example — you just saw how we could build and a train a neural network to classify handwritten digits, in<br>less than 20 lines of Python code. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really<br>going on behind the scenes. You will learn about “tensors”, the data-storing objects going into the network, about tensor operations, which<br>layers are made of, and about gradient descent, which allows our network to learn from its training examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一篇突然爆红网络的博士论文致谢</title>
      <link href="2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/"/>
      <url>2021/04/19/%E4%B8%80%E7%AF%87%E7%AA%81%E7%84%B6%E7%88%86%E7%BA%A2%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/4.19.png" alt="他的世界本无光，他把自己活成了光。"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>What&#39;s Maching Learning?</title>
      <link href="2021/04/17/What-s-Maching-Learning/"/>
      <url>2021/04/17/What-s-Maching-Learning/</url>
      
        <content type="html"><![CDATA[<p>贴上了黄博的github笔记</p><span id="more"></span><h2 id="M-D-L的一些资源"><a href="#M-D-L的一些资源" class="headerlink" title="M(D)L的一些资源"></a>M(D)L的一些资源</h2><ol><li><p>黄海广github： <a href="https://github.com/fengdu78">https://github.com/fengdu78</a></p></li><li><p>ML：笔记在线阅读： <a href="http://www.ai-start.com/ml2014/">http://www.ai-start.com/ml2014/</a></p></li><li><p>ML： github：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p></li><li><p>DL：笔记在线阅读： <a href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p></li><li><p>DL： github： <a href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></p></li><li><p>数据科学笔记：github：<a href="https://github.com/fengdu78/Data-Science-Notes">https://github.com/fengdu78/Data-Science-Notes</a></p></li><li><p>统计学习方法代码复现：github：<a href="https://github.com/wzyonggege/">https://github.com/wzyonggege/</a></p></li><li><p>statistical-learning-method or <a href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p></li><li><p>李宏毅ML教材习题解答及实现： <a href="https://github.com/Doraemonzzz/Learning-from-data">https://github.com/Doraemonzzz/Learning-from-data</a></p></li><li><p>李宏毅讲义： <a href="https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg">https://pan.baidu.com/s/1t0EpHwx46u_yzgPfpzOJyg</a> key：h74o</p></li><li><p>吴恩达有关tensorflow的介绍：对应4笔记p251</p></li><li><p>tensorflow简要介绍： <a href="https://link.zhihu.com/?target=https%3A//github.com/aymericdamien/TensorFlow-Examples">https://link.zhihu.com/?target=https%3A//github.com/aymericdamien/TensorFlow-Examples</a></p></li><li><p>DL with python：<a href="https://github.com/fchollet/deep-learning-with-python-notebooks">https://github.com/fchollet/deep-learning-with-python-notebooks</a></p></li><li><p>pytorch入门：<a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p></li><li><p>sql： 语法查询：<a href="http://www.w3school.com.cn/sql/index.asp">http://www.w3school.com.cn/sql/index.asp</a></p></li><li><p>sql： 语法题库：<a href="https://leetcode-cn.com/problemset/database/">https://leetcode-cn.com/problemset/database/</a></p></li><li><p>Ubuntu 18.04深度学习环境配置（CUDA9.0+CUDNN7.4+TensorFlow1.8：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484401&amp;idx=1&amp;sn=73e97612c8a8c6cbb65461f999c024ff&amp;source=41#wechat_redirect</a></p></li><li><p>徐亦达老师课件及下载（中文目录）：<a href="https://github.com/roboticcam/machine-learning-notes">https://github.com/roboticcam/machine-learning-notes</a></p></li><li><p>华校专老师笔记： <a href="https://github.com/huaxz1986">https://github.com/huaxz1986</a></p></li><li><p>华校专网站： <a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></p></li><li><p>论文排班教程： <a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484392&amp;idx=1&amp;sn=6fb2fe9f619154bcbf0b6d8475e56901&amp;source=41#wechat_redirect</a></p></li><li><p>zotero论文管理工具：<a href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&amp;mid=2247484429&amp;idx=1&amp;sn=5663338e5c76374512fa354d68b5a67d&amp;source=41#wechat_redirect</a></p></li><li><p>tf，pytorch，keras样例资源：</p></li></ol><h2 id="一、TensorFlow"><a href="#一、TensorFlow" class="headerlink" title="一、TensorFlow"></a>一、TensorFlow</h2><h3 id="资源地址"><a href="#资源地址" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p><h3 id="资源介绍"><a href="#资源介绍" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>本资源旨在通过示例轻松深入了解TensorFlow。 为了便于阅读，它包括notebook和带注释的源代码。</p><p>它适合想要找到关于TensorFlow的清晰简洁示例的初学者。 除了传统的“原始”TensorFlow实现，您还可以找到最新的TensorFlow API实践（例如layers,estimator,dataset, ……）。</p><p>最后更新（07/25/2018）：添加新示例（GBDT，Word2Vec）和 TF1.9兼容性！ （TF v1.9 +推荐）。</p><h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，TensorFlow 1.8+</p><h3 id="资源目录"><a href="#资源目录" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-先决条件"><a href="#0-先决条件" class="headerlink" title="0  - 先决条件"></a>0  - 先决条件</h4><p>机器学习简介</p><p>MNIST数据集简介</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1  - 简介"></a>1  - 简介</h4><p>Hello World(包含notebook和py源代码)。非常简单的例子，学习如何使用TensorFlow打印“hello world”。</p><p>基本操作(包含notebook和py源代码)。一个涵盖TensorFlow基本操作的简单示例。</p><p>TensorFlow Eager API基础知识(包含notebook和py源代码)。开始使用TensorFlow的Eager API。</p><h4 id="2-基础模型"><a href="#2-基础模型" class="headerlink" title="2  - 基础模型"></a>2  - 基础模型</h4><p>线性回归(包含notebook和py源代码)。使用TensorFlow实现线性回归。</p><p>线性回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现线性回归。</p><p>Logistic回归(包含notebook和py源代码)。使用TensorFlow实现Logistic回归。</p><p>Logistic回归（eager api）(包含notebook和py源代码)。使用TensorFlow的Eager API实现Logistic回归。</p><p>最近邻(包含notebook和py源代码)。使用TensorFlow实现最近邻算法。</p><p>K-Means(包含notebook和py源代码)。使用TensorFlow构建K-Means分类器。</p><p>随机森林(包含notebook和py源代码)。使用TensorFlow构建随机森林分类器。</p><p>Gradient Boosted Decision Tree（GBDT）(包含notebook和py源代码)。使用TensorFlow构建梯度提升决策树（GBDT）。</p><p>Word2Vec（词嵌入）(包含notebook和py源代码)。使用TensorFlow从Wikipedia数据构建词嵌入模型（Word2Vec）。</p><h4 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3  - 神经网络"></a>3  - 神经网络</h4><h5 id="监督学习部分"><a href="#监督学习部分" class="headerlink" title="监督学习部分"></a>监督学习部分</h5><p>简单神经网络(包含notebook和py源代码)。构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>简单神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建一个简单的神经网络（如：Multi-layer Perceptron）来对MNIST数字数据集进行分类。</p><p>简单神经网络（Eager API）(包含notebook和py源代码)。使用TensorFlow Eager API构建一个简单的神经网络（如多层感知器）来对MNIST数字数据集进行分类。</p><p>卷积神经网络(包含notebook和py源代码)。构建卷积神经网络以对MNIST数字数据集进行分类。 Raw TensorFlow实现。</p><p>卷积神经网络（tf.layers / estimator api）(包含notebook和py源代码)。使用TensorFlow’layers’和’estimator’API构建卷积神经网络，对MNIST数字数据集进行分类。</p><p>递归神经网络（LSTM）(包含notebook和py源代码)。构建递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>双向LSTM(包含notebook和py源代码)。构建双向递归神经网络（LSTM）以对MNIST数字数据集进行分类。</p><p>动态LSTM(包含notebook和py源代码)。构建一个递归神经网络（LSTM），执行动态计算以对不同长度的序列进行分类。</p><h5 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h5><p>自动编码器(包含notebook和py源代码)。构建自动编码器以将图像编码为较低维度并重新构建它。</p><p>变分自动编码器（(包含notebook和py源代码)。构建变分自动编码器（VAE），对噪声进行编码和生成图像。</p><p>GAN（Generative Adversarial Networks）(包含notebook和py源代码)。构建生成对抗网络（GAN）以从噪声生成图像。</p><p>DCGAN（Deep Convolutional Generative Adversarial Networks）(包含notebook和py源代码)。构建深度卷积生成对抗网络（DCGAN）以从噪声生成图像。</p><h4 id="4-工具"><a href="#4-工具" class="headerlink" title="4  - 工具"></a>4  - 工具</h4><p>保存和还原模型(包含notebook和py源代码)。使用TensorFlow保存和还原模型。</p><p>Tensorboard  - 图形和损失可视化(包含notebook和py源代码)。使用Tensorboard可视化计算图并绘制损失。</p><p>Tensorboard  - 高级可视化(包含notebook和py源代码)。深入了解Tensorboard;可视化变量，梯度等……</p><h4 id="5-数据管理"><a href="#5-数据管理" class="headerlink" title="5  - 数据管理"></a>5  - 数据管理</h4><p>构建图像数据集(包含notebook和py源代码)。使用TensorFlow数据队列，从图像文件夹或数据集文件构建您自己的图像数据集。</p><p>TensorFlow数据集API(包含notebook和py源代码)。引入TensorFlow数据集API以优化输入数据管道。</p><h4 id="6-多GPU"><a href="#6-多GPU" class="headerlink" title="6  - 多GPU"></a>6  - 多GPU</h4><p>多GPU的基本操作(包含notebook和py源代码)。在TensorFlow中引入多GPU的简单示例。</p><p>在多GPU上训练神经网络(包含notebook和py源代码)。一个清晰简单的TensorFlow实现，用于在多个GPU上训练卷积神经网络。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>一些示例需要MNIST数据集进行训练和测试。官方网站：<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p><h2 id="二、Keras"><a href="#二、Keras" class="headerlink" title="二、Keras"></a>二、Keras</h2><h3 id="资源地址-1"><a href="#资源地址-1" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/erhwenkuo/deep-learning-with-keras-notebooks">https://github.com/erhwenkuo/deep-learning-with-keras-notebooks</a></p><h3 id="资源介绍-1"><a href="#资源介绍-1" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个github的repository主要是ErhWen Kuo在学习Keras的一些记录及练习。希望在学习过程中发现到一些好的信息与示例也可以对想要学习使用Keras来解决问题的同学带来帮助。这些notebooks主要是使用Python 3.6与Keras 2.1.1版本跑在一台配置Nivida 1080Ti的Windows 10的机台所产生的结果，但有些部份会参杂一些Tensorflow与其它的函式库的介绍。</p><h3 id="配置环境-1"><a href="#配置环境-1" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 3.6以上，Keras 2.1.1</p><h3 id="资源目录-1"><a href="#资源目录-1" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="0-图象数据集-工具介绍"><a href="#0-图象数据集-工具介绍" class="headerlink" title="0.图象数据集/工具介绍"></a>0.图象数据集/工具介绍</h4><h5 id="0-0-COCO-API解说与简单示例"><a href="#0-0-COCO-API解说与简单示例" class="headerlink" title="0.0: COCO API解说与简单示例"></a>0.0: COCO API解说与简单示例</h5><h5 id="0-1-土炮自制扑克牌图象数据集"><a href="#0-1-土炮自制扑克牌图象数据集" class="headerlink" title="0.1:土炮自制扑克牌图象数据集"></a>0.1:土炮自制扑克牌图象数据集</h5><h5 id="0-2-使用Pillow来进行图像处理"><a href="#0-2-使用Pillow来进行图像处理" class="headerlink" title="0.2:使用Pillow来进行图像处理"></a>0.2:使用Pillow来进行图像处理</h5><h4 id="1-Keras-API示例"><a href="#1-Keras-API示例" class="headerlink" title="1.Keras API示例"></a>1.Keras API示例</h4><h5 id="1-0-使用图像增强来进行深度学习"><a href="#1-0-使用图像增强来进行深度学习" class="headerlink" title="1.0:使用图像增强来进行深度学习"></a>1.0:使用图像增强来进行深度学习</h5><h5 id="1-1-如何使用Keras函数式API进行深度学习"><a href="#1-1-如何使用Keras函数式API进行深度学习" class="headerlink" title="1.1:如何使用Keras函数式API进行深度学习"></a>1.1:如何使用Keras函数式API进行深度学习</h5><h5 id="1-2-从零开始构建VGG网络来学习Keras"><a href="#1-2-从零开始构建VGG网络来学习Keras" class="headerlink" title="1.2:从零开始构建VGG网络来学习Keras"></a>1.2:从零开始构建VGG网络来学习Keras</h5><h5 id="1-3-使用预训练的模型来分类照片中的物体"><a href="#1-3-使用预训练的模型来分类照片中的物体" class="headerlink" title="1.3:使用预训练的模型来分类照片中的物体"></a>1.3:使用预训练的模型来分类照片中的物体</h5><h5 id="1-4-使用图像增强来训练小数据集"><a href="#1-4-使用图像增强来训练小数据集" class="headerlink" title="1.4:使用图像增强来训练小数据集"></a>1.4:使用图像增强来训练小数据集</h5><h5 id="1-5-使用预先训练的卷积网络模型"><a href="#1-5-使用预先训练的卷积网络模型" class="headerlink" title="1.5:使用预先训练的卷积网络模型"></a>1.5:使用预先训练的卷积网络模型</h5><h5 id="1-6-卷积网络模型学习到什么的可视化"><a href="#1-6-卷积网络模型学习到什么的可视化" class="headerlink" title="1.6:卷积网络模型学习到什么的可视化"></a>1.6:卷积网络模型学习到什么的可视化</h5><h5 id="1-7-构建自动编码器（Autoencoder）"><a href="#1-7-构建自动编码器（Autoencoder）" class="headerlink" title="1.7:构建自动编码器（Autoencoder）"></a>1.7:构建自动编码器（Autoencoder）</h5><h5 id="1-8-序列到序列（Seq-to-Seq）学习介绍"><a href="#1-8-序列到序列（Seq-to-Seq）学习介绍" class="headerlink" title="1.8:序列到序列（Seq-to-Seq）学习介绍"></a>1.8:序列到序列（Seq-to-Seq）学习介绍</h5><h5 id="1-9-One-hot编码工具程序介绍"><a href="#1-9-One-hot编码工具程序介绍" class="headerlink" title="1.9: One-hot编码工具程序介绍"></a>1.9: One-hot编码工具程序介绍</h5><h5 id="1-10-循环神经网络（RNN）介绍"><a href="#1-10-循环神经网络（RNN）介绍" class="headerlink" title="1.10:循环神经网络（RNN）介绍"></a>1.10:循环神经网络（RNN）介绍</h5><h5 id="1-11-LSTM的返回序列和返回状态之间的区别"><a href="#1-11-LSTM的返回序列和返回状态之间的区别" class="headerlink" title="1.11: LSTM的返回序列和返回状态之间的区别"></a>1.11: LSTM的返回序列和返回状态之间的区别</h5><h5 id="1-12-用LSTM来学习英文字母表顺序"><a href="#1-12-用LSTM来学习英文字母表顺序" class="headerlink" title="1.12:用LSTM来学习英文字母表顺序"></a>1.12:用LSTM来学习英文字母表顺序</h5><h4 id="2-图像分类（Image-Classification）"><a href="#2-图像分类（Image-Classification）" class="headerlink" title="2.图像分类（Image Classification）"></a>2.图像分类（Image Classification）</h4><h5 id="2-0-Julia（Chars74K）字母图像分类"><a href="#2-0-Julia（Chars74K）字母图像分类" class="headerlink" title="2.0: Julia（Chars74K）字母图像分类"></a>2.0: Julia（Chars74K）字母图像分类</h5><h5 id="2-1-交通标志图像分类"><a href="#2-1-交通标志图像分类" class="headerlink" title="2.1:交通标志图像分类"></a>2.1:交通标志图像分类</h5><h5 id="2-2-辛普森卡通图像角色分类"><a href="#2-2-辛普森卡通图像角色分类" class="headerlink" title="2.2:辛普森卡通图像角色分类"></a>2.2:辛普森卡通图像角色分类</h5><h5 id="2-3-时尚服饰图像分类"><a href="#2-3-时尚服饰图像分类" class="headerlink" title="2.3:时尚服饰图像分类"></a>2.3:时尚服饰图像分类</h5><h5 id="2-4-人脸关键点辨识"><a href="#2-4-人脸关键点辨识" class="headerlink" title="2.4:人脸关键点辨识"></a>2.4:人脸关键点辨识</h5><h5 id="2-5-Captcha验证码分类"><a href="#2-5-Captcha验证码分类" class="headerlink" title="2.5: Captcha验证码分类"></a>2.5: Captcha验证码分类</h5><h5 id="2-6-Mnist手写图像分类（MLP）"><a href="#2-6-Mnist手写图像分类（MLP）" class="headerlink" title="2.6: Mnist手写图像分类（MLP）"></a>2.6: Mnist手写图像分类（MLP）</h5><h5 id="2-7-Mnist手写图像分类（CNN）"><a href="#2-7-Mnist手写图像分类（CNN）" class="headerlink" title="2.7: Mnist手写图像分类（CNN）"></a>2.7: Mnist手写图像分类（CNN）</h5><h4 id="3-目标检测（Object-Recognition）"><a href="#3-目标检测（Object-Recognition）" class="headerlink" title="3.目标检测（Object Recognition）"></a>3.目标检测（Object Recognition）</h4><h5 id="3-0-YOLO目标检测算法概念与介绍"><a href="#3-0-YOLO目标检测算法概念与介绍" class="headerlink" title="3.0: YOLO目标检测算法概念与介绍"></a>3.0: YOLO目标检测算法概念与介绍</h5><h5 id="3-1-YOLOv2目标检测示例"><a href="#3-1-YOLOv2目标检测示例" class="headerlink" title="3.1: YOLOv2目标检测示例"></a>3.1: YOLOv2目标检测示例</h5><h5 id="3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整"><a href="#3-2-浣熊（Racoon）检测-YOLOv2模型训练与调整" class="headerlink" title="3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整"></a>3.2:浣熊（Racoon）检测-YOLOv2模型训练与调整</h5><h5 id="3-3-浣熊（Racoon）检测-YOLOv2模型的使用"><a href="#3-3-浣熊（Racoon）检测-YOLOv2模型的使用" class="headerlink" title="3.3:浣熊（Racoon）检测-YOLOv2模型的使用"></a>3.3:浣熊（Racoon）检测-YOLOv2模型的使用</h5><h5 id="3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"><a href="#3-4-袋鼠（Kangaroo）检测-YOLOv2模型训练与调整" class="headerlink" title="3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整"></a>3.4:袋鼠（Kangaroo）检测-YOLOv2模型训练与调整</h5><h5 id="3-5-双手（Hands）检测-YOLOv2模型训练与调整"><a href="#3-5-双手（Hands）检测-YOLOv2模型训练与调整" class="headerlink" title="3.5:双手（Hands）检测-YOLOv2模型训练与调整"></a>3.5:双手（Hands）检测-YOLOv2模型训练与调整</h5><h5 id="3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"><a href="#3-6-辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整" class="headerlink" title="3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整"></a>3.6:辛普森卡通图象角色（Simpson）检测-YOLOv2模型训练与调整</h5><h5 id="3-7-MS-COCO图象检测-YOLOv2模型训练与调整"><a href="#3-7-MS-COCO图象检测-YOLOv2模型训练与调整" class="headerlink" title="3.7: MS COCO图象检测-YOLOv2模型训练与调整"></a>3.7: MS COCO图象检测-YOLOv2模型训练与调整</h5><h4 id="4-物体分割（Object-Segmentation）"><a href="#4-物体分割（Object-Segmentation）" class="headerlink" title="4.物体分割（Object Segmentation）"></a>4.物体分割（Object Segmentation）</h4><h4 id="5-关键点检测（Keypoint-Detection）"><a href="#5-关键点检测（Keypoint-Detection）" class="headerlink" title="5.关键点检测（Keypoint Detection）"></a>5.关键点检测（Keypoint Detection）</h4><h4 id="6-图象标题（Image-Caption）"><a href="#6-图象标题（Image-Caption）" class="headerlink" title="6.图象标题（Image Caption）"></a>6.图象标题（Image Caption）</h4><h4 id="7-人脸检测识别（Face-Detection-Recognition）"><a href="#7-人脸检测识别（Face-Detection-Recognition）" class="headerlink" title="7.人脸检测识别（Face Detection/Recognition）"></a>7.人脸检测识别（Face Detection/Recognition）</h4><h5 id="7-0-人脸检测-OpenCV（Haar特征分类器）"><a href="#7-0-人脸检测-OpenCV（Haar特征分类器）" class="headerlink" title="7.0:人脸检测- OpenCV（Haar特征分类器）"></a>7.0:人脸检测- OpenCV（Haar特征分类器）</h5><h5 id="7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）"><a href="#7-1-人脸检测-MTCNN（Multi-task-Cascaded-Convolutional-Networks）" class="headerlink" title="7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）"></a>7.1:人脸检测- MTCNN（Multi-task Cascaded Convolutional Networks）</h5><h5 id="7-2-人脸识别-脸部检测、对齐-amp-裁剪"><a href="#7-2-人脸识别-脸部检测、对齐-amp-裁剪" class="headerlink" title="7.2:人脸识别-脸部检测、对齐&amp;裁剪"></a>7.2:人脸识别-脸部检测、对齐&amp;裁剪</h5><h5 id="7-3-人脸识别-人脸部特征提取-amp-人脸分类器"><a href="#7-3-人脸识别-人脸部特征提取-amp-人脸分类器" class="headerlink" title="7.3:人脸识别-人脸部特征提取&amp;人脸分类器"></a>7.3:人脸识别-人脸部特征提取&amp;人脸分类器</h5><h5 id="7-4-人脸识别-转换、对齐、裁剪、特征提取与比对"><a href="#7-4-人脸识别-转换、对齐、裁剪、特征提取与比对" class="headerlink" title="7.4:人脸识别-转换、对齐、裁剪、特征提取与比对"></a>7.4:人脸识别-转换、对齐、裁剪、特征提取与比对</h5><h5 id="7-5-脸部关键点检测（dlib）"><a href="#7-5-脸部关键点检测（dlib）" class="headerlink" title="7.5:脸部关键点检测（dlib）"></a>7.5:脸部关键点检测（dlib）</h5><h5 id="7-6-头部姿态（Head-pose）估计（dlib）"><a href="#7-6-头部姿态（Head-pose）估计（dlib）" class="headerlink" title="7.6:头部姿态（Head pose）估计（dlib）"></a>7.6:头部姿态（Head pose）估计（dlib）</h5><h4 id="8-自然语言处理（Natural-Language-Processing）"><a href="#8-自然语言处理（Natural-Language-Processing）" class="headerlink" title="8.自然语言处理（Natural Language Processing）"></a>8.自然语言处理（Natural Language Processing）</h4><h5 id="8-0-词嵌入（word-embeddings）介绍"><a href="#8-0-词嵌入（word-embeddings）介绍" class="headerlink" title="8.0:词嵌入（word embeddings）介绍"></a>8.0:词嵌入（word embeddings）介绍</h5><h5 id="8-1-使用结巴（jieba）进行中文分词"><a href="#8-1-使用结巴（jieba）进行中文分词" class="headerlink" title="8.1:使用结巴（jieba）进行中文分词"></a>8.1:使用结巴（jieba）进行中文分词</h5><h5 id="8-2-Word2vec词嵌入（word-embeddings）的基本概念"><a href="#8-2-Word2vec词嵌入（word-embeddings）的基本概念" class="headerlink" title="8.2: Word2vec词嵌入（word embeddings）的基本概念"></a>8.2: Word2vec词嵌入（word embeddings）的基本概念</h5><h5 id="8-3-使用结巴（jieba）进行歌词分析"><a href="#8-3-使用结巴（jieba）进行歌词分析" class="headerlink" title="8.3:使用结巴（jieba）进行歌词分析"></a>8.3:使用结巴（jieba）进行歌词分析</h5><h5 id="8-4-使用gensim训练中文词向量（word2vec）"><a href="#8-4-使用gensim训练中文词向量（word2vec）" class="headerlink" title="8.4:使用gensim训练中文词向量（word2vec）"></a>8.4:使用gensim训练中文词向量（word2vec）</h5><h2 id="三、Pytorch"><a href="#三、Pytorch" class="headerlink" title="三、Pytorch"></a>三、Pytorch</h2><h3 id="资源地址-2"><a href="#资源地址-2" class="headerlink" title="资源地址"></a>资源地址</h3><p><a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p><h3 id="资源介绍-2"><a href="#资源介绍-2" class="headerlink" title="资源介绍"></a>资源介绍</h3><p>这个资源为深度学习研究人员提供了学习PyTorch的教程代码大多数模型都使用少于30行代码实现。 在开始本教程之前，建议先看完Pytorch官方教程。</p><h3 id="配置环境-2"><a href="#配置环境-2" class="headerlink" title="配置环境"></a>配置环境</h3><p>python 2.7或者3.5以上，pytorch 0.4</p><h3 id="资源目录-2"><a href="#资源目录-2" class="headerlink" title="资源目录"></a>资源目录</h3><h4 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1.基础知识"></a>1.基础知识</h4><p>PyTorch基础知识</p><p>线性回归</p><p>Logistic回归</p><p>前馈神经网络</p><h4 id="2-中级"><a href="#2-中级" class="headerlink" title="2.中级"></a>2.中级</h4><p>卷积神经网络</p><p>深度残差网络</p><p>递归神经网络</p><p>双向递归神经网络</p><p>语言模型（RNN-LM）</p><h4 id="3-高级"><a href="#3-高级" class="headerlink" title="3.高级"></a>3.高级</h4><p>生成性对抗网络</p><p>变分自动编码器</p><p>神经风格转移</p><p>图像字幕（CNN-RNN）</p><h4 id="4-工具-1"><a href="#4-工具-1" class="headerlink" title="4.工具"></a>4.工具</h4><p>PyTorch中的TensorBoard</p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>my first blog</title>
      <link href="2021/04/17/my-first-blog/"/>
      <url>2021/04/17/my-first-blog/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p>先发一篇试试^_^</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Test</title>
      <link href="2021/04/17/Test/"/>
      <url>2021/04/17/Test/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p>A blog for test.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/04/16/hello-world/"/>
      <url>2021/04/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>搞了**的整整两天，我的个人blog终于能在云端访问了^^^^</p><span id="more"></span><p>After two fucking days, Lao Tzu’s personal blog is finally online, fuck</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
